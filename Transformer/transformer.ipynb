{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    filters = 128  # 内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize，因为要确保每个layer后的输出维度和输入维度是一致的。\n",
    "    numHeads = 8  # Attention 的头数\n",
    "    numBlocks = 1  # 设置transformer block的数量\n",
    "    epsilon = 1e-8  # LayerNorm 层中的最小除数\n",
    "    keepPorp = 0.9  # multi head attention 中的dropout\n",
    "    \n",
    "    dropoutKeepProb = 0.5 # 全连接层的dropout\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置嵌入\n",
    "def fixedPositionEmbedding(batchSize, sequenceLen):\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], name=\"embeddedPosition\")\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层, 位置向量的定义方式有两种：一是直接用固定的one-hot的形式传入，然后和词向量拼接，在当前的数据集上表现效果更好。另一种\n",
    "        # 就是按照论文中的方法实现，这样的效果反而更差，可能是增大了模型的复杂度，在小数据集上表现不佳。\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "\n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            for i in range(config.model.numBlocks):\n",
    "                with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    multiHeadAtt = self._multiheadAttention(rawKeys=self.embedded, queries=self.embeddedWords,\n",
    "                                                            keys=self.embeddedWords)\n",
    "                    # 维度[batch_size, sequence_length, embedding_size]\n",
    "                    self.embeddedWords = self._feedForward(multiHeadAtt, \n",
    "                                                           [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "                \n",
    "            outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "\n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "\n",
    "#         with tf.name_scope(\"wordEmbedding\"):\n",
    "#             self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "#             self.wordEmbedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "        \n",
    "#         with tf.name_scope(\"positionEmbedding\"):\n",
    "#             print(self.wordEmbedded)\n",
    "#             self.positionEmbedded = self._positionEmbedding()\n",
    "            \n",
    "#         self.embeddedWords = self.wordEmbedded + self.positionEmbedded\n",
    "            \n",
    "#         with tf.name_scope(\"transformer\"):\n",
    "#             for i in range(config.model.numBlocks):\n",
    "#                 with tf.name_scope(\"transformer-{}\".format(i + 1)):\n",
    "            \n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     multiHeadAtt = self._multiheadAttention(rawKeys=self.wordEmbedded, queries=self.embeddedWords,\n",
    "#                                                             keys=self.embeddedWords, reuse=True)\n",
    "#                     # 维度[batch_size, sequence_length, embedding_size]\n",
    "#                     self.embeddedWords = self._feedForward(multiHeadAtt, [config.model.filters, config.model.embeddingSize], reuse=True)\n",
    "                \n",
    "#             outputs = tf.reshape(self.embeddedWords, [-1, config.sequenceLength * (config.model.embeddingSize)])\n",
    "\n",
    "#         outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            outputs = tf.nn.dropout(outputs, keep_prob=self.dropoutKeepProb)\n",
    "    \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def _layerNormalization(self, inputs, scope=\"layerNorm\"):\n",
    "        # LayerNorm层和BN层有所不同\n",
    "        epsilon = self.config.model.epsilon\n",
    "\n",
    "        inputsShape = inputs.get_shape() # [batch_size, sequence_length, embedding_size]\n",
    "\n",
    "        paramsShape = inputsShape[-1:]\n",
    "\n",
    "        # LayerNorm是在最后的维度上计算输入的数据的均值和方差，BN层是考虑所有维度的\n",
    "        # mean, variance的维度都是[batch_size, sequence_len, 1]\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "\n",
    "        beta = tf.Variable(tf.zeros(paramsShape))\n",
    "\n",
    "        gamma = tf.Variable(tf.ones(paramsShape))\n",
    "        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "        \n",
    "        outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def _multiheadAttention(self, rawKeys, queries, keys, numUnits=None, causality=False, scope=\"multiheadAttention\"):\n",
    "        \n",
    "        numHeads = self.config.model.numHeads\n",
    "        keepP\n",
    "        \n",
    "        if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "            numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "        # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "        # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "        # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]\n",
    "        Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "        K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "        V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "\n",
    "        # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "        # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "        K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "        V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "\n",
    "        # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "        similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "\n",
    "        # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "        scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "        # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "        # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "        # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "        # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "\n",
    "        # 将每一时序上的向量中的值相加取平均值\n",
    "        keyMasks = tf.sign(tf.abs(tf.reduce_sum(rawKeys, axis=-1)))  # 维度[batch_size, time_step]\n",
    "\n",
    "        # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度\n",
    "        keyMasks = tf.tile(keyMasks, [numHeads, 1]) \n",
    "\n",
    "        # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]\n",
    "        keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "\n",
    "        # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "        paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "\n",
    "        # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "        # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "        maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * numHeads, queries_len, key_len]\n",
    "\n",
    "        # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "        # Decoder是生成模型，主要用在语言生成中\n",
    "        if causality:\n",
    "            diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "            maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "        # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]\n",
    "        weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "        # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "        outputs = tf.matmul(weights, V_)\n",
    "\n",
    "        # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "        \n",
    "        outputs = tf.nn.dropout(outputs, keep_prob=0.9)\n",
    "\n",
    "        # 对每个subLayers建立残差连接，即H(x) = F(x) + x\n",
    "        outputs += queries\n",
    "        # normalization 层\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def _feedForward(self, inputs, filters, scope=\"multiheadAttention\"):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        \n",
    "        # 内层\n",
    "        params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                  \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 外层\n",
    "        params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                  \"activation\": None, \"use_bias\": True}\n",
    "\n",
    "        # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "        # 维度[batch_size, sequence_length, embedding_size]\n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "        # 残差连接\n",
    "        outputs += inputs\n",
    "\n",
    "        # 归一化处理\n",
    "        outputs = self._layerNormalization(outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _positionEmbedding(self, scope=\"positionEmbedding\"):\n",
    "        # 生成可训练的位置向量\n",
    "        batchSize = self.config.batchSize\n",
    "        sequenceLen = self.config.sequenceLength\n",
    "        embeddingSize = self.config.model.embeddingSize\n",
    "        \n",
    "        # 生成位置的索引，并扩张到batch中所有的样本上\n",
    "        positionIndex = tf.tile(tf.expand_dims(tf.range(sequenceLen), 0), [batchSize, 1])\n",
    "\n",
    "        # 根据正弦和余弦函数来获得每个位置上的embedding的第一部分\n",
    "        positionEmbedding = np.array([[pos / np.power(10000, (i-i%2) / embeddingSize) for i in range(embeddingSize)] \n",
    "                                      for pos in range(sequenceLen)])\n",
    "\n",
    "        # 然后根据奇偶性分别用sin和cos函数来包装\n",
    "        positionEmbedding[:, 0::2] = np.sin(positionEmbedding[:, 0::2])\n",
    "        positionEmbedding[:, 1::2] = np.cos(positionEmbedding[:, 1::2])\n",
    "\n",
    "        # 将positionEmbedding转换成tensor的格式\n",
    "        positionEmbedding_ = tf.cast(positionEmbedding, dtype=tf.float32)\n",
    "\n",
    "        # 得到三维的矩阵[batchSize, sequenceLen, embeddingSize]\n",
    "        positionEmbedded = tf.nn.embedding_lookup(positionEmbedding_, positionIndex)\n",
    "\n",
    "        return positionEmbedded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/hist is illegal; using dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0/grad/sparsity is illegal; using dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/hist is illegal; using dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0/grad/sparsity is illegal; using dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/hist is illegal; using dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0/grad/sparsity is illegal; using dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/hist is illegal; using dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0/grad/sparsity is illegal; using dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/hist is illegal; using dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0/grad/sparsity is illegal; using dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/hist is illegal; using dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0/grad/sparsity is illegal; using dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/hist is illegal; using transformer/transformer-1/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/hist is illegal; using transformer/transformer-1/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_1:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/hist is illegal; using conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/kernel:0/grad/sparsity is illegal; using conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/hist is illegal; using conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d/bias:0/grad/sparsity is illegal; using conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/hist is illegal; using conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/kernel:0/grad/sparsity is illegal; using conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/hist is illegal; using conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv1d_1/bias:0/grad/sparsity is illegal; using conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/hist is illegal; using transformer/transformer-1/Variable_2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_2:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/hist is illegal; using transformer/transformer-1/Variable_3_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/transformer-1/Variable_3:0/grad/sparsity is illegal; using transformer/transformer-1/Variable_3_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Transformer/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-14T17:34:14.422511, step: 1, loss: 0.9639316201210022, acc: 0.4922, auc: 0.4768, precision: 0.4607, recall: 0.7069\n",
      "2019-01-14T17:34:14.646165, step: 2, loss: 24.526830673217773, acc: 0.4844, auc: 0.4875, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:14.863532, step: 3, loss: 11.537639617919922, acc: 0.4844, auc: 0.5235, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:15.074862, step: 4, loss: 5.99610710144043, acc: 0.4688, auc: 0.4284, precision: 0.4688, recall: 1.0\n",
      "2019-01-14T17:34:15.301781, step: 5, loss: 6.395857810974121, acc: 0.4766, auc: 0.4443, precision: 0.4766, recall: 1.0\n",
      "2019-01-14T17:34:15.528501, step: 6, loss: 1.1198750734329224, acc: 0.5156, auc: 0.5016, precision: 0.5758, recall: 0.2836\n",
      "2019-01-14T17:34:15.746275, step: 7, loss: 3.4636199474334717, acc: 0.5, auc: 0.5024, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:15.962547, step: 8, loss: 1.7098071575164795, acc: 0.4375, auc: 0.5212, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:16.181669, step: 9, loss: 3.3191754817962646, acc: 0.4844, auc: 0.5103, precision: 0.4844, recall: 1.0\n",
      "2019-01-14T17:34:16.402247, step: 10, loss: 2.8448760509490967, acc: 0.6328, auc: 0.5598, precision: 0.6328, recall: 1.0\n",
      "2019-01-14T17:34:16.615369, step: 11, loss: 2.4968833923339844, acc: 0.5078, auc: 0.4765, precision: 0.4959, recall: 0.9839\n",
      "2019-01-14T17:34:16.841513, step: 12, loss: 1.484389305114746, acc: 0.5781, auc: 0.5663, precision: 0.5, recall: 0.0185\n",
      "2019-01-14T17:34:17.064863, step: 13, loss: 3.1787514686584473, acc: 0.5234, auc: 0.519, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:17.299289, step: 14, loss: 2.2565789222717285, acc: 0.4922, auc: 0.547, precision: 0.0, recall: 0.0\n",
      "2019-01-14T17:34:17.525842, step: 15, loss: 1.0481683015823364, acc: 0.6016, auc: 0.6168, precision: 0.6163, recall: 0.7465\n",
      "2019-01-14T17:34:17.744911, step: 16, loss: 2.3680410385131836, acc: 0.5625, auc: 0.4754, precision: 0.56, recall: 0.9859\n",
      "2019-01-14T17:34:17.964431, step: 17, loss: 2.0832555294036865, acc: 0.5312, auc: 0.6014, precision: 0.528, recall: 0.9851\n",
      "2019-01-14T17:34:18.197168, step: 18, loss: 1.0602797269821167, acc: 0.6094, auc: 0.6296, precision: 0.56, recall: 0.7119\n",
      "2019-01-14T17:34:18.425594, step: 19, loss: 1.9038841724395752, acc: 0.5078, auc: 0.4752, precision: 0.3846, recall: 0.0833\n",
      "2019-01-14T17:34:18.653703, step: 20, loss: 2.2217090129852295, acc: 0.5312, auc: 0.549, precision: 0.375, recall: 0.0517\n",
      "2019-01-14T17:34:18.887182, step: 21, loss: 1.8688937425613403, acc: 0.5156, auc: 0.5745, precision: 0.75, recall: 0.0909\n",
      "2019-01-14T17:34:19.114699, step: 22, loss: 0.8913322687149048, acc: 0.5938, auc: 0.5942, precision: 0.566, recall: 0.5085\n",
      "2019-01-14T17:34:19.344092, step: 23, loss: 1.5915098190307617, acc: 0.5391, auc: 0.6198, precision: 0.5185, recall: 0.8889\n",
      "2019-01-14T17:34:19.573938, step: 24, loss: 1.59859299659729, acc: 0.5781, auc: 0.6324, precision: 0.5614, recall: 0.9412\n",
      "2019-01-14T17:34:19.799365, step: 25, loss: 1.2623555660247803, acc: 0.5234, auc: 0.5747, precision: 0.5, recall: 0.7049\n",
      "2019-01-14T17:34:20.032002, step: 26, loss: 1.129453420639038, acc: 0.5938, auc: 0.5863, precision: 0.6333, recall: 0.3167\n",
      "2019-01-14T17:34:20.254974, step: 27, loss: 1.6928375959396362, acc: 0.5156, auc: 0.5765, precision: 0.5, recall: 0.0484\n",
      "2019-01-14T17:34:20.480763, step: 28, loss: 1.6297876834869385, acc: 0.5547, auc: 0.5934, precision: 0.8462, recall: 0.1667\n",
      "2019-01-14T17:34:20.708039, step: 29, loss: 0.6792205572128296, acc: 0.7188, auc: 0.7463, precision: 0.7083, recall: 0.6071\n",
      "2019-01-14T17:34:20.945841, step: 30, loss: 1.0519649982452393, acc: 0.5781, auc: 0.6042, precision: 0.5876, recall: 0.8028\n",
      "2019-01-14T17:34:21.165673, step: 31, loss: 1.4918212890625, acc: 0.5391, auc: 0.567, precision: 0.5146, recall: 0.8548\n",
      "2019-01-14T17:34:21.403540, step: 32, loss: 0.9299808144569397, acc: 0.6328, auc: 0.7253, precision: 0.5067, recall: 0.7917\n",
      "2019-01-14T17:34:21.628792, step: 33, loss: 0.9136167168617249, acc: 0.5781, auc: 0.6714, precision: 0.6522, recall: 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:34:21.868015, step: 34, loss: 1.304847002029419, acc: 0.5312, auc: 0.6439, precision: 0.5833, recall: 0.1129\n",
      "2019-01-14T17:34:22.100852, step: 35, loss: 1.288620114326477, acc: 0.4609, auc: 0.6308, precision: 0.5714, recall: 0.1127\n",
      "2019-01-14T17:34:22.343487, step: 36, loss: 0.823313295841217, acc: 0.625, auc: 0.6775, precision: 0.6981, recall: 0.5362\n",
      "2019-01-14T17:34:22.578904, step: 37, loss: 1.3947309255599976, acc: 0.5703, auc: 0.627, precision: 0.5429, recall: 0.8906\n",
      "2019-01-14T17:34:22.800744, step: 38, loss: 1.305511713027954, acc: 0.6094, auc: 0.6594, precision: 0.566, recall: 0.9375\n",
      "2019-01-14T17:34:23.029602, step: 39, loss: 0.8942735195159912, acc: 0.6016, auc: 0.6517, precision: 0.5479, recall: 0.6897\n",
      "2019-01-14T17:34:23.260079, step: 40, loss: 1.1262472867965698, acc: 0.5781, auc: 0.5744, precision: 0.5769, recall: 0.2586\n",
      "2019-01-14T17:34:23.492447, step: 41, loss: 1.1378706693649292, acc: 0.6016, auc: 0.7128, precision: 0.7857, recall: 0.1864\n",
      "2019-01-14T17:34:23.725712, step: 42, loss: 1.2091726064682007, acc: 0.5469, auc: 0.6118, precision: 0.8, recall: 0.2286\n",
      "2019-01-14T17:34:23.960788, step: 43, loss: 0.8306401371955872, acc: 0.6094, auc: 0.6603, precision: 0.6364, recall: 0.5385\n",
      "2019-01-14T17:34:24.190734, step: 44, loss: 1.1145230531692505, acc: 0.625, auc: 0.717, precision: 0.5918, recall: 0.8788\n",
      "2019-01-14T17:34:24.419558, step: 45, loss: 1.380733847618103, acc: 0.5547, auc: 0.651, precision: 0.5421, recall: 0.8788\n",
      "2019-01-14T17:34:24.657919, step: 46, loss: 0.970030665397644, acc: 0.5703, auc: 0.6625, precision: 0.5352, recall: 0.6333\n",
      "2019-01-14T17:34:24.883844, step: 47, loss: 1.175410270690918, acc: 0.4922, auc: 0.6724, precision: 0.6471, recall: 0.1571\n",
      "2019-01-14T17:34:25.100305, step: 48, loss: 1.1258747577667236, acc: 0.5078, auc: 0.756, precision: 0.7619, recall: 0.2162\n",
      "2019-01-14T17:34:25.324895, step: 49, loss: 0.8958710432052612, acc: 0.6328, auc: 0.7118, precision: 0.871, recall: 0.3857\n",
      "2019-01-14T17:34:25.545347, step: 50, loss: 1.0528321266174316, acc: 0.5859, auc: 0.6621, precision: 0.5287, recall: 0.7931\n",
      "2019-01-14T17:34:25.771190, step: 51, loss: 1.0467352867126465, acc: 0.6328, auc: 0.6951, precision: 0.6111, recall: 0.9296\n",
      "2019-01-14T17:34:25.991663, step: 52, loss: 1.0298885107040405, acc: 0.5938, auc: 0.7135, precision: 0.5217, recall: 0.8571\n",
      "2019-01-14T17:34:26.220483, step: 53, loss: 0.7805257439613342, acc: 0.6406, auc: 0.7272, precision: 0.7586, recall: 0.3607\n",
      "2019-01-14T17:34:26.452719, step: 54, loss: 0.9597148895263672, acc: 0.5859, auc: 0.7558, precision: 0.8571, recall: 0.1905\n",
      "2019-01-14T17:34:26.676359, step: 55, loss: 1.149213194847107, acc: 0.5469, auc: 0.7648, precision: 0.9286, recall: 0.1857\n",
      "2019-01-14T17:34:26.901310, step: 56, loss: 0.52491694688797, acc: 0.7734, auc: 0.8413, precision: 0.86, recall: 0.6615\n",
      "2019-01-14T17:34:27.126786, step: 57, loss: 1.1090412139892578, acc: 0.6328, auc: 0.7095, precision: 0.5842, recall: 0.9219\n",
      "2019-01-14T17:34:27.344332, step: 58, loss: 1.086750864982605, acc: 0.6094, auc: 0.7238, precision: 0.5567, recall: 0.8852\n",
      "2019-01-14T17:34:27.567029, step: 59, loss: 0.7799776792526245, acc: 0.6797, auc: 0.752, precision: 0.6667, recall: 0.7941\n",
      "2019-01-14T17:34:27.787486, step: 60, loss: 0.7396306991577148, acc: 0.6641, auc: 0.7712, precision: 0.7692, recall: 0.4688\n",
      "2019-01-14T17:34:28.010634, step: 61, loss: 0.9398101568222046, acc: 0.6641, auc: 0.7424, precision: 0.8, recall: 0.3448\n",
      "2019-01-14T17:34:28.256928, step: 62, loss: 0.7788159847259521, acc: 0.6797, auc: 0.8089, precision: 0.8696, recall: 0.3448\n",
      "2019-01-14T17:34:28.481786, step: 63, loss: 0.47878730297088623, acc: 0.7266, auc: 0.8581, precision: 0.7826, recall: 0.5902\n",
      "2019-01-14T17:34:28.703657, step: 64, loss: 0.6326030492782593, acc: 0.7266, auc: 0.8128, precision: 0.7067, recall: 0.803\n",
      "2019-01-14T17:34:28.929926, step: 65, loss: 0.8364788293838501, acc: 0.6797, auc: 0.771, precision: 0.6316, recall: 0.7869\n",
      "2019-01-14T17:34:29.150796, step: 66, loss: 0.6366314888000488, acc: 0.7578, auc: 0.8273, precision: 0.75, recall: 0.8261\n",
      "2019-01-14T17:34:29.377033, step: 67, loss: 0.5695192813873291, acc: 0.7109, auc: 0.8349, precision: 0.76, recall: 0.6032\n",
      "2019-01-14T17:34:29.609282, step: 68, loss: 0.8235071897506714, acc: 0.625, auc: 0.7795, precision: 0.7353, recall: 0.3906\n",
      "2019-01-14T17:34:29.843367, step: 69, loss: 0.7000598907470703, acc: 0.6797, auc: 0.7924, precision: 0.8043, recall: 0.5362\n",
      "2019-01-14T17:34:30.060617, step: 70, loss: 0.6555427312850952, acc: 0.7812, auc: 0.8425, precision: 0.6984, recall: 0.8302\n",
      "2019-01-14T17:34:30.279866, step: 71, loss: 0.5504982471466064, acc: 0.7578, auc: 0.8617, precision: 0.7121, recall: 0.7966\n",
      "2019-01-14T17:34:30.509499, step: 72, loss: 0.4394475221633911, acc: 0.7578, auc: 0.8801, precision: 0.7903, recall: 0.7313\n",
      "2019-01-14T17:34:30.725087, step: 73, loss: 0.6436725854873657, acc: 0.75, auc: 0.8187, precision: 0.7755, recall: 0.6441\n",
      "2019-01-14T17:34:30.953641, step: 74, loss: 0.3855772018432617, acc: 0.8047, auc: 0.9149, precision: 0.875, recall: 0.7313\n",
      "2019-01-14T17:34:31.170620, step: 75, loss: 0.45438438653945923, acc: 0.7656, auc: 0.8907, precision: 0.8261, recall: 0.6333\n",
      "2019-01-14T17:34:31.407305, step: 76, loss: 0.5439025163650513, acc: 0.7969, auc: 0.8583, precision: 0.8182, recall: 0.7941\n",
      "2019-01-14T17:34:31.626579, step: 77, loss: 0.5290598273277283, acc: 0.7969, auc: 0.8909, precision: 0.7656, recall: 0.8167\n",
      "2019-01-14T17:34:31.843072, step: 78, loss: 0.37918707728385925, acc: 0.8047, auc: 0.9183, precision: 0.8636, recall: 0.7808\n",
      "2019-01-14T17:34:32.078701, step: 79, loss: 0.3144311010837555, acc: 0.8359, auc: 0.9405, precision: 0.8696, recall: 0.8333\n",
      "2019-01-14T17:34:32.304312, step: 80, loss: 0.6137219667434692, acc: 0.75, auc: 0.8471, precision: 0.7593, recall: 0.6833\n",
      "2019-01-14T17:34:32.526595, step: 81, loss: 0.4716983139514923, acc: 0.7891, auc: 0.907, precision: 0.8627, recall: 0.6875\n",
      "2019-01-14T17:34:32.755008, step: 82, loss: 0.5278261303901672, acc: 0.7812, auc: 0.8851, precision: 0.7458, recall: 0.7719\n",
      "2019-01-14T17:34:32.976404, step: 83, loss: 0.5540834069252014, acc: 0.7969, auc: 0.8618, precision: 0.8548, recall: 0.7571\n",
      "2019-01-14T17:34:33.196008, step: 84, loss: 0.48559126257896423, acc: 0.8359, auc: 0.9049, precision: 0.806, recall: 0.871\n",
      "2019-01-14T17:34:33.435034, step: 85, loss: 0.4906650185585022, acc: 0.8125, auc: 0.8894, precision: 0.7846, recall: 0.8361\n",
      "2019-01-14T17:34:33.669032, step: 86, loss: 0.5002070069313049, acc: 0.7734, auc: 0.8788, precision: 0.8475, recall: 0.7143\n",
      "2019-01-14T17:34:33.923145, step: 87, loss: 0.5549001097679138, acc: 0.7578, auc: 0.8971, precision: 0.8919, recall: 0.55\n",
      "2019-01-14T17:34:34.130806, step: 88, loss: 0.4691024124622345, acc: 0.8203, auc: 0.8921, precision: 0.8226, recall: 0.8095\n",
      "2019-01-14T17:34:34.334480, step: 89, loss: 0.519430935382843, acc: 0.8047, auc: 0.8852, precision: 0.7838, recall: 0.8657\n",
      "2019-01-14T17:34:34.537873, step: 90, loss: 0.5882952213287354, acc: 0.7656, auc: 0.8359, precision: 0.7692, recall: 0.7692\n",
      "2019-01-14T17:34:34.749620, step: 91, loss: 0.3921874761581421, acc: 0.875, auc: 0.9156, precision: 0.8358, recall: 0.918\n",
      "2019-01-14T17:34:34.977698, step: 92, loss: 0.541262149810791, acc: 0.7812, auc: 0.8777, precision: 0.8889, recall: 0.6349\n",
      "2019-01-14T17:34:35.177132, step: 93, loss: 0.5094298124313354, acc: 0.7734, auc: 0.8936, precision: 0.8909, recall: 0.6806\n",
      "2019-01-14T17:34:35.395103, step: 94, loss: 0.5167421698570251, acc: 0.7969, auc: 0.8827, precision: 0.8125, recall: 0.7879\n",
      "2019-01-14T17:34:35.612558, step: 95, loss: 0.5155705213546753, acc: 0.7891, auc: 0.9053, precision: 0.7361, recall: 0.8689\n",
      "2019-01-14T17:34:35.839807, step: 96, loss: 0.44286203384399414, acc: 0.8594, auc: 0.9162, precision: 0.8676, recall: 0.8676\n",
      "2019-01-14T17:34:36.061562, step: 97, loss: 0.5054905414581299, acc: 0.7734, auc: 0.8805, precision: 0.8033, recall: 0.7424\n",
      "2019-01-14T17:34:36.264320, step: 98, loss: 0.4884605407714844, acc: 0.7812, auc: 0.8994, precision: 0.9245, recall: 0.6712\n",
      "2019-01-14T17:34:36.487137, step: 99, loss: 0.40611034631729126, acc: 0.8594, auc: 0.9125, precision: 0.8971, recall: 0.8472\n",
      "2019-01-14T17:34:36.713555, step: 100, loss: 0.521208643913269, acc: 0.8125, auc: 0.8886, precision: 0.7808, recall: 0.8769\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:34:45.381201, step: 100, loss: 0.4630444095684932, acc: 0.8229128205128203, auc: 0.9003461538461537, precision: 0.8045461538461542, recall: 0.8557999999999998\n",
      "2019-01-14T17:34:45.594763, step: 101, loss: 0.35381412506103516, acc: 0.8594, auc: 0.9375, precision: 0.8421, recall: 0.8421\n",
      "2019-01-14T17:34:45.816020, step: 102, loss: 0.5819772481918335, acc: 0.7734, auc: 0.8544, precision: 0.7925, recall: 0.7\n",
      "2019-01-14T17:34:46.029822, step: 103, loss: 0.49502885341644287, acc: 0.7734, auc: 0.8961, precision: 0.9231, recall: 0.6575\n",
      "2019-01-14T17:34:46.245743, step: 104, loss: 0.42522257566452026, acc: 0.8047, auc: 0.8938, precision: 0.875, recall: 0.7313\n",
      "2019-01-14T17:34:46.464095, step: 105, loss: 0.43410366773605347, acc: 0.8203, auc: 0.9136, precision: 0.803, recall: 0.8413\n",
      "2019-01-14T17:34:46.686803, step: 106, loss: 0.44924473762512207, acc: 0.8047, auc: 0.9067, precision: 0.7385, recall: 0.8571\n",
      "2019-01-14T17:34:46.903066, step: 107, loss: 0.531976580619812, acc: 0.7891, auc: 0.877, precision: 0.8511, recall: 0.6667\n",
      "2019-01-14T17:34:47.118827, step: 108, loss: 0.500984787940979, acc: 0.7891, auc: 0.9076, precision: 0.9, recall: 0.5294\n",
      "2019-01-14T17:34:47.326978, step: 109, loss: 0.431339830160141, acc: 0.7969, auc: 0.8931, precision: 0.8793, recall: 0.7286\n",
      "2019-01-14T17:34:47.538686, step: 110, loss: 0.4187345802783966, acc: 0.8438, auc: 0.9073, precision: 0.8507, recall: 0.8507\n",
      "2019-01-14T17:34:47.751223, step: 111, loss: 0.71141117811203, acc: 0.7812, auc: 0.8501, precision: 0.6857, recall: 0.8889\n",
      "2019-01-14T17:34:47.956813, step: 112, loss: 0.4677707552909851, acc: 0.8047, auc: 0.8783, precision: 0.8254, recall: 0.7879\n",
      "2019-01-14T17:34:48.178879, step: 113, loss: 0.5345861911773682, acc: 0.7344, auc: 0.9097, precision: 0.9412, recall: 0.5\n",
      "2019-01-14T17:34:48.391336, step: 114, loss: 0.672788143157959, acc: 0.7188, auc: 0.8407, precision: 0.8298, recall: 0.5821\n",
      "2019-01-14T17:34:48.608301, step: 115, loss: 0.5315940976142883, acc: 0.7969, auc: 0.8811, precision: 0.7879, recall: 0.8125\n",
      "2019-01-14T17:34:48.824431, step: 116, loss: 0.6162027716636658, acc: 0.8281, auc: 0.919, precision: 0.7468, recall: 0.9672\n",
      "2019-01-14T17:34:49.039597, step: 117, loss: 0.3346644639968872, acc: 0.8672, auc: 0.9547, precision: 0.8235, recall: 0.918\n",
      "2019-01-14T17:34:49.252210, step: 118, loss: 0.5265946388244629, acc: 0.7344, auc: 0.9049, precision: 0.907, recall: 0.5652\n",
      "2019-01-14T17:34:49.469828, step: 119, loss: 0.5220078229904175, acc: 0.7812, auc: 0.9273, precision: 0.9189, recall: 0.5763\n",
      "2019-01-14T17:34:49.679979, step: 120, loss: 0.45068368315696716, acc: 0.8125, auc: 0.9038, precision: 0.8475, recall: 0.7692\n",
      "2019-01-14T17:34:49.888467, step: 121, loss: 0.5344073176383972, acc: 0.8125, auc: 0.8926, precision: 0.775, recall: 0.9118\n",
      "2019-01-14T17:34:50.129395, step: 122, loss: 0.3961050808429718, acc: 0.8516, auc: 0.9251, precision: 0.8289, recall: 0.913\n",
      "2019-01-14T17:34:50.339926, step: 123, loss: 0.3094085156917572, acc: 0.8438, auc: 0.9454, precision: 0.9091, recall: 0.8108\n",
      "2019-01-14T17:34:50.552148, step: 124, loss: 0.49137401580810547, acc: 0.8203, auc: 0.8889, precision: 0.8226, recall: 0.8095\n",
      "2019-01-14T17:34:50.765705, step: 125, loss: 0.3903071880340576, acc: 0.8281, auc: 0.9234, precision: 0.9, recall: 0.7714\n",
      "2019-01-14T17:34:50.975424, step: 126, loss: 0.38801324367523193, acc: 0.8438, auc: 0.9252, precision: 0.9259, recall: 0.7576\n",
      "2019-01-14T17:34:51.188002, step: 127, loss: 0.4112088978290558, acc: 0.8125, auc: 0.9121, precision: 0.8361, recall: 0.7846\n",
      "2019-01-14T17:34:51.400577, step: 128, loss: 0.4282545745372772, acc: 0.8359, auc: 0.9126, precision: 0.7846, recall: 0.8793\n",
      "2019-01-14T17:34:51.613188, step: 129, loss: 0.38969987630844116, acc: 0.8438, auc: 0.9261, precision: 0.8906, recall: 0.8143\n",
      "2019-01-14T17:34:51.824877, step: 130, loss: 0.5181295871734619, acc: 0.8203, auc: 0.8848, precision: 0.8246, recall: 0.7833\n",
      "2019-01-14T17:34:52.031572, step: 131, loss: 0.3266848921775818, acc: 0.875, auc: 0.9388, precision: 0.9344, recall: 0.8261\n",
      "2019-01-14T17:34:52.247581, step: 132, loss: 0.3733564019203186, acc: 0.8516, auc: 0.928, precision: 0.9091, recall: 0.7812\n",
      "2019-01-14T17:34:52.478860, step: 133, loss: 0.43575093150138855, acc: 0.8828, auc: 0.9118, precision: 0.9592, recall: 0.7833\n",
      "2019-01-14T17:34:52.689429, step: 134, loss: 0.4436745047569275, acc: 0.8281, auc: 0.9032, precision: 0.8548, recall: 0.803\n",
      "2019-01-14T17:34:52.904942, step: 135, loss: 0.41485831141471863, acc: 0.8438, auc: 0.9042, precision: 0.8472, recall: 0.8714\n",
      "2019-01-14T17:34:53.124046, step: 136, loss: 0.4830181896686554, acc: 0.8281, auc: 0.92, precision: 0.7647, recall: 0.8966\n",
      "2019-01-14T17:34:53.336681, step: 137, loss: 0.4263647198677063, acc: 0.8047, auc: 0.9102, precision: 0.8197, recall: 0.7812\n",
      "2019-01-14T17:34:53.551564, step: 138, loss: 0.4993011951446533, acc: 0.8203, auc: 0.9041, precision: 0.9074, recall: 0.7313\n",
      "2019-01-14T17:34:53.772222, step: 139, loss: 0.3340541124343872, acc: 0.8203, auc: 0.9528, precision: 0.9286, recall: 0.661\n",
      "2019-01-14T17:34:53.982096, step: 140, loss: 0.5098655223846436, acc: 0.7656, auc: 0.8826, precision: 0.8444, recall: 0.623\n",
      "2019-01-14T17:34:54.193130, step: 141, loss: 0.6690250039100647, acc: 0.7422, auc: 0.9305, precision: 0.6154, recall: 0.9412\n",
      "2019-01-14T17:34:54.413366, step: 142, loss: 0.38229110836982727, acc: 0.8203, auc: 0.918, precision: 0.8615, recall: 0.8\n",
      "2019-01-14T17:34:54.630818, step: 143, loss: 0.6683564186096191, acc: 0.7891, auc: 0.896, precision: 0.94, recall: 0.662\n",
      "2019-01-14T17:34:54.849014, step: 144, loss: 0.49518871307373047, acc: 0.8203, auc: 0.8963, precision: 0.8462, recall: 0.7458\n",
      "2019-01-14T17:34:55.064016, step: 145, loss: 0.5194368362426758, acc: 0.8359, auc: 0.8955, precision: 0.8143, recall: 0.8769\n",
      "2019-01-14T17:34:55.279248, step: 146, loss: 0.5815134048461914, acc: 0.7656, auc: 0.8725, precision: 0.7746, recall: 0.7971\n",
      "2019-01-14T17:34:55.494336, step: 147, loss: 0.3807854950428009, acc: 0.8672, auc: 0.9262, precision: 0.8367, recall: 0.82\n",
      "2019-01-14T17:34:55.710497, step: 148, loss: 0.45308953523635864, acc: 0.7734, auc: 0.9093, precision: 0.8605, recall: 0.6167\n",
      "2019-01-14T17:34:55.917301, step: 149, loss: 0.5768441557884216, acc: 0.7812, auc: 0.892, precision: 0.913, recall: 0.6364\n",
      "2019-01-14T17:34:56.123404, step: 150, loss: 0.46715205907821655, acc: 0.8438, auc: 0.9133, precision: 0.8116, recall: 0.8889\n",
      "2019-01-14T17:34:56.328151, step: 151, loss: 0.6177085041999817, acc: 0.8359, auc: 0.9234, precision: 0.7564, recall: 0.9672\n",
      "2019-01-14T17:34:56.540787, step: 152, loss: 0.3803462088108063, acc: 0.8516, auc: 0.9219, precision: 0.8689, recall: 0.8281\n",
      "2019-01-14T17:34:56.764221, step: 153, loss: 0.611513614654541, acc: 0.7969, auc: 0.8876, precision: 0.9286, recall: 0.629\n",
      "2019-01-14T17:34:56.982786, step: 154, loss: 0.7028308510780334, acc: 0.7812, auc: 0.8728, precision: 0.9024, recall: 0.6066\n",
      "2019-01-14T17:34:57.193544, step: 155, loss: 0.32956787943840027, acc: 0.8828, auc: 0.9417, precision: 0.9016, recall: 0.8594\n",
      "2019-01-14T17:34:57.408373, step: 156, loss: 0.4731489419937134, acc: 0.8125, auc: 0.895, precision: 0.8136, recall: 0.7869\n",
      "start training model\n",
      "2019-01-14T17:34:57.663197, step: 157, loss: 0.31135380268096924, acc: 0.9141, auc: 0.9766, precision: 0.8507, recall: 0.9828\n",
      "2019-01-14T17:34:57.865421, step: 158, loss: 0.19973352551460266, acc: 0.9375, auc: 0.99, precision: 0.8857, recall: 1.0\n",
      "2019-01-14T17:34:58.074196, step: 159, loss: 0.2442752718925476, acc: 0.8672, auc: 0.9702, precision: 0.9444, recall: 0.7846\n",
      "2019-01-14T17:34:58.294238, step: 160, loss: 0.2434619963169098, acc: 0.8984, auc: 0.9764, precision: 1.0, recall: 0.74\n",
      "2019-01-14T17:34:58.502127, step: 161, loss: 0.23784929513931274, acc: 0.8906, auc: 0.9716, precision: 0.9388, recall: 0.807\n",
      "2019-01-14T17:34:58.710917, step: 162, loss: 0.16526147723197937, acc: 0.9375, auc: 0.9824, precision: 0.9483, recall: 0.9167\n",
      "2019-01-14T17:34:58.930100, step: 163, loss: 0.2080225795507431, acc: 0.9453, auc: 0.9728, precision: 0.9571, recall: 0.9437\n",
      "2019-01-14T17:34:59.141963, step: 164, loss: 0.37752145528793335, acc: 0.8906, auc: 0.9605, precision: 0.8136, recall: 0.9412\n",
      "2019-01-14T17:34:59.355217, step: 165, loss: 0.23383468389511108, acc: 0.9375, auc: 0.9658, precision: 0.9844, recall: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:34:59.567687, step: 166, loss: 0.27281492948532104, acc: 0.8906, auc: 0.9668, precision: 0.9636, recall: 0.8154\n",
      "2019-01-14T17:34:59.778583, step: 167, loss: 0.32069113850593567, acc: 0.8828, auc: 0.9582, precision: 0.9643, recall: 0.806\n",
      "2019-01-14T17:34:59.989900, step: 168, loss: 0.24743615090847015, acc: 0.9141, auc: 0.9621, precision: 0.9375, recall: 0.8955\n",
      "2019-01-14T17:35:00.201378, step: 169, loss: 0.13683587312698364, acc: 0.9688, auc: 0.9899, precision: 0.9818, recall: 0.9474\n",
      "2019-01-14T17:35:00.416591, step: 170, loss: 0.32393941283226013, acc: 0.8828, auc: 0.9756, precision: 0.8228, recall: 0.9848\n",
      "2019-01-14T17:35:00.621977, step: 171, loss: 0.19676049053668976, acc: 0.8984, auc: 0.9779, precision: 0.8833, recall: 0.8983\n",
      "2019-01-14T17:35:00.855510, step: 172, loss: 0.2226095348596573, acc: 0.9062, auc: 0.9719, precision: 0.9661, recall: 0.8507\n",
      "2019-01-14T17:35:01.070063, step: 173, loss: 0.15410731732845306, acc: 0.9219, auc: 0.9868, precision: 0.9592, recall: 0.8545\n",
      "2019-01-14T17:35:01.286209, step: 174, loss: 0.22548189759254456, acc: 0.8828, auc: 0.9864, precision: 1.0, recall: 0.7368\n",
      "2019-01-14T17:35:01.504235, step: 175, loss: 0.2015531361103058, acc: 0.9141, auc: 0.977, precision: 0.9194, recall: 0.9048\n",
      "2019-01-14T17:35:01.711094, step: 176, loss: 0.25323614478111267, acc: 0.9375, auc: 0.9638, precision: 0.9464, recall: 0.9138\n",
      "2019-01-14T17:35:01.924753, step: 177, loss: 0.2548443675041199, acc: 0.9141, auc: 0.9712, precision: 0.8857, recall: 0.9538\n",
      "2019-01-14T17:35:02.135765, step: 178, loss: 0.1550300568342209, acc: 0.9609, auc: 0.987, precision: 0.9552, recall: 0.9697\n",
      "2019-01-14T17:35:02.352091, step: 179, loss: 0.17227712273597717, acc: 0.9141, auc: 0.9847, precision: 0.9683, recall: 0.8714\n",
      "2019-01-14T17:35:02.564075, step: 180, loss: 0.21517133712768555, acc: 0.8984, auc: 0.9736, precision: 0.918, recall: 0.875\n",
      "2019-01-14T17:35:02.780959, step: 181, loss: 0.1709791123867035, acc: 0.9453, auc: 0.9824, precision: 0.9831, recall: 0.9062\n",
      "2019-01-14T17:35:02.996220, step: 182, loss: 0.13477785885334015, acc: 0.9219, auc: 0.9902, precision: 0.9275, recall: 0.9275\n",
      "2019-01-14T17:35:03.208969, step: 183, loss: 0.1618732511997223, acc: 0.9453, auc: 0.9834, precision: 0.9672, recall: 0.9219\n",
      "2019-01-14T17:35:03.422784, step: 184, loss: 0.24130240082740784, acc: 0.8906, auc: 0.9676, precision: 0.8772, recall: 0.8772\n",
      "2019-01-14T17:35:03.634756, step: 185, loss: 0.18399548530578613, acc: 0.9453, auc: 0.9819, precision: 0.9254, recall: 0.9688\n",
      "2019-01-14T17:35:03.846729, step: 186, loss: 0.2007598876953125, acc: 0.9219, auc: 0.9764, precision: 0.9298, recall: 0.8983\n",
      "2019-01-14T17:35:04.057366, step: 187, loss: 0.23550313711166382, acc: 0.9141, auc: 0.9751, precision: 0.9583, recall: 0.8364\n",
      "2019-01-14T17:35:04.272269, step: 188, loss: 0.2773803174495697, acc: 0.8906, auc: 0.9662, precision: 0.9206, recall: 0.8657\n",
      "2019-01-14T17:35:04.486577, step: 189, loss: 0.18551059067249298, acc: 0.8984, auc: 0.9802, precision: 0.9355, recall: 0.8657\n",
      "2019-01-14T17:35:04.705463, step: 190, loss: 0.2532990574836731, acc: 0.9297, auc: 0.9666, precision: 0.9054, recall: 0.971\n",
      "2019-01-14T17:35:04.912289, step: 191, loss: 0.36934471130371094, acc: 0.8828, auc: 0.9467, precision: 0.8701, recall: 0.9306\n",
      "2019-01-14T17:35:05.130333, step: 192, loss: 0.28730812668800354, acc: 0.8984, auc: 0.9618, precision: 0.8814, recall: 0.8966\n",
      "2019-01-14T17:35:05.337778, step: 193, loss: 0.29988041520118713, acc: 0.8828, auc: 0.9578, precision: 0.9464, recall: 0.8154\n",
      "2019-01-14T17:35:05.556116, step: 194, loss: 0.3088115453720093, acc: 0.8906, auc: 0.9589, precision: 0.98, recall: 0.7903\n",
      "2019-01-14T17:35:05.771236, step: 195, loss: 0.11205402761697769, acc: 0.9453, auc: 0.9931, precision: 0.9464, recall: 0.9298\n",
      "2019-01-14T17:35:05.997851, step: 196, loss: 0.2442951798439026, acc: 0.9062, auc: 0.9666, precision: 0.9123, recall: 0.8814\n",
      "2019-01-14T17:35:06.216207, step: 197, loss: 0.22637727856636047, acc: 0.8906, auc: 0.9717, precision: 0.8923, recall: 0.8923\n",
      "2019-01-14T17:35:06.444081, step: 198, loss: 0.21392974257469177, acc: 0.8828, auc: 0.9751, precision: 0.9016, recall: 0.8594\n",
      "2019-01-14T17:35:06.658575, step: 199, loss: 0.3535953760147095, acc: 0.9219, auc: 0.9471, precision: 0.8676, recall: 0.9833\n",
      "2019-01-14T17:35:06.877488, step: 200, loss: 0.16960376501083374, acc: 0.9141, auc: 0.9874, precision: 0.9839, recall: 0.8592\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:35:15.492876, step: 200, loss: 0.39452621111503017, acc: 0.841146153846154, auc: 0.9253205128205128, precision: 0.8911666666666667, recall: 0.7800615384615385\n",
      "2019-01-14T17:35:15.705321, step: 201, loss: 0.23246383666992188, acc: 0.8906, auc: 0.9701, precision: 0.8868, recall: 0.8545\n",
      "2019-01-14T17:35:15.930820, step: 202, loss: 0.19199207425117493, acc: 0.9141, auc: 0.9763, precision: 0.9107, recall: 0.8947\n",
      "2019-01-14T17:35:16.148397, step: 203, loss: 0.09744621813297272, acc: 0.9688, auc: 0.996, precision: 0.9855, recall: 0.9577\n",
      "2019-01-14T17:35:16.362673, step: 204, loss: 0.1565905213356018, acc: 0.9297, auc: 0.9831, precision: 0.9385, recall: 0.9242\n",
      "2019-01-14T17:35:16.579435, step: 205, loss: 0.20595379173755646, acc: 0.9297, auc: 0.9753, precision: 0.9254, recall: 0.9394\n",
      "2019-01-14T17:35:16.809916, step: 206, loss: 0.16834816336631775, acc: 0.9297, auc: 0.9824, precision: 0.9365, recall: 0.9219\n",
      "2019-01-14T17:35:17.027720, step: 207, loss: 0.14039628207683563, acc: 0.9297, auc: 0.9885, precision: 0.9231, recall: 0.9375\n",
      "2019-01-14T17:35:17.255345, step: 208, loss: 0.19054360687732697, acc: 0.9219, auc: 0.9787, precision: 0.9559, recall: 0.9028\n",
      "2019-01-14T17:35:17.480982, step: 209, loss: 0.29434138536453247, acc: 0.8828, auc: 0.9645, precision: 0.9792, recall: 0.7705\n",
      "2019-01-14T17:35:17.707380, step: 210, loss: 0.23084506392478943, acc: 0.9062, auc: 0.9695, precision: 0.9231, recall: 0.8571\n",
      "2019-01-14T17:35:17.931557, step: 211, loss: 0.2789681553840637, acc: 0.8984, auc: 0.9633, precision: 0.8732, recall: 0.9394\n",
      "2019-01-14T17:35:18.145026, step: 212, loss: 0.20899717509746552, acc: 0.9219, auc: 0.9822, precision: 0.8793, recall: 0.9444\n",
      "2019-01-14T17:35:18.351398, step: 213, loss: 0.24545298516750336, acc: 0.8984, auc: 0.9621, precision: 0.9091, recall: 0.8621\n",
      "2019-01-14T17:35:18.577314, step: 214, loss: 0.2777306139469147, acc: 0.875, auc: 0.9734, precision: 0.9808, recall: 0.7727\n",
      "2019-01-14T17:35:18.784923, step: 215, loss: 0.22175826132297516, acc: 0.8984, auc: 0.9849, precision: 1.0, recall: 0.8\n",
      "2019-01-14T17:35:19.016080, step: 216, loss: 0.2109396904706955, acc: 0.9297, auc: 0.9737, precision: 0.9437, recall: 0.9306\n",
      "2019-01-14T17:35:19.241804, step: 217, loss: 0.2978970408439636, acc: 0.8906, auc: 0.9819, precision: 0.8354, recall: 0.9851\n",
      "2019-01-14T17:35:19.467161, step: 218, loss: 0.22033162415027618, acc: 0.9141, auc: 0.9839, precision: 0.8649, recall: 0.9846\n",
      "2019-01-14T17:35:19.680340, step: 219, loss: 0.22745421528816223, acc: 0.8984, auc: 0.9776, precision: 0.9792, recall: 0.7966\n",
      "2019-01-14T17:35:19.912312, step: 220, loss: 0.24759605526924133, acc: 0.875, auc: 0.9851, precision: 0.9811, recall: 0.7761\n",
      "2019-01-14T17:35:20.132127, step: 221, loss: 0.22543540596961975, acc: 0.8594, auc: 0.9839, precision: 1.0, recall: 0.7313\n",
      "2019-01-14T17:35:20.347229, step: 222, loss: 0.227876216173172, acc: 0.9297, auc: 0.9774, precision: 0.8947, recall: 0.9855\n",
      "2019-01-14T17:35:20.572912, step: 223, loss: 0.24647200107574463, acc: 0.9141, auc: 0.9807, precision: 0.8846, recall: 0.9718\n",
      "2019-01-14T17:35:20.795449, step: 224, loss: 0.15497159957885742, acc: 0.9375, auc: 0.9895, precision: 0.9403, recall: 0.9403\n",
      "2019-01-14T17:35:21.014387, step: 225, loss: 0.1513642817735672, acc: 0.9453, auc: 0.9874, precision: 0.9444, recall: 0.9577\n",
      "2019-01-14T17:35:21.248566, step: 226, loss: 0.150167778134346, acc: 0.9453, auc: 0.9867, precision: 0.9796, recall: 0.8889\n",
      "2019-01-14T17:35:21.474825, step: 227, loss: 0.31295767426490784, acc: 0.8281, auc: 0.9819, precision: 1.0, recall: 0.6667\n",
      "2019-01-14T17:35:21.694153, step: 228, loss: 0.14744310081005096, acc: 0.9297, auc: 0.9875, precision: 0.9565, recall: 0.8627\n",
      "2019-01-14T17:35:21.939191, step: 229, loss: 0.22128254175186157, acc: 0.9375, auc: 0.9828, precision: 0.8939, recall: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:35:22.164517, step: 230, loss: 0.27866417169570923, acc: 0.8906, auc: 0.9768, precision: 0.8507, recall: 0.9344\n",
      "2019-01-14T17:35:22.389206, step: 231, loss: 0.23549692332744598, acc: 0.9219, auc: 0.9707, precision: 0.9231, recall: 0.9231\n",
      "2019-01-14T17:35:22.600194, step: 232, loss: 0.15705811977386475, acc: 0.9297, auc: 0.9916, precision: 1.0, recall: 0.8448\n",
      "2019-01-14T17:35:22.827012, step: 233, loss: 0.32812827825546265, acc: 0.8516, auc: 0.969, precision: 0.9762, recall: 0.6949\n",
      "2019-01-14T17:35:23.045232, step: 234, loss: 0.30672717094421387, acc: 0.8594, auc: 0.9506, precision: 0.9153, recall: 0.806\n",
      "2019-01-14T17:35:23.271893, step: 235, loss: 0.24966078996658325, acc: 0.9141, auc: 0.9748, precision: 0.8769, recall: 0.95\n",
      "2019-01-14T17:35:23.503590, step: 236, loss: 0.2129371613264084, acc: 0.9297, auc: 0.9793, precision: 0.9067, recall: 0.9714\n",
      "2019-01-14T17:35:23.730391, step: 237, loss: 0.4337489902973175, acc: 0.8516, auc: 0.9438, precision: 0.7973, recall: 0.9365\n",
      "2019-01-14T17:35:23.947383, step: 238, loss: 0.23688751459121704, acc: 0.8828, auc: 0.9687, precision: 0.9333, recall: 0.8358\n",
      "2019-01-14T17:35:24.152879, step: 239, loss: 0.4740496277809143, acc: 0.8125, auc: 0.9716, precision: 1.0, recall: 0.6418\n",
      "2019-01-14T17:35:24.378319, step: 240, loss: 0.2891364097595215, acc: 0.8672, auc: 0.9626, precision: 0.9375, recall: 0.8219\n",
      "2019-01-14T17:35:24.604028, step: 241, loss: 0.34801194071769714, acc: 0.8828, auc: 0.9626, precision: 0.8529, recall: 0.9206\n",
      "2019-01-14T17:35:24.830871, step: 242, loss: 0.4732140898704529, acc: 0.8594, auc: 0.9489, precision: 0.8, recall: 0.9697\n",
      "2019-01-14T17:35:25.058814, step: 243, loss: 0.21960853040218353, acc: 0.9453, auc: 0.9746, precision: 0.9394, recall: 0.9538\n",
      "2019-01-14T17:35:25.301010, step: 244, loss: 0.19817468523979187, acc: 0.9297, auc: 0.9824, precision: 0.9828, recall: 0.8769\n",
      "2019-01-14T17:35:25.541746, step: 245, loss: 0.22830770909786224, acc: 0.8906, auc: 0.9919, precision: 1.0, recall: 0.8182\n",
      "2019-01-14T17:35:25.796523, step: 246, loss: 0.16059890389442444, acc: 0.9375, auc: 0.9902, precision: 1.0, recall: 0.8689\n",
      "2019-01-14T17:35:26.034161, step: 247, loss: 0.23606175184249878, acc: 0.8984, auc: 0.9714, precision: 0.8769, recall: 0.9194\n",
      "2019-01-14T17:35:26.264069, step: 248, loss: 0.12186639755964279, acc: 0.9688, auc: 0.9941, precision: 0.9718, recall: 0.9718\n",
      "2019-01-14T17:35:26.478664, step: 249, loss: 0.19814854860305786, acc: 0.9219, auc: 0.979, precision: 0.9091, recall: 0.9375\n",
      "2019-01-14T17:35:26.703227, step: 250, loss: 0.2282620668411255, acc: 0.9297, auc: 0.9707, precision: 0.9231, recall: 0.9375\n",
      "2019-01-14T17:35:26.945289, step: 251, loss: 0.17316359281539917, acc: 0.9297, auc: 0.9831, precision: 0.9677, recall: 0.8955\n",
      "2019-01-14T17:35:27.161486, step: 252, loss: 0.22157585620880127, acc: 0.8516, auc: 0.9823, precision: 0.9655, recall: 0.7671\n",
      "2019-01-14T17:35:27.380543, step: 253, loss: 0.22601041197776794, acc: 0.8672, auc: 0.9734, precision: 0.9492, recall: 0.8\n",
      "2019-01-14T17:35:27.614380, step: 254, loss: 0.18340085446834564, acc: 0.9297, auc: 0.9883, precision: 0.8784, recall: 1.0\n",
      "2019-01-14T17:35:27.844698, step: 255, loss: 0.25712525844573975, acc: 0.8828, auc: 0.9819, precision: 0.831, recall: 0.9516\n",
      "2019-01-14T17:35:28.071208, step: 256, loss: 0.1769627332687378, acc: 0.9297, auc: 0.9801, precision: 0.9474, recall: 0.9\n",
      "2019-01-14T17:35:28.284353, step: 257, loss: 0.2449391633272171, acc: 0.9297, auc: 0.9687, precision: 0.9524, recall: 0.9091\n",
      "2019-01-14T17:35:28.534487, step: 258, loss: 0.3205238878726959, acc: 0.875, auc: 0.9566, precision: 0.9828, recall: 0.7917\n",
      "2019-01-14T17:35:28.762223, step: 259, loss: 0.15727965533733368, acc: 0.9531, auc: 0.9848, precision: 0.9701, recall: 0.942\n",
      "2019-01-14T17:35:28.985063, step: 260, loss: 0.22754043340682983, acc: 0.875, auc: 0.97, precision: 0.8919, recall: 0.8919\n",
      "2019-01-14T17:35:29.198213, step: 261, loss: 0.20783159136772156, acc: 0.9141, auc: 0.9763, precision: 0.9344, recall: 0.8906\n",
      "2019-01-14T17:35:29.428530, step: 262, loss: 0.16482248902320862, acc: 0.9297, auc: 0.9902, precision: 0.8814, recall: 0.963\n",
      "2019-01-14T17:35:29.670100, step: 263, loss: 0.21745741367340088, acc: 0.9219, auc: 0.9807, precision: 0.9014, recall: 0.9552\n",
      "2019-01-14T17:35:29.904460, step: 264, loss: 0.13941293954849243, acc: 0.9219, auc: 0.9923, precision: 0.9859, recall: 0.8861\n",
      "2019-01-14T17:35:30.131464, step: 265, loss: 0.28016141057014465, acc: 0.8906, auc: 0.9705, precision: 0.9787, recall: 0.7797\n",
      "2019-01-14T17:35:30.372395, step: 266, loss: 0.10477636754512787, acc: 0.9531, auc: 0.9934, precision: 0.9692, recall: 0.9403\n",
      "2019-01-14T17:35:30.589023, step: 267, loss: 0.20753838121891022, acc: 0.9297, auc: 0.9719, precision: 0.9492, recall: 0.9032\n",
      "2019-01-14T17:35:30.848893, step: 268, loss: 0.14867860078811646, acc: 0.9609, auc: 0.9854, precision: 0.9861, recall: 0.9467\n",
      "2019-01-14T17:35:31.085117, step: 269, loss: 0.2614808976650238, acc: 0.9219, auc: 0.9747, precision: 0.8833, recall: 0.9464\n",
      "2019-01-14T17:35:31.305953, step: 270, loss: 0.10528405010700226, acc: 0.9766, auc: 0.998, precision: 0.9683, recall: 0.9839\n",
      "2019-01-14T17:35:31.531578, step: 271, loss: 0.21634969115257263, acc: 0.9062, auc: 0.977, precision: 0.98, recall: 0.8167\n",
      "2019-01-14T17:35:31.759435, step: 272, loss: 0.3973201811313629, acc: 0.8438, auc: 0.955, precision: 0.9792, recall: 0.7121\n",
      "2019-01-14T17:35:32.000494, step: 273, loss: 0.18879935145378113, acc: 0.9375, auc: 0.9785, precision: 0.9649, recall: 0.9016\n",
      "2019-01-14T17:35:32.215959, step: 274, loss: 0.28476443886756897, acc: 0.8984, auc: 0.9743, precision: 0.8393, recall: 0.9216\n",
      "2019-01-14T17:35:32.437660, step: 275, loss: 0.27284303307533264, acc: 0.9141, auc: 0.9748, precision: 0.8676, recall: 0.9672\n",
      "2019-01-14T17:35:32.673276, step: 276, loss: 0.1973479837179184, acc: 0.9141, auc: 0.9805, precision: 0.9846, recall: 0.8649\n",
      "2019-01-14T17:35:32.893083, step: 277, loss: 0.2477113902568817, acc: 0.9141, auc: 0.9748, precision: 0.963, recall: 0.8525\n",
      "2019-01-14T17:35:33.122916, step: 278, loss: 0.11560734361410141, acc: 0.9531, auc: 0.9945, precision: 0.9808, recall: 0.9107\n",
      "2019-01-14T17:35:33.348731, step: 279, loss: 0.25792989134788513, acc: 0.8672, auc: 0.9732, precision: 0.9672, recall: 0.7973\n",
      "2019-01-14T17:35:33.575900, step: 280, loss: 0.24802018702030182, acc: 0.9141, auc: 0.9743, precision: 0.8769, recall: 0.95\n",
      "2019-01-14T17:35:33.799221, step: 281, loss: 0.21507154405117035, acc: 0.9141, auc: 0.9826, precision: 0.8806, recall: 0.9516\n",
      "2019-01-14T17:35:34.035141, step: 282, loss: 0.1881689876317978, acc: 0.9141, auc: 0.9907, precision: 0.8714, recall: 0.9683\n",
      "2019-01-14T17:35:34.266878, step: 283, loss: 0.23303073644638062, acc: 0.8906, auc: 0.9761, precision: 0.9623, recall: 0.8095\n",
      "2019-01-14T17:35:34.491149, step: 284, loss: 0.3145734667778015, acc: 0.8672, auc: 0.9681, precision: 0.9375, recall: 0.7627\n",
      "2019-01-14T17:35:34.724597, step: 285, loss: 0.3487764000892639, acc: 0.8672, auc: 0.959, precision: 0.9538, recall: 0.8158\n",
      "2019-01-14T17:35:34.962611, step: 286, loss: 0.09836843609809875, acc: 0.9531, auc: 0.9955, precision: 0.9459, recall: 0.9722\n",
      "2019-01-14T17:35:35.189589, step: 287, loss: 0.30330416560173035, acc: 0.9141, auc: 0.9755, precision: 0.8571, recall: 0.9836\n",
      "2019-01-14T17:35:35.416508, step: 288, loss: 0.19181063771247864, acc: 0.9141, auc: 0.9777, precision: 0.9385, recall: 0.8971\n",
      "2019-01-14T17:35:35.639871, step: 289, loss: 0.17607323825359344, acc: 0.9375, auc: 0.9807, precision: 0.9275, recall: 0.9552\n",
      "2019-01-14T17:35:35.860533, step: 290, loss: 0.2033352553844452, acc: 0.9297, auc: 0.9844, precision: 1.0, recall: 0.875\n",
      "2019-01-14T17:35:36.085698, step: 291, loss: 0.20917782187461853, acc: 0.9219, auc: 0.9733, precision: 0.931, recall: 0.9\n",
      "2019-01-14T17:35:36.311419, step: 292, loss: 0.19596047699451447, acc: 0.9141, auc: 0.9778, precision: 0.9206, recall: 0.9062\n",
      "2019-01-14T17:35:36.532810, step: 293, loss: 0.21030348539352417, acc: 0.9219, auc: 0.9717, precision: 0.9667, recall: 0.8788\n",
      "2019-01-14T17:35:36.760782, step: 294, loss: 0.31890225410461426, acc: 0.875, auc: 0.9526, precision: 0.8596, recall: 0.8596\n",
      "2019-01-14T17:35:36.977778, step: 295, loss: 0.17063117027282715, acc: 0.9531, auc: 0.979, precision: 1.0, recall: 0.9155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:35:37.216216, step: 296, loss: 0.22737637162208557, acc: 0.9219, auc: 0.9701, precision: 0.9245, recall: 0.8909\n",
      "2019-01-14T17:35:37.443801, step: 297, loss: 0.29841285943984985, acc: 0.875, auc: 0.9563, precision: 0.8841, recall: 0.8841\n",
      "2019-01-14T17:35:37.664986, step: 298, loss: 0.30419236421585083, acc: 0.8594, auc: 0.9526, precision: 0.8421, recall: 0.8421\n",
      "2019-01-14T17:35:37.907117, step: 299, loss: 0.29996275901794434, acc: 0.8906, auc: 0.9553, precision: 0.8923, recall: 0.8923\n",
      "2019-01-14T17:35:38.126728, step: 300, loss: 0.22176508605480194, acc: 0.8906, auc: 0.9791, precision: 0.9592, recall: 0.7966\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:35:46.811093, step: 300, loss: 0.43820246519186556, acc: 0.8351333333333332, auc: 0.9203538461538461, precision: 0.8778794871794869, recall: 0.783448717948718\n",
      "2019-01-14T17:35:47.027540, step: 301, loss: 0.17701920866966248, acc: 0.9141, auc: 0.9814, precision: 0.9667, recall: 0.8657\n",
      "2019-01-14T17:35:47.233317, step: 302, loss: 0.18148952722549438, acc: 0.9453, auc: 0.981, precision: 0.9516, recall: 0.9365\n",
      "2019-01-14T17:35:47.465248, step: 303, loss: 0.17905551195144653, acc: 0.9219, auc: 0.9808, precision: 0.9298, recall: 0.8983\n",
      "2019-01-14T17:35:47.699651, step: 304, loss: 0.23518985509872437, acc: 0.9219, auc: 0.9721, precision: 0.9265, recall: 0.9265\n",
      "2019-01-14T17:35:47.936989, step: 305, loss: 0.1413014978170395, acc: 0.9609, auc: 0.9865, precision: 0.9365, recall: 0.9833\n",
      "2019-01-14T17:35:48.152267, step: 306, loss: 0.2005932480096817, acc: 0.9375, auc: 0.9786, precision: 0.9608, recall: 0.8909\n",
      "2019-01-14T17:35:48.386733, step: 307, loss: 0.35987526178359985, acc: 0.8594, auc: 0.9629, precision: 0.9792, recall: 0.7344\n",
      "2019-01-14T17:35:48.604256, step: 308, loss: 0.17169849574565887, acc: 0.9609, auc: 0.979, precision: 0.9667, recall: 0.9508\n",
      "2019-01-14T17:35:48.826887, step: 309, loss: 0.23926225304603577, acc: 0.9141, auc: 0.98, precision: 0.8852, recall: 0.931\n",
      "2019-01-14T17:35:49.045935, step: 310, loss: 0.1981719583272934, acc: 0.9062, auc: 0.9814, precision: 0.8841, recall: 0.9385\n",
      "2019-01-14T17:35:49.270008, step: 311, loss: 0.3311704099178314, acc: 0.9062, auc: 0.9466, precision: 0.875, recall: 0.9333\n",
      "2019-01-14T17:35:49.498021, step: 312, loss: 0.3822133243083954, acc: 0.8281, auc: 0.9715, precision: 0.963, recall: 0.7222\n",
      "start training model\n",
      "2019-01-14T17:35:49.747214, step: 313, loss: 0.12813016772270203, acc: 0.9375, auc: 0.9923, precision: 0.9796, recall: 0.8727\n",
      "2019-01-14T17:35:49.972148, step: 314, loss: 0.07080016285181046, acc: 0.9844, auc: 0.9983, precision: 0.9848, recall: 0.9848\n",
      "2019-01-14T17:35:50.202710, step: 315, loss: 0.09464359283447266, acc: 0.9609, auc: 0.997, precision: 0.96, recall: 0.973\n",
      "2019-01-14T17:35:50.430255, step: 316, loss: 0.273458331823349, acc: 0.9141, auc: 0.9948, precision: 0.8333, recall: 1.0\n",
      "2019-01-14T17:35:50.659283, step: 317, loss: 0.1519726663827896, acc: 0.9531, auc: 0.9822, precision: 0.9667, recall: 0.9355\n",
      "2019-01-14T17:35:50.881246, step: 318, loss: 0.08651642501354218, acc: 0.9453, auc: 0.9995, precision: 1.0, recall: 0.8833\n",
      "2019-01-14T17:35:51.100612, step: 319, loss: 0.06788915395736694, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9306\n",
      "2019-01-14T17:35:51.324993, step: 320, loss: 0.1102333590388298, acc: 0.9453, auc: 0.9963, precision: 0.9848, recall: 0.9155\n",
      "2019-01-14T17:35:51.546936, step: 321, loss: 0.053860560059547424, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:35:51.768126, step: 322, loss: 0.06988215446472168, acc: 0.9766, auc: 0.998, precision: 0.9821, recall: 0.9649\n",
      "2019-01-14T17:35:51.993615, step: 323, loss: 0.06452228128910065, acc: 0.9766, auc: 0.9993, precision: 0.9839, recall: 0.9683\n",
      "2019-01-14T17:35:52.223076, step: 324, loss: 0.06452086567878723, acc: 0.9844, auc: 0.9987, precision: 0.9737, recall: 1.0\n",
      "2019-01-14T17:35:52.462679, step: 325, loss: 0.04996771737933159, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:35:52.699767, step: 326, loss: 0.04357115551829338, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:35:52.954231, step: 327, loss: 0.07752901315689087, acc: 0.9375, auc: 0.9987, precision: 1.0, recall: 0.8933\n",
      "2019-01-14T17:35:53.201107, step: 328, loss: 0.06339848041534424, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9333\n",
      "2019-01-14T17:35:53.426627, step: 329, loss: 0.07462763786315918, acc: 0.9688, auc: 0.9979, precision: 0.9792, recall: 0.94\n",
      "2019-01-14T17:35:53.666524, step: 330, loss: 0.043198198080062866, acc: 0.9922, auc: 0.999, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:35:53.905513, step: 331, loss: 0.04763685539364815, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9444\n",
      "2019-01-14T17:35:54.132855, step: 332, loss: 0.04952932894229889, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9538\n",
      "2019-01-14T17:35:54.353739, step: 333, loss: 0.07276495546102524, acc: 0.9688, auc: 0.998, precision: 0.9524, recall: 0.9836\n",
      "2019-01-14T17:35:54.585083, step: 334, loss: 0.1154136061668396, acc: 0.9531, auc: 0.9907, precision: 1.0, recall: 0.9062\n",
      "2019-01-14T17:35:54.805936, step: 335, loss: 0.04093330353498459, acc: 0.9922, auc: 0.9998, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:35:55.027945, step: 336, loss: 0.056713178753852844, acc: 0.9844, auc: 0.9971, precision: 1.0, recall: 0.9697\n",
      "2019-01-14T17:35:55.246069, step: 337, loss: 0.12485431134700775, acc: 0.9531, auc: 0.9926, precision: 0.9355, recall: 0.9667\n",
      "2019-01-14T17:35:55.457506, step: 338, loss: 0.06720400601625443, acc: 0.9766, auc: 0.9965, precision: 0.9857, recall: 0.9718\n",
      "2019-01-14T17:35:55.677649, step: 339, loss: 0.03520979359745979, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-14T17:35:55.885605, step: 340, loss: 0.025593256577849388, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:35:56.099632, step: 341, loss: 0.02597939781844616, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:35:56.326144, step: 342, loss: 0.032734207808971405, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2019-01-14T17:35:56.544064, step: 343, loss: 0.028921741992235184, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:35:56.762342, step: 344, loss: 0.09536626935005188, acc: 0.9531, auc: 0.9954, precision: 0.9516, recall: 0.9516\n",
      "2019-01-14T17:35:56.980900, step: 345, loss: 0.09476377815008163, acc: 0.9766, auc: 0.9907, precision: 0.9853, recall: 0.971\n",
      "2019-01-14T17:35:57.206158, step: 346, loss: 0.05936975032091141, acc: 0.9844, auc: 0.9998, precision: 0.9726, recall: 1.0\n",
      "2019-01-14T17:35:57.421441, step: 347, loss: 0.059904925525188446, acc: 0.9688, auc: 0.9982, precision: 0.9868, recall: 0.9615\n",
      "2019-01-14T17:35:57.641307, step: 348, loss: 0.033939506858587265, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:35:57.864534, step: 349, loss: 0.043383922427892685, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9839\n",
      "2019-01-14T17:35:58.088502, step: 350, loss: 0.02418471872806549, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-14T17:35:58.309139, step: 351, loss: 0.074655681848526, acc: 0.9609, auc: 0.998, precision: 1.0, recall: 0.9242\n",
      "2019-01-14T17:35:58.523909, step: 352, loss: 0.054707083851099014, acc: 0.9844, auc: 0.9983, precision: 1.0, recall: 0.9677\n",
      "2019-01-14T17:35:58.736333, step: 353, loss: 0.07025276124477386, acc: 0.9609, auc: 0.9968, precision: 0.9811, recall: 0.9286\n",
      "2019-01-14T17:35:58.967170, step: 354, loss: 0.12202979624271393, acc: 0.9453, auc: 0.9907, precision: 0.9231, recall: 0.9677\n",
      "2019-01-14T17:35:59.177128, step: 355, loss: 0.07113136351108551, acc: 0.9844, auc: 0.9963, precision: 0.9661, recall: 1.0\n",
      "2019-01-14T17:35:59.393760, step: 356, loss: 0.08024196326732635, acc: 0.9844, auc: 0.9963, precision: 0.9688, recall: 1.0\n",
      "2019-01-14T17:35:59.625363, step: 357, loss: 0.05667015537619591, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.918\n",
      "2019-01-14T17:35:59.851959, step: 358, loss: 0.12590567767620087, acc: 0.9453, auc: 0.9932, precision: 0.9828, recall: 0.9048\n",
      "2019-01-14T17:36:00.065297, step: 359, loss: 0.052903205156326294, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9718\n",
      "2019-01-14T17:36:00.279042, step: 360, loss: 0.050358083099126816, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:36:00.502408, step: 361, loss: 0.048521749675273895, acc: 0.9844, auc: 1.0, precision: 0.9701, recall: 1.0\n",
      "2019-01-14T17:36:00.714955, step: 362, loss: 0.10525567829608917, acc: 0.9688, auc: 0.9953, precision: 0.9483, recall: 0.9821\n",
      "2019-01-14T17:36:00.938258, step: 363, loss: 0.0665103867650032, acc: 0.9688, auc: 0.9978, precision: 0.9672, recall: 0.9672\n",
      "2019-01-14T17:36:01.154323, step: 364, loss: 0.0370599739253521, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9828\n",
      "2019-01-14T17:36:01.384744, step: 365, loss: 0.05357800051569939, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9324\n",
      "2019-01-14T17:36:01.607338, step: 366, loss: 0.06245076283812523, acc: 0.9688, auc: 0.9988, precision: 1.0, recall: 0.9444\n",
      "2019-01-14T17:36:01.827175, step: 367, loss: 0.0346212163567543, acc: 0.9922, auc: 1.0, precision: 0.9857, recall: 1.0\n",
      "2019-01-14T17:36:02.038006, step: 368, loss: 0.06015198677778244, acc: 0.9922, auc: 0.999, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:36:02.257852, step: 369, loss: 0.06545299291610718, acc: 0.9609, auc: 0.998, precision: 0.9692, recall: 0.9545\n",
      "2019-01-14T17:36:02.477201, step: 370, loss: 0.06678098440170288, acc: 0.9844, auc: 0.999, precision: 0.9667, recall: 1.0\n",
      "2019-01-14T17:36:02.695030, step: 371, loss: 0.034477293491363525, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-01-14T17:36:02.916060, step: 372, loss: 0.07786931842565536, acc: 0.9688, auc: 0.9973, precision: 1.0, recall: 0.9333\n",
      "2019-01-14T17:36:03.134844, step: 373, loss: 0.0665704607963562, acc: 0.9609, auc: 0.9998, precision: 1.0, recall: 0.918\n",
      "2019-01-14T17:36:03.350726, step: 374, loss: 0.0675959438085556, acc: 0.9609, auc: 0.999, precision: 1.0, recall: 0.9167\n",
      "2019-01-14T17:36:03.562161, step: 375, loss: 0.03902469575405121, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:36:03.776796, step: 376, loss: 0.1065550297498703, acc: 0.9766, auc: 0.9961, precision: 0.9545, recall: 1.0\n",
      "2019-01-14T17:36:03.985152, step: 377, loss: 0.04988478869199753, acc: 0.9922, auc: 0.9995, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:36:04.197621, step: 378, loss: 0.09091279655694962, acc: 0.9766, auc: 0.9956, precision: 0.9531, recall: 1.0\n",
      "2019-01-14T17:36:04.411441, step: 379, loss: 0.05256054177880287, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9429\n",
      "2019-01-14T17:36:04.625597, step: 380, loss: 0.056690413504838943, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9545\n",
      "2019-01-14T17:36:04.841718, step: 381, loss: 0.0856478363275528, acc: 0.9531, auc: 1.0, precision: 1.0, recall: 0.9032\n",
      "2019-01-14T17:36:05.056443, step: 382, loss: 0.04214564338326454, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9589\n",
      "2019-01-14T17:36:05.263646, step: 383, loss: 0.07636094093322754, acc: 0.9688, auc: 0.9995, precision: 0.9394, recall: 1.0\n",
      "2019-01-14T17:36:05.479968, step: 384, loss: 0.09261120855808258, acc: 0.9844, auc: 0.9985, precision: 0.9688, recall: 1.0\n",
      "2019-01-14T17:36:05.696129, step: 385, loss: 0.04751472920179367, acc: 0.9922, auc: 0.9985, precision: 0.9853, recall: 1.0\n",
      "2019-01-14T17:36:05.916962, step: 386, loss: 0.055424973368644714, acc: 0.9688, auc: 0.9976, precision: 0.9839, recall: 0.9531\n",
      "2019-01-14T17:36:06.125620, step: 387, loss: 0.10171855241060257, acc: 0.9766, auc: 0.9914, precision: 1.0, recall: 0.9516\n",
      "2019-01-14T17:36:06.327597, step: 388, loss: 0.08876864612102509, acc: 0.9453, auc: 1.0, precision: 1.0, recall: 0.8986\n",
      "2019-01-14T17:36:06.556024, step: 389, loss: 0.06929653137922287, acc: 0.9766, auc: 0.9978, precision: 1.0, recall: 0.9531\n",
      "2019-01-14T17:36:06.773424, step: 390, loss: 0.09709125012159348, acc: 0.9609, auc: 0.9952, precision: 0.96, recall: 0.973\n",
      "2019-01-14T17:36:06.983916, step: 391, loss: 0.1257961243391037, acc: 0.9688, auc: 0.9956, precision: 0.9412, recall: 1.0\n",
      "2019-01-14T17:36:07.198648, step: 392, loss: 0.1666758954524994, acc: 0.9609, auc: 0.9914, precision: 0.9355, recall: 0.9831\n",
      "2019-01-14T17:36:07.408332, step: 393, loss: 0.05248052999377251, acc: 0.9766, auc: 0.999, precision: 0.9859, recall: 0.9722\n",
      "2019-01-14T17:36:07.617989, step: 394, loss: 0.09248687326908112, acc: 0.9688, auc: 0.9978, precision: 1.0, recall: 0.9394\n",
      "2019-01-14T17:36:07.830446, step: 395, loss: 0.03687915951013565, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2019-01-14T17:36:08.037310, step: 396, loss: 0.02220071107149124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:08.259064, step: 397, loss: 0.08265340328216553, acc: 0.9766, auc: 0.9951, precision: 1.0, recall: 0.9492\n",
      "2019-01-14T17:36:08.483410, step: 398, loss: 0.06526345759630203, acc: 0.9688, auc: 0.9976, precision: 0.9677, recall: 0.9677\n",
      "2019-01-14T17:36:08.726498, step: 399, loss: 0.05960792675614357, acc: 0.9844, auc: 0.9985, precision: 0.9851, recall: 0.9851\n",
      "2019-01-14T17:36:08.971549, step: 400, loss: 0.03966802731156349, acc: 0.9766, auc: 0.9995, precision: 0.9831, recall: 0.9667\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:36:17.434964, step: 400, loss: 0.43722602572196567, acc: 0.8575769230769231, auc: 0.932702564102564, precision: 0.8631641025641026, recall: 0.8519512820512821\n",
      "2019-01-14T17:36:17.649495, step: 401, loss: 0.04227972403168678, acc: 0.9844, auc: 0.9998, precision: 0.9853, recall: 0.9853\n",
      "2019-01-14T17:36:17.872471, step: 402, loss: 0.03274862840771675, acc: 0.9766, auc: 0.9995, precision: 0.9833, recall: 0.9672\n",
      "2019-01-14T17:36:18.093200, step: 403, loss: 0.048380907624959946, acc: 0.9766, auc: 0.9987, precision: 0.9863, recall: 0.973\n",
      "2019-01-14T17:36:18.315080, step: 404, loss: 0.08299680054187775, acc: 0.9453, auc: 0.9963, precision: 0.9815, recall: 0.8983\n",
      "2019-01-14T17:36:18.525310, step: 405, loss: 0.07968581467866898, acc: 0.9766, auc: 0.9974, precision: 0.9608, recall: 0.98\n",
      "2019-01-14T17:36:18.745191, step: 406, loss: 0.04115137457847595, acc: 0.9922, auc: 0.9995, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:36:18.970279, step: 407, loss: 0.0459640808403492, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9692\n",
      "2019-01-14T17:36:19.181326, step: 408, loss: 0.07936806976795197, acc: 0.9609, auc: 0.9968, precision: 0.9828, recall: 0.9344\n",
      "2019-01-14T17:36:19.414313, step: 409, loss: 0.03316064178943634, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:36:19.644454, step: 410, loss: 0.06089361757040024, acc: 0.9844, auc: 0.9985, precision: 0.971, recall: 1.0\n",
      "2019-01-14T17:36:19.879812, step: 411, loss: 0.08491959422826767, acc: 0.9609, auc: 0.9954, precision: 0.9692, recall: 0.9545\n",
      "2019-01-14T17:36:20.095357, step: 412, loss: 0.03618820011615753, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:36:20.310851, step: 413, loss: 0.02785200998187065, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:20.523242, step: 414, loss: 0.055027350783348083, acc: 0.9766, auc: 0.9985, precision: 0.9661, recall: 0.9828\n",
      "2019-01-14T17:36:20.740549, step: 415, loss: 0.09138868749141693, acc: 0.9531, auc: 0.997, precision: 0.9857, recall: 0.9324\n",
      "2019-01-14T17:36:20.958295, step: 416, loss: 0.07543663680553436, acc: 0.9688, auc: 0.9971, precision: 0.9841, recall: 0.9538\n",
      "2019-01-14T17:36:21.193017, step: 417, loss: 0.04147762060165405, acc: 0.9766, auc: 0.9993, precision: 0.9844, recall: 0.9692\n",
      "2019-01-14T17:36:21.405518, step: 418, loss: 0.12074768543243408, acc: 0.9531, auc: 0.9956, precision: 0.9324, recall: 0.9857\n",
      "2019-01-14T17:36:21.635582, step: 419, loss: 0.04951052367687225, acc: 0.9922, auc: 0.9963, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:36:21.863917, step: 420, loss: 0.09917580336332321, acc: 0.9609, auc: 0.9944, precision: 0.9844, recall: 0.9403\n",
      "2019-01-14T17:36:22.097539, step: 421, loss: 0.11854653060436249, acc: 0.9609, auc: 0.996, precision: 1.0, recall: 0.9306\n",
      "2019-01-14T17:36:22.326812, step: 422, loss: 0.07952292263507843, acc: 0.9609, auc: 0.9971, precision: 1.0, recall: 0.9242\n",
      "2019-01-14T17:36:22.534426, step: 423, loss: 0.1001187264919281, acc: 0.9688, auc: 0.9975, precision: 0.9487, recall: 1.0\n",
      "2019-01-14T17:36:22.771753, step: 424, loss: 0.08848593384027481, acc: 0.9844, auc: 0.9998, precision: 0.9692, recall: 1.0\n",
      "2019-01-14T17:36:22.999783, step: 425, loss: 0.035177551209926605, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9831\n",
      "2019-01-14T17:36:23.210179, step: 426, loss: 0.05378379672765732, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:36:23.420983, step: 427, loss: 0.0992620438337326, acc: 0.9688, auc: 0.9934, precision: 1.0, recall: 0.9344\n",
      "2019-01-14T17:36:23.657146, step: 428, loss: 0.0684405118227005, acc: 0.9688, auc: 0.9988, precision: 1.0, recall: 0.9333\n",
      "2019-01-14T17:36:23.877556, step: 429, loss: 0.052659809589385986, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2019-01-14T17:36:24.107158, step: 430, loss: 0.05421249940991402, acc: 0.9844, auc: 0.9985, precision: 0.9841, recall: 0.9841\n",
      "2019-01-14T17:36:24.322067, step: 431, loss: 0.11093436181545258, acc: 0.9609, auc: 0.9978, precision: 0.9365, recall: 0.9833\n",
      "2019-01-14T17:36:24.573607, step: 432, loss: 0.06787979602813721, acc: 0.9922, auc: 0.9985, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:36:24.816360, step: 433, loss: 0.05723496526479721, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9577\n",
      "2019-01-14T17:36:25.044352, step: 434, loss: 0.10725779831409454, acc: 0.9453, auc: 0.9968, precision: 1.0, recall: 0.8871\n",
      "2019-01-14T17:36:25.259728, step: 435, loss: 0.09683198481798172, acc: 0.9375, auc: 0.9997, precision: 1.0, recall: 0.8987\n",
      "2019-01-14T17:36:25.498784, step: 436, loss: 0.060249458998441696, acc: 0.9766, auc: 0.9985, precision: 0.9853, recall: 0.971\n",
      "2019-01-14T17:36:25.728961, step: 437, loss: 0.10234703868627548, acc: 0.9688, auc: 0.9976, precision: 0.9524, recall: 0.9836\n",
      "2019-01-14T17:36:25.946553, step: 438, loss: 0.06690274178981781, acc: 0.9766, auc: 1.0, precision: 0.9595, recall: 1.0\n",
      "2019-01-14T17:36:26.162167, step: 439, loss: 0.07623641937971115, acc: 0.9609, auc: 0.9973, precision: 0.9531, recall: 0.9683\n",
      "2019-01-14T17:36:26.391725, step: 440, loss: 0.04363034665584564, acc: 0.9766, auc: 0.999, precision: 0.9672, recall: 0.9833\n",
      "2019-01-14T17:36:26.611039, step: 441, loss: 0.051654670387506485, acc: 0.9766, auc: 0.9983, precision: 0.9833, recall: 0.9672\n",
      "2019-01-14T17:36:26.830098, step: 442, loss: 0.08772560209035873, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.9306\n",
      "2019-01-14T17:36:27.046163, step: 443, loss: 0.05961516126990318, acc: 0.9609, auc: 0.9995, precision: 1.0, recall: 0.9107\n",
      "2019-01-14T17:36:27.280099, step: 444, loss: 0.023761380463838577, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:36:27.505131, step: 445, loss: 0.061541784554719925, acc: 0.9844, auc: 0.9993, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:36:27.729663, step: 446, loss: 0.13522453606128693, acc: 0.9531, auc: 0.9993, precision: 0.9104, recall: 1.0\n",
      "2019-01-14T17:36:27.962436, step: 447, loss: 0.09982949495315552, acc: 0.9609, auc: 0.9939, precision: 0.9833, recall: 0.9365\n",
      "2019-01-14T17:36:28.180761, step: 448, loss: 0.049436405301094055, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9552\n",
      "2019-01-14T17:36:28.409316, step: 449, loss: 0.08583516627550125, acc: 0.9531, auc: 0.997, precision: 1.0, recall: 0.8889\n",
      "2019-01-14T17:36:28.646091, step: 450, loss: 0.07873275130987167, acc: 0.9531, auc: 0.997, precision: 0.9815, recall: 0.9138\n",
      "2019-01-14T17:36:28.867515, step: 451, loss: 0.0558624193072319, acc: 0.9844, auc: 0.998, precision: 1.0, recall: 0.9683\n",
      "2019-01-14T17:36:29.115821, step: 452, loss: 0.14199641346931458, acc: 0.9688, auc: 0.9954, precision: 0.9412, recall: 1.0\n",
      "2019-01-14T17:36:29.343313, step: 453, loss: 0.044408805668354034, acc: 0.9766, auc: 0.9993, precision: 0.9706, recall: 0.9851\n",
      "2019-01-14T17:36:29.578967, step: 454, loss: 0.05376540869474411, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-14T17:36:29.804159, step: 455, loss: 0.03640296682715416, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:36:30.032629, step: 456, loss: 0.13401472568511963, acc: 0.9453, auc: 0.9901, precision: 0.9623, recall: 0.9107\n",
      "2019-01-14T17:36:30.255186, step: 457, loss: 0.16056689620018005, acc: 0.9297, auc: 0.9936, precision: 0.9831, recall: 0.8788\n",
      "2019-01-14T17:36:30.479208, step: 458, loss: 0.033569999039173126, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9677\n",
      "2019-01-14T17:36:30.712162, step: 459, loss: 0.05703260377049446, acc: 0.9766, auc: 0.9976, precision: 0.9841, recall: 0.9688\n",
      "2019-01-14T17:36:30.940895, step: 460, loss: 0.0406845286488533, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:36:31.166154, step: 461, loss: 0.09385187178850174, acc: 0.9688, auc: 1.0, precision: 0.9444, recall: 1.0\n",
      "2019-01-14T17:36:31.379768, step: 462, loss: 0.10316084325313568, acc: 0.9375, auc: 0.9939, precision: 0.9492, recall: 0.918\n",
      "2019-01-14T17:36:31.587718, step: 463, loss: 0.10437984019517899, acc: 0.9844, auc: 0.9862, precision: 0.9857, recall: 0.9857\n",
      "2019-01-14T17:36:31.812574, step: 464, loss: 0.1347334086894989, acc: 0.9297, auc: 0.9944, precision: 0.9808, recall: 0.8644\n",
      "2019-01-14T17:36:32.022540, step: 465, loss: 0.03662886470556259, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9322\n",
      "2019-01-14T17:36:32.255927, step: 466, loss: 0.07881435006856918, acc: 0.9609, auc: 0.9985, precision: 1.0, recall: 0.902\n",
      "2019-01-14T17:36:32.499142, step: 467, loss: 0.10357421636581421, acc: 0.9688, auc: 0.9951, precision: 0.95, recall: 0.9828\n",
      "2019-01-14T17:36:32.711406, step: 468, loss: 0.13755309581756592, acc: 0.9609, auc: 0.9936, precision: 0.9516, recall: 0.9672\n",
      "start training model\n",
      "2019-01-14T17:36:32.964148, step: 469, loss: 0.020362507551908493, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:36:33.191396, step: 470, loss: 0.014757546596229076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:33.411534, step: 471, loss: 0.03374876454472542, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9726\n",
      "2019-01-14T17:36:33.642890, step: 472, loss: 0.028108904138207436, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:36:33.859418, step: 473, loss: 0.02934163622558117, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:36:34.098813, step: 474, loss: 0.030080106109380722, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2019-01-14T17:36:34.325276, step: 475, loss: 0.029465286061167717, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:34.551955, step: 476, loss: 0.019158270210027695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:34.787007, step: 477, loss: 0.02994583174586296, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-01-14T17:36:35.009755, step: 478, loss: 0.02627214416861534, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9714\n",
      "2019-01-14T17:36:35.240334, step: 479, loss: 0.025965368375182152, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:36:35.458244, step: 480, loss: 0.018430646508932114, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:35.673942, step: 481, loss: 0.04750654101371765, acc: 0.9766, auc: 0.999, precision: 0.9833, recall: 0.9672\n",
      "2019-01-14T17:36:35.887528, step: 482, loss: 0.013926085084676743, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:36:36.103474, step: 483, loss: 0.03105492889881134, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9672\n",
      "2019-01-14T17:36:36.312721, step: 484, loss: 0.01762024685740471, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:36:36.532528, step: 485, loss: 0.00850729551166296, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:36.741918, step: 486, loss: 0.012474019080400467, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:36.958239, step: 487, loss: 0.011282265186309814, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-14T17:36:37.170725, step: 488, loss: 0.012402311898767948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:37.394930, step: 489, loss: 0.01350594125688076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:37.615316, step: 490, loss: 0.023836586624383926, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:36:37.839266, step: 491, loss: 0.010738336481153965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:38.068991, step: 492, loss: 0.02746187523007393, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.971\n",
      "2019-01-14T17:36:38.296730, step: 493, loss: 0.039185166358947754, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9655\n",
      "2019-01-14T17:36:38.512589, step: 494, loss: 0.010571401566267014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:36:38.731455, step: 495, loss: 0.033059995621442795, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9701\n",
      "2019-01-14T17:36:38.946634, step: 496, loss: 0.03966405242681503, acc: 0.9922, auc: 0.9988, precision: 0.9821, recall: 1.0\n",
      "2019-01-14T17:36:39.158529, step: 497, loss: 0.00846091192215681, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:39.373254, step: 498, loss: 0.008473577909171581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:39.587747, step: 499, loss: 0.017896782606840134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:39.804997, step: 500, loss: 0.024528775364160538, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:36:48.222632, step: 500, loss: 0.43148889946631896, acc: 0.8571666666666664, auc: 0.9380153846153844, precision: 0.8676000000000001, recall: 0.8456179487179488\n",
      "2019-01-14T17:36:48.442633, step: 501, loss: 0.011814489960670471, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:48.666924, step: 502, loss: 0.010391185991466045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:48.880373, step: 503, loss: 0.02004210464656353, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:36:49.091788, step: 504, loss: 0.019987137988209724, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:36:49.310189, step: 505, loss: 0.007677245885133743, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:49.540076, step: 506, loss: 0.011046021245419979, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:49.768948, step: 507, loss: 0.012011116370558739, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:36:49.988799, step: 508, loss: 0.013362769037485123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:50.202274, step: 509, loss: 0.021778617054224014, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:36:50.415480, step: 510, loss: 0.007335786707699299, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:50.640604, step: 511, loss: 0.0076312837190926075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:50.862619, step: 512, loss: 0.018108565360307693, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:51.078367, step: 513, loss: 0.017253268510103226, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-14T17:36:51.302052, step: 514, loss: 0.008235848508775234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:51.515497, step: 515, loss: 0.015935510396957397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:51.746569, step: 516, loss: 0.01327361911535263, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:36:51.977913, step: 517, loss: 0.015767715871334076, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:36:52.217901, step: 518, loss: 0.0049852849915623665, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:52.449161, step: 519, loss: 0.017324253916740417, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:52.684989, step: 520, loss: 0.017514772713184357, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:36:52.907466, step: 521, loss: 0.03473353758454323, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-14T17:36:53.136778, step: 522, loss: 0.0064729005098342896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:53.368151, step: 523, loss: 0.05239718779921532, acc: 0.9922, auc: 0.9966, precision: 1.0, recall: 0.9836\n",
      "2019-01-14T17:36:53.582482, step: 524, loss: 0.006229223217815161, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:53.809734, step: 525, loss: 0.009534269571304321, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:54.019213, step: 526, loss: 0.011615143157541752, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:54.221559, step: 527, loss: 0.016134820878505707, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9875\n",
      "2019-01-14T17:36:54.450277, step: 528, loss: 0.014976798556745052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:54.668972, step: 529, loss: 0.01052633486688137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:54.902978, step: 530, loss: 0.008053171448409557, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:55.126421, step: 531, loss: 0.010121841914951801, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:55.350058, step: 532, loss: 0.010125106200575829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:55.573205, step: 533, loss: 0.02685578167438507, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-01-14T17:36:55.801456, step: 534, loss: 0.007300635799765587, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:56.028032, step: 535, loss: 0.007233814802020788, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:56.255176, step: 536, loss: 0.0077168866991996765, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:56.462326, step: 537, loss: 0.014132101088762283, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:56.695151, step: 538, loss: 0.029841354116797447, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9677\n",
      "2019-01-14T17:36:56.937517, step: 539, loss: 0.025427795946598053, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-14T17:36:57.174049, step: 540, loss: 0.023067615926265717, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:57.378118, step: 541, loss: 0.02485811337828636, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:57.604454, step: 542, loss: 0.03299367055296898, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9851\n",
      "2019-01-14T17:36:57.829229, step: 543, loss: 0.008765110746026039, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:58.046568, step: 544, loss: 0.009789474308490753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:58.289841, step: 545, loss: 0.007102755829691887, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:58.513409, step: 546, loss: 0.009399892762303352, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:58.736508, step: 547, loss: 0.014557464979588985, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:58.977488, step: 548, loss: 0.02583782561123371, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2019-01-14T17:36:59.217745, step: 549, loss: 0.01655198074877262, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:36:59.446826, step: 550, loss: 0.012324415147304535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:59.675693, step: 551, loss: 0.0055078319273889065, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:36:59.926183, step: 552, loss: 0.01035269908607006, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:00.165088, step: 553, loss: 0.02676427736878395, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:37:00.389863, step: 554, loss: 0.01582906022667885, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:00.598330, step: 555, loss: 0.028162885457277298, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:00.838351, step: 556, loss: 0.012324854731559753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:01.054880, step: 557, loss: 0.003667070996016264, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:01.292499, step: 558, loss: 0.00917975977063179, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:01.523071, step: 559, loss: 0.011571330949664116, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:01.748721, step: 560, loss: 0.010559420101344585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:01.961660, step: 561, loss: 0.018172049894928932, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-14T17:37:02.174245, step: 562, loss: 0.02480294555425644, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-14T17:37:02.403049, step: 563, loss: 0.020712438970804214, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:37:02.621147, step: 564, loss: 0.00811002403497696, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:02.836117, step: 565, loss: 0.015072722919285297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:03.055238, step: 566, loss: 0.009589592926204205, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:03.263262, step: 567, loss: 0.01878630556166172, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-14T17:37:03.488992, step: 568, loss: 0.01149794366210699, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:03.713477, step: 569, loss: 0.010849027894437313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:03.940561, step: 570, loss: 0.011082868091762066, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:37:04.151609, step: 571, loss: 0.009003750048577785, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:04.375570, step: 572, loss: 0.01769915409386158, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2019-01-14T17:37:04.603706, step: 573, loss: 0.03196374326944351, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:37:04.819578, step: 574, loss: 0.00456968042999506, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:05.057223, step: 575, loss: 0.00861305557191372, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:05.286782, step: 576, loss: 0.020811373367905617, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-01-14T17:37:05.507203, step: 577, loss: 0.024569759145379066, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:37:05.732439, step: 578, loss: 0.009992127306759357, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:05.962568, step: 579, loss: 0.014863116666674614, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:06.205470, step: 580, loss: 0.01955680176615715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:06.430184, step: 581, loss: 0.02238614670932293, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:37:06.651680, step: 582, loss: 0.01501806266605854, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9868\n",
      "2019-01-14T17:37:06.879851, step: 583, loss: 0.005850702524185181, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:07.095687, step: 584, loss: 0.03188931196928024, acc: 0.9844, auc: 0.9998, precision: 0.9859, recall: 0.9859\n",
      "2019-01-14T17:37:07.323956, step: 585, loss: 0.011407515965402126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:07.550761, step: 586, loss: 0.013712059706449509, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:07.777718, step: 587, loss: 0.017931099981069565, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:07.999841, step: 588, loss: 0.012757006101310253, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-14T17:37:08.228975, step: 589, loss: 0.021420225501060486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:08.468323, step: 590, loss: 0.01083996519446373, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:08.692868, step: 591, loss: 0.010984434746205807, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:08.910499, step: 592, loss: 0.0600670725107193, acc: 0.9844, auc: 0.9983, precision: 1.0, recall: 0.9726\n",
      "2019-01-14T17:37:09.148814, step: 593, loss: 0.01723923534154892, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:37:09.367171, step: 594, loss: 0.01763913780450821, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-14T17:37:09.594888, step: 595, loss: 0.012133133597671986, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:09.817298, step: 596, loss: 0.02544044516980648, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2019-01-14T17:37:10.064463, step: 597, loss: 0.020792903378605843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:10.293287, step: 598, loss: 0.01129482127726078, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:10.514309, step: 599, loss: 0.0137109300121665, acc: 0.9922, auc: 1.0, precision: 0.9861, recall: 1.0\n",
      "2019-01-14T17:37:10.745418, step: 600, loss: 0.016257334500551224, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:37:19.104262, step: 600, loss: 0.4736949220681802, acc: 0.852169230769231, auc: 0.9385307692307694, precision: 0.8878487179487178, recall: 0.8083384615384616\n",
      "2019-01-14T17:37:19.309955, step: 601, loss: 0.007593238260596991, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:19.529685, step: 602, loss: 0.00972664263099432, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:19.751269, step: 603, loss: 0.008821483701467514, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:19.977198, step: 604, loss: 0.06695631891489029, acc: 0.9922, auc: 0.9937, precision: 0.9844, recall: 1.0\n",
      "2019-01-14T17:37:20.193631, step: 605, loss: 0.013996982015669346, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:20.397471, step: 606, loss: 0.05916505679488182, acc: 0.9609, auc: 0.999, precision: 1.0, recall: 0.9219\n",
      "2019-01-14T17:37:20.600291, step: 607, loss: 0.01026703231036663, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:20.824047, step: 608, loss: 0.01803293265402317, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-14T17:37:21.035754, step: 609, loss: 0.0207262821495533, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:21.266041, step: 610, loss: 0.015608781948685646, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:37:21.492411, step: 611, loss: 0.014676269143819809, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:21.717017, step: 612, loss: 0.0077004279009997845, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:21.937623, step: 613, loss: 0.02629849687218666, acc: 0.9922, auc: 0.9998, precision: 0.9825, recall: 1.0\n",
      "2019-01-14T17:37:22.163418, step: 614, loss: 0.02417784184217453, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:22.382937, step: 615, loss: 0.026228075847029686, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9661\n",
      "2019-01-14T17:37:22.599467, step: 616, loss: 0.03188787028193474, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-14T17:37:22.827587, step: 617, loss: 0.01546783559024334, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:37:23.041287, step: 618, loss: 0.015234870836138725, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9861\n",
      "2019-01-14T17:37:23.266129, step: 619, loss: 0.006677316036075354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:23.498960, step: 620, loss: 0.015255250036716461, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:23.738993, step: 621, loss: 0.01086333952844143, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:23.965578, step: 622, loss: 0.06135363131761551, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-14T17:37:24.200789, step: 623, loss: 0.05956912413239479, acc: 0.9844, auc: 0.998, precision: 0.9706, recall: 1.0\n",
      "2019-01-14T17:37:24.422994, step: 624, loss: 0.030579406768083572, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9615\n",
      "start training model\n",
      "2019-01-14T17:37:24.656718, step: 625, loss: 0.018663939088582993, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9701\n",
      "2019-01-14T17:37:24.868886, step: 626, loss: 0.006454471964389086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:25.085808, step: 627, loss: 0.04107144847512245, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9474\n",
      "2019-01-14T17:37:25.316447, step: 628, loss: 0.01294233463704586, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:37:25.539184, step: 629, loss: 0.005326038226485252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:25.761839, step: 630, loss: 0.004109949339181185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:25.989254, step: 631, loss: 0.03048941306769848, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:37:26.207078, step: 632, loss: 0.00829517561942339, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:26.432757, step: 633, loss: 0.015483479015529156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:26.646522, step: 634, loss: 0.005169077310711145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:26.872059, step: 635, loss: 0.007456491701304913, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:27.098548, step: 636, loss: 0.00240101246163249, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:27.325860, step: 637, loss: 0.004023370798677206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:27.534223, step: 638, loss: 0.0047699566930532455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:27.744463, step: 639, loss: 0.009546239860355854, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:27.958333, step: 640, loss: 0.01182791218161583, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:28.176382, step: 641, loss: 0.014853177592158318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:28.398722, step: 642, loss: 0.0062767500057816505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:28.619347, step: 643, loss: 0.005289605353027582, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:28.848282, step: 644, loss: 0.006143145263195038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:29.071714, step: 645, loss: 0.005080359056591988, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:29.282841, step: 646, loss: 0.00806130189448595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:29.505973, step: 647, loss: 0.009590556845068932, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:29.730613, step: 648, loss: 0.016625596210360527, acc: 0.9922, auc: 1.0, precision: 0.9861, recall: 1.0\n",
      "2019-01-14T17:37:29.956004, step: 649, loss: 0.002537339460104704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:30.165714, step: 650, loss: 0.004456837195903063, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:30.388979, step: 651, loss: 0.008832154795527458, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:30.595629, step: 652, loss: 0.006798058748245239, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:30.801921, step: 653, loss: 0.003566225292161107, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:31.006132, step: 654, loss: 0.006083199754357338, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:31.223597, step: 655, loss: 0.003167996648699045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:31.443986, step: 656, loss: 0.0072703235782682896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:31.658155, step: 657, loss: 0.0051996298134326935, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:31.875449, step: 658, loss: 0.0035112001933157444, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:32.099010, step: 659, loss: 0.00822000578045845, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:32.324077, step: 660, loss: 0.006553854793310165, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:32.528668, step: 661, loss: 0.004402128513902426, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:32.741636, step: 662, loss: 0.0048326533287763596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:32.953120, step: 663, loss: 0.003406002651900053, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:33.160586, step: 664, loss: 0.0030922910664230585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:33.376934, step: 665, loss: 0.003856626572087407, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:33.607499, step: 666, loss: 0.0033623003400862217, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:33.828931, step: 667, loss: 0.004595180042088032, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:34.050458, step: 668, loss: 0.011211658827960491, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:34.274642, step: 669, loss: 0.005579243414103985, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:34.499458, step: 670, loss: 0.006587645038962364, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:37:34.715090, step: 671, loss: 0.00286304485052824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:34.940169, step: 672, loss: 0.03328234329819679, acc: 0.9922, auc: 0.9993, precision: 0.9836, recall: 1.0\n",
      "2019-01-14T17:37:35.154271, step: 673, loss: 0.006365648936480284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:35.364209, step: 674, loss: 0.006805581972002983, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:35.576924, step: 675, loss: 0.0026564975269138813, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:35.794231, step: 676, loss: 0.0036446242593228817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:36.005576, step: 677, loss: 0.010844291187822819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:36.223062, step: 678, loss: 0.0029851580038666725, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:36.430521, step: 679, loss: 0.007272977381944656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:36.644792, step: 680, loss: 0.0024021342396736145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:36.858650, step: 681, loss: 0.006409852299839258, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:37.074322, step: 682, loss: 0.004682828672230244, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:37.297411, step: 683, loss: 0.00272601330652833, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:37.522256, step: 684, loss: 0.004676211159676313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:37.735837, step: 685, loss: 0.004809086676687002, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:37.953681, step: 686, loss: 0.005763641092926264, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:38.171337, step: 687, loss: 0.004050279036164284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:38.380547, step: 688, loss: 0.0049213082529604435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:38.601387, step: 689, loss: 0.0016622934490442276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:38.809428, step: 690, loss: 0.004205831792205572, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:39.031345, step: 691, loss: 0.004929284565150738, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:39.255959, step: 692, loss: 0.003909245133399963, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:39.470027, step: 693, loss: 0.004325877409428358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:39.693985, step: 694, loss: 0.0030583511106669903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:39.917834, step: 695, loss: 0.0054808068089187145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:40.134052, step: 696, loss: 0.005716424435377121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:40.351199, step: 697, loss: 0.007476350292563438, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:40.567738, step: 698, loss: 0.004174146335572004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:40.779216, step: 699, loss: 0.0026593327056616545, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:41.001077, step: 700, loss: 0.006638365797698498, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:37:49.427217, step: 700, loss: 0.4899635100976015, acc: 0.8583794871794871, auc: 0.9380769230769231, precision: 0.8842025641025643, recall: 0.8280076923076922\n",
      "2019-01-14T17:37:49.643944, step: 701, loss: 0.0033362703397870064, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:37:49.863604, step: 702, loss: 0.0031422737520188093, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:50.081209, step: 703, loss: 0.0028839174192398787, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:50.290299, step: 704, loss: 0.010522495955228806, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-14T17:37:50.515949, step: 705, loss: 0.005291001871228218, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:50.741411, step: 706, loss: 0.0032399354968219995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:50.966037, step: 707, loss: 0.004618623293936253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:51.177678, step: 708, loss: 0.004141396842896938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:51.405455, step: 709, loss: 0.008992093615233898, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:51.625795, step: 710, loss: 0.004011680372059345, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:51.851519, step: 711, loss: 0.004222102463245392, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:52.074608, step: 712, loss: 0.0042128912173211575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:52.297988, step: 713, loss: 0.003914493601769209, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:52.517197, step: 714, loss: 0.0032834664452821016, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:52.744487, step: 715, loss: 0.006675868760794401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:52.961541, step: 716, loss: 0.003962043207138777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:53.183897, step: 717, loss: 0.00601896271109581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:53.402888, step: 718, loss: 0.004640617407858372, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:53.616506, step: 719, loss: 0.003012422239407897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:53.844201, step: 720, loss: 0.007315428927540779, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:54.054732, step: 721, loss: 0.004220138769596815, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:54.285669, step: 722, loss: 0.005315876565873623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:54.513953, step: 723, loss: 0.0029195027891546488, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:54.741697, step: 724, loss: 0.009796228259801865, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:54.985213, step: 725, loss: 0.005389954429119825, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:55.198306, step: 726, loss: 0.005596984643489122, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:55.429396, step: 727, loss: 0.004008038900792599, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:55.656623, step: 728, loss: 0.005073249340057373, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:55.880318, step: 729, loss: 0.007582363672554493, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9818\n",
      "2019-01-14T17:37:56.101421, step: 730, loss: 0.009879490360617638, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-14T17:37:56.320715, step: 731, loss: 0.004714767914265394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:56.540022, step: 732, loss: 0.005348703823983669, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:56.760766, step: 733, loss: 0.0036056209355592728, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:56.996507, step: 734, loss: 0.002590527292340994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:57.219685, step: 735, loss: 0.0073875850066542625, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:57.447079, step: 736, loss: 0.009930508211255074, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:57.673186, step: 737, loss: 0.005091379396617413, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:57.883108, step: 738, loss: 0.0033693923614919186, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:58.105811, step: 739, loss: 0.00946059264242649, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-14T17:37:58.327693, step: 740, loss: 0.008718518540263176, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-14T17:37:58.537188, step: 741, loss: 0.004112524446099997, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:58.767237, step: 742, loss: 0.007666085381060839, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:58.987201, step: 743, loss: 0.005666974000632763, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:59.215882, step: 744, loss: 0.0027999053709208965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:59.426047, step: 745, loss: 0.005771413445472717, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:59.638526, step: 746, loss: 0.003759918734431267, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:37:59.865409, step: 747, loss: 0.003778600599616766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:00.088299, step: 748, loss: 0.005082686897367239, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:00.313453, step: 749, loss: 0.004288422409445047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:00.529183, step: 750, loss: 0.0028238394297659397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:00.752116, step: 751, loss: 0.0024443797301501036, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:00.974613, step: 752, loss: 0.006677838042378426, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-14T17:38:01.190503, step: 753, loss: 0.008093185722827911, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:01.415196, step: 754, loss: 0.008848994970321655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:01.639059, step: 755, loss: 0.002529328456148505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:01.855766, step: 756, loss: 0.002232553204521537, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:02.067421, step: 757, loss: 0.004080709535628557, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:02.291424, step: 758, loss: 0.0046399254351854324, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:02.518877, step: 759, loss: 0.002152849454432726, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:02.729453, step: 760, loss: 0.0015946348430588841, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:02.948940, step: 761, loss: 0.0024387710727751255, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:03.166209, step: 762, loss: 0.005752319470047951, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:03.373814, step: 763, loss: 0.004942222964018583, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:03.595244, step: 764, loss: 0.016355939209461212, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-14T17:38:03.825814, step: 765, loss: 0.002587861381471157, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:04.052179, step: 766, loss: 0.0028241416439414024, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:04.271012, step: 767, loss: 0.006498976610600948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:04.495121, step: 768, loss: 0.009331349283456802, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:04.729360, step: 769, loss: 0.005551567766815424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:04.953720, step: 770, loss: 0.006369687616825104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:05.174305, step: 771, loss: 0.0016863038763403893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:05.400899, step: 772, loss: 0.0036340500228106976, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:38:05.627321, step: 773, loss: 0.007501561194658279, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:05.845714, step: 774, loss: 0.004183632787317038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:06.067717, step: 775, loss: 0.0057284655049443245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:06.291201, step: 776, loss: 0.005910538136959076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:06.506606, step: 777, loss: 0.0022873987909406424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:06.730513, step: 778, loss: 0.002794623374938965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:06.955555, step: 779, loss: 0.003281207522377372, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:07.175731, step: 780, loss: 0.005905788857489824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:38:07.416661, step: 781, loss: 0.0032039976213127375, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:07.631397, step: 782, loss: 0.004265607800334692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:07.846419, step: 783, loss: 0.005984284915030003, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:08.060936, step: 784, loss: 0.001248367945663631, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:08.272880, step: 785, loss: 0.002990391571074724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:08.495954, step: 786, loss: 0.0029422095976769924, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:08.723336, step: 787, loss: 0.0012532268883660436, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:08.950350, step: 788, loss: 0.0013466065283864737, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:09.178475, step: 789, loss: 0.0015322358813136816, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:09.393807, step: 790, loss: 0.002820504829287529, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:09.605289, step: 791, loss: 0.001189577393233776, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:09.825489, step: 792, loss: 0.005417773965746164, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:10.036967, step: 793, loss: 0.002756142057478428, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:10.257528, step: 794, loss: 0.0024102358147501945, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:10.469216, step: 795, loss: 0.0035126744769513607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:10.690642, step: 796, loss: 0.001108056167140603, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:10.916511, step: 797, loss: 0.0016709129558876157, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:11.135310, step: 798, loss: 0.0019008300732821226, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:11.358413, step: 799, loss: 0.0011192257516086102, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:11.568678, step: 800, loss: 0.0016886588418856263, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:38:20.058259, step: 800, loss: 0.48736221133134305, acc: 0.8635769230769228, auc: 0.9413538461538462, precision: 0.8730641025641024, recall: 0.8555871794871793\n",
      "2019-01-14T17:38:20.265418, step: 801, loss: 0.0028778756968677044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:20.478435, step: 802, loss: 0.0023301781620830297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:20.704553, step: 803, loss: 0.0020707272924482822, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:20.933900, step: 804, loss: 0.001671979553066194, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:21.156254, step: 805, loss: 0.0024340215604752302, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:21.374434, step: 806, loss: 0.020099231973290443, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-14T17:38:21.591603, step: 807, loss: 0.0013620443642139435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:21.817205, step: 808, loss: 0.004332403186708689, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:22.043987, step: 809, loss: 0.00251080677844584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:22.267941, step: 810, loss: 0.002019186271354556, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:22.476156, step: 811, loss: 0.002887171460315585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:22.685691, step: 812, loss: 0.002202327363193035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:22.904434, step: 813, loss: 0.0017394296592101455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:23.127288, step: 814, loss: 0.0007190622854977846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:23.360107, step: 815, loss: 0.0017847573617473245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:23.584938, step: 816, loss: 0.0021759583614766598, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:23.809244, step: 817, loss: 0.001740297069773078, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:24.037156, step: 818, loss: 0.004091306589543819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:24.253852, step: 819, loss: 0.0049560414627194405, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:24.485600, step: 820, loss: 0.0020088632591068745, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:24.708209, step: 821, loss: 0.0024146868381649256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:24.920779, step: 822, loss: 0.002855071099475026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:25.140960, step: 823, loss: 0.001458476297557354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:25.354412, step: 824, loss: 0.00329568306915462, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:25.583080, step: 825, loss: 0.0017474095802754164, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:25.803408, step: 826, loss: 0.001429211813956499, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:26.025012, step: 827, loss: 0.001705053262412548, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:26.248820, step: 828, loss: 0.0032215628307312727, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:26.473011, step: 829, loss: 0.0014883815310895443, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:26.706437, step: 830, loss: 0.0023478087969124317, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:26.934594, step: 831, loss: 0.002075641881674528, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:27.149708, step: 832, loss: 0.002215968444943428, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:27.374563, step: 833, loss: 0.0011905892752110958, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:27.598268, step: 834, loss: 0.002070456277579069, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:27.819531, step: 835, loss: 0.0036517204716801643, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:28.042461, step: 836, loss: 0.0028279763646423817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:28.256385, step: 837, loss: 0.0026653525419533253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:28.478462, step: 838, loss: 0.0019375683041289449, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:28.693802, step: 839, loss: 0.002334762830287218, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:28.903407, step: 840, loss: 0.005177412182092667, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:29.123336, step: 841, loss: 0.0017595133977010846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:29.346168, step: 842, loss: 0.0020860363729298115, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:38:29.567116, step: 843, loss: 0.0013562887907028198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:29.792277, step: 844, loss: 0.002589381765574217, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:30.019676, step: 845, loss: 0.002446998143568635, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:30.239102, step: 846, loss: 0.006103715859353542, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:30.452341, step: 847, loss: 0.0015244358219206333, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:30.663223, step: 848, loss: 0.0009340300457552075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:30.878115, step: 849, loss: 0.0015790157485753298, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:31.101699, step: 850, loss: 0.002528147306293249, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:31.324739, step: 851, loss: 0.002039806917309761, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:31.536540, step: 852, loss: 0.0016745771281421185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:31.749621, step: 853, loss: 0.0016609590966254473, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:31.982236, step: 854, loss: 0.0016065094387158751, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:32.205120, step: 855, loss: 0.0016240559052675962, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:32.426981, step: 856, loss: 0.0011212698882445693, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:32.638744, step: 857, loss: 0.002478844951838255, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:32.863029, step: 858, loss: 0.0022040987387299538, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:33.090016, step: 859, loss: 0.001580808311700821, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:33.308919, step: 860, loss: 0.0024290091823786497, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:33.534568, step: 861, loss: 0.00295436242595315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:33.749414, step: 862, loss: 0.002849803538993001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:33.969236, step: 863, loss: 0.0024713417515158653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:34.201209, step: 864, loss: 0.002015192760154605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:34.420312, step: 865, loss: 0.0015226497780531645, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:34.638484, step: 866, loss: 0.0014216321287676692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:34.848240, step: 867, loss: 0.002021373948082328, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:35.072724, step: 868, loss: 0.002554528182372451, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:35.280079, step: 869, loss: 0.002309569623321295, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:35.514695, step: 870, loss: 0.005318882875144482, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:35.761393, step: 871, loss: 0.0010960829677060246, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:35.985917, step: 872, loss: 0.0016217846423387527, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:36.207745, step: 873, loss: 0.0011411409359425306, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:36.419990, step: 874, loss: 0.00362838851287961, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:36.629356, step: 875, loss: 0.0007533931639045477, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:36.846085, step: 876, loss: 0.001082502887584269, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:37.063296, step: 877, loss: 0.0011143696028739214, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:37.287545, step: 878, loss: 0.0019693132489919662, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:37.511515, step: 879, loss: 0.001405656454153359, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:37.737376, step: 880, loss: 0.001404422800987959, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:37.961215, step: 881, loss: 0.0015716899652034044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:38.171058, step: 882, loss: 0.0012519161682575941, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:38.388589, step: 883, loss: 0.001740296371281147, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:38.625728, step: 884, loss: 0.0015364617574959993, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:38.839859, step: 885, loss: 0.0013490418205037713, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:39.054001, step: 886, loss: 0.0007561794482171535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:39.272784, step: 887, loss: 0.0018753993790596724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:39.493935, step: 888, loss: 0.0018445419846102595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:39.703937, step: 889, loss: 0.0010773054091259837, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:39.916819, step: 890, loss: 0.001479740021750331, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:40.139002, step: 891, loss: 0.0020228764042258263, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:40.352593, step: 892, loss: 0.001033335691317916, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:40.563510, step: 893, loss: 0.0011815689504146576, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:40.774427, step: 894, loss: 0.0033758149947971106, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:40.988758, step: 895, loss: 0.0022107455879449844, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:41.205635, step: 896, loss: 0.0013647270388901234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:41.420676, step: 897, loss: 0.0011505079455673695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:41.636133, step: 898, loss: 0.001920793205499649, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:41.857966, step: 899, loss: 0.0030859126709401608, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:42.085086, step: 900, loss: 0.002131996676325798, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:38:50.516754, step: 900, loss: 0.5151930459034748, acc: 0.8639794871794871, auc: 0.9421589743589743, precision: 0.8823717948717947, recall: 0.843074358974359\n",
      "2019-01-14T17:38:50.723487, step: 901, loss: 0.002631555777043104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:50.935957, step: 902, loss: 0.0011529874755069613, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:51.152289, step: 903, loss: 0.001400104956701398, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:51.376923, step: 904, loss: 0.0009845731547102332, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:51.597610, step: 905, loss: 0.00244078878313303, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:51.808326, step: 906, loss: 0.0009837725665420294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:52.030603, step: 907, loss: 0.0017751962877810001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:52.245971, step: 908, loss: 0.0007600505487062037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:52.453352, step: 909, loss: 0.0013383866753429174, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:52.662473, step: 910, loss: 0.0012200191849842668, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:52.881419, step: 911, loss: 0.0020397084299474955, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:53.112736, step: 912, loss: 0.0012617940083146095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:38:53.332637, step: 913, loss: 0.0014472807524725795, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:53.553764, step: 914, loss: 0.0014591639628633857, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:53.747540, step: 915, loss: 0.0010281975846737623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:53.946889, step: 916, loss: 0.0032983021810650826, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:54.158486, step: 917, loss: 0.0018343800911679864, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:54.380538, step: 918, loss: 0.0016624323325231671, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:54.593368, step: 919, loss: 0.0018672642763704062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:54.815741, step: 920, loss: 0.002935961354523897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:55.032704, step: 921, loss: 0.001924664480611682, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:55.256607, step: 922, loss: 0.0017797686159610748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:55.463010, step: 923, loss: 0.0012045656330883503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:55.671524, step: 924, loss: 0.0010503814555704594, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:55.906356, step: 925, loss: 0.0016073192236945033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:56.142720, step: 926, loss: 0.0020422572270035744, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:56.359630, step: 927, loss: 0.001562945544719696, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:56.585432, step: 928, loss: 0.0017091225599870086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:56.819497, step: 929, loss: 0.0009631874272599816, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:57.036026, step: 930, loss: 0.0013420027680695057, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:57.262237, step: 931, loss: 0.0016905267257243395, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:57.487707, step: 932, loss: 0.0009184223017655313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:57.704745, step: 933, loss: 0.0017886341083794832, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:57.926892, step: 934, loss: 0.0015636911848559976, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:58.140625, step: 935, loss: 0.0011449526064097881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:58.356644, step: 936, loss: 0.0006904872134327888, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:38:58.594683, step: 937, loss: 0.0009680112707428634, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:58.804886, step: 938, loss: 0.0011865308042615652, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:59.010859, step: 939, loss: 0.0006960625178180635, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:59.228149, step: 940, loss: 0.0023746842052787542, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:59.441920, step: 941, loss: 0.0011127667967230082, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:59.664831, step: 942, loss: 0.0015740079106763005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:38:59.883385, step: 943, loss: 0.0014197122072800994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:00.092950, step: 944, loss: 0.0010672463104128838, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:00.313753, step: 945, loss: 0.0008251287508755922, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:00.532123, step: 946, loss: 0.001626300741918385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:00.746259, step: 947, loss: 0.0012785405851900578, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:00.957816, step: 948, loss: 0.0015726517885923386, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:01.170467, step: 949, loss: 0.0029819360934197903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:01.394646, step: 950, loss: 0.0007023133221082389, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:01.605468, step: 951, loss: 0.0007846469525247812, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:01.824978, step: 952, loss: 0.0009525294881314039, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:02.046223, step: 953, loss: 0.0009884742321446538, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:02.269555, step: 954, loss: 0.0015133133856579661, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:02.489496, step: 955, loss: 0.0012060021981596947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:02.714101, step: 956, loss: 0.0015574560966342688, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:02.926745, step: 957, loss: 0.0009461026638746262, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:03.150101, step: 958, loss: 0.0017925892025232315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:03.372935, step: 959, loss: 0.0011262563057243824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:03.596337, step: 960, loss: 0.0009121081093326211, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:03.828931, step: 961, loss: 0.0007678126567043364, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:04.028049, step: 962, loss: 0.0009563672356307507, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:04.241994, step: 963, loss: 0.0006921020685695112, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:04.431005, step: 964, loss: 0.0010217572562396526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:04.642181, step: 965, loss: 0.001264687511138618, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:04.854739, step: 966, loss: 0.0007241747225634754, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:05.077166, step: 967, loss: 0.0014986004680395126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:05.285091, step: 968, loss: 0.001860444201156497, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:05.501731, step: 969, loss: 0.0009435454267077148, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:05.730947, step: 970, loss: 0.001486999448388815, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:05.957635, step: 971, loss: 0.0009058405412361026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:06.188815, step: 972, loss: 0.00123731535859406, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:06.401322, step: 973, loss: 0.0009557402227073908, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:06.630569, step: 974, loss: 0.0010496685281395912, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:06.855554, step: 975, loss: 0.001262217410840094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:07.070075, step: 976, loss: 0.0005966659518890083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:07.314155, step: 977, loss: 0.0014985206071287394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:07.534977, step: 978, loss: 0.0007751306402496994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:07.770480, step: 979, loss: 0.0006869928911328316, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:08.014228, step: 980, loss: 0.002151188673451543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:08.238282, step: 981, loss: 0.0010082591325044632, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:08.456800, step: 982, loss: 0.0010939235799014568, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:08.676437, step: 983, loss: 0.0005869192536920309, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:39:08.889038, step: 984, loss: 0.0006516184657812119, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:09.100703, step: 985, loss: 0.0007862627971917391, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:09.310832, step: 986, loss: 0.000948483997490257, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:09.521628, step: 987, loss: 0.0009285433916375041, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:09.737986, step: 988, loss: 0.001146474969573319, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:09.954346, step: 989, loss: 0.0007428714307025075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:10.198391, step: 990, loss: 0.0011151968501508236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:10.415711, step: 991, loss: 0.0007834007847122848, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:10.625983, step: 992, loss: 0.0008751398418098688, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:10.838767, step: 993, loss: 0.0009511474054306746, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:11.062201, step: 994, loss: 0.0005439255037344992, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:11.286510, step: 995, loss: 0.0008737110765650868, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:11.510637, step: 996, loss: 0.0006432866211980581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:11.734769, step: 997, loss: 0.001012542168609798, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:11.943201, step: 998, loss: 0.0009748500888235867, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:12.163932, step: 999, loss: 0.00041973532643169165, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:12.395279, step: 1000, loss: 0.0007029667613096535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:39:20.638882, step: 1000, loss: 0.5284028416260694, acc: 0.8681820512820508, auc: 0.9426846153846153, precision: 0.8803435897435898, recall: 0.8566333333333329\n",
      "2019-01-14T17:39:20.843896, step: 1001, loss: 0.001346474513411522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:21.052961, step: 1002, loss: 0.0009574323776178062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:21.266961, step: 1003, loss: 0.0008831986924633384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:21.491578, step: 1004, loss: 0.0014516355004161596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:21.709151, step: 1005, loss: 0.0009880604920908809, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:21.922301, step: 1006, loss: 0.0010037111351266503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:22.137904, step: 1007, loss: 0.0008289465331472456, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:22.345685, step: 1008, loss: 0.0005557737313210964, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:22.563475, step: 1009, loss: 0.001326030702330172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:22.785227, step: 1010, loss: 0.0014204377075657248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:22.991844, step: 1011, loss: 0.0009023354505188763, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:23.200957, step: 1012, loss: 0.0015235480386763811, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:23.405462, step: 1013, loss: 0.0009051028755493462, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:23.627531, step: 1014, loss: 0.001383456401526928, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:23.840714, step: 1015, loss: 0.000699721509590745, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:24.060988, step: 1016, loss: 0.0009256253251805902, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:24.280663, step: 1017, loss: 0.0007468223921023309, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:24.509525, step: 1018, loss: 0.0015691013541072607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:24.703032, step: 1019, loss: 0.0007513313903473318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:24.926864, step: 1020, loss: 0.0009523575427010655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:25.122093, step: 1021, loss: 0.0012756158830597997, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:25.334136, step: 1022, loss: 0.00045555562246590853, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:25.559633, step: 1023, loss: 0.001279566902667284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:25.785338, step: 1024, loss: 0.0013384767808020115, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:26.009478, step: 1025, loss: 0.0007924381643533707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:26.225117, step: 1026, loss: 0.0008435848285444081, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:26.443156, step: 1027, loss: 0.0012035855324938893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:26.657546, step: 1028, loss: 0.0015432825312018394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:26.873032, step: 1029, loss: 0.0015410498017445207, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:27.093245, step: 1030, loss: 0.0013580075465142727, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:27.318221, step: 1031, loss: 0.002523981034755707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:27.541638, step: 1032, loss: 0.0008917771046981215, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:27.763590, step: 1033, loss: 0.0007836862932890654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:27.978657, step: 1034, loss: 0.0012217327021062374, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:28.196661, step: 1035, loss: 0.0014167679473757744, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:28.414975, step: 1036, loss: 0.001036185072734952, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:28.630983, step: 1037, loss: 0.0012611343991011381, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:28.849844, step: 1038, loss: 0.0009277466451749206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:29.064297, step: 1039, loss: 0.0005419885856099427, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:29.280898, step: 1040, loss: 0.0006398528348654509, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:29.497877, step: 1041, loss: 0.0008727633394300938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:29.714668, step: 1042, loss: 0.0016570104053243995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:29.935078, step: 1043, loss: 0.001446141628548503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:30.151360, step: 1044, loss: 0.0007438494358211756, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:30.366879, step: 1045, loss: 0.0008402682142332196, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:30.583401, step: 1046, loss: 0.0010840322356671095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:30.802476, step: 1047, loss: 0.0012183950748294592, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:31.021446, step: 1048, loss: 0.001686012838035822, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:31.251273, step: 1049, loss: 0.0006622938090004027, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:31.468426, step: 1050, loss: 0.0005062679410912097, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:31.684302, step: 1051, loss: 0.0005832331953570247, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:31.902603, step: 1052, loss: 0.0018696463666856289, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:39:32.121967, step: 1053, loss: 0.0016211544862017035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:32.338260, step: 1054, loss: 0.0015490036457777023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:32.552909, step: 1055, loss: 0.0009604376973584294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:32.770364, step: 1056, loss: 0.0013419734314084053, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:32.988124, step: 1057, loss: 0.0009966986253857613, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:33.214480, step: 1058, loss: 0.0005179049912840128, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:33.424226, step: 1059, loss: 0.0005950835766270757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:33.641107, step: 1060, loss: 0.0010229424806311727, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:33.856728, step: 1061, loss: 0.0013301761355251074, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:34.071152, step: 1062, loss: 0.0013319991994649172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:34.299155, step: 1063, loss: 0.0012887970078736544, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:34.519401, step: 1064, loss: 0.0016276154201477766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:34.735846, step: 1065, loss: 0.0007333278190344572, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:34.960929, step: 1066, loss: 0.0007359306327998638, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:35.171159, step: 1067, loss: 0.0006105948705226183, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:35.397912, step: 1068, loss: 0.0012605582596734166, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:35.607489, step: 1069, loss: 0.000807375879958272, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:35.814885, step: 1070, loss: 0.0006208894774317741, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:36.049311, step: 1071, loss: 0.0004745805636048317, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:36.281098, step: 1072, loss: 0.0010212429333478212, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:36.504320, step: 1073, loss: 0.0010545124532654881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:36.735462, step: 1074, loss: 0.0011387331178411841, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:36.961028, step: 1075, loss: 0.0009786926675587893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:37.187448, step: 1076, loss: 0.00074423523619771, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:37.415024, step: 1077, loss: 0.000980554148554802, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:37.646788, step: 1078, loss: 0.00066343107027933, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:37.875718, step: 1079, loss: 0.0005863801343366504, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:38.106215, step: 1080, loss: 0.0006135989679023623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:38.330112, step: 1081, loss: 0.0009940925519913435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:38.558437, step: 1082, loss: 0.0008516431553289294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:38.773055, step: 1083, loss: 0.0007443063077516854, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:38.995712, step: 1084, loss: 0.0011925302678719163, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:39.218379, step: 1085, loss: 0.001680255401879549, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:39.453083, step: 1086, loss: 0.0007619410171173513, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:39.684611, step: 1087, loss: 0.0006361410487443209, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:39.903226, step: 1088, loss: 0.0014907743316143751, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:40.136859, step: 1089, loss: 0.0007168977754190564, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:40.365609, step: 1090, loss: 0.0007845713989809155, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:40.597198, step: 1091, loss: 0.0009025149047374725, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:40.840239, step: 1092, loss: 0.0006496177520602942, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:39:41.101458, step: 1093, loss: 0.0007279550191015005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:41.317997, step: 1094, loss: 0.00046288821613416076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:41.532332, step: 1095, loss: 0.0007982631213963032, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:41.764377, step: 1096, loss: 0.0006752185290679336, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:41.985078, step: 1097, loss: 0.0007922356016933918, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:42.207331, step: 1098, loss: 0.0007618424715474248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:42.436893, step: 1099, loss: 0.0007838298915885389, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:42.660743, step: 1100, loss: 0.0006188831757754087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:39:51.116877, step: 1100, loss: 0.5467681273435935, acc: 0.8677846153846155, auc: 0.9435384615384614, precision: 0.879102564102564, recall: 0.8546948717948718\n",
      "2019-01-14T17:39:51.341183, step: 1101, loss: 0.0006490162923000753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:51.557412, step: 1102, loss: 0.0003514396375976503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:51.772881, step: 1103, loss: 0.00036170962266623974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:51.983398, step: 1104, loss: 0.0005776113830506802, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:52.209502, step: 1105, loss: 0.0004966075648553669, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:52.425129, step: 1106, loss: 0.0007744258618913591, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:52.640882, step: 1107, loss: 0.0006964734056964517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:52.861099, step: 1108, loss: 0.0009427834884263575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:53.079204, step: 1109, loss: 0.0008115688106045127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:53.291706, step: 1110, loss: 0.0006394088268280029, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:53.512610, step: 1111, loss: 0.0006200130446814001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:53.729018, step: 1112, loss: 0.0004981897654943168, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:53.955257, step: 1113, loss: 0.0005045977886766195, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:54.170553, step: 1114, loss: 0.0010780789889395237, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:54.396612, step: 1115, loss: 0.0005893987836316228, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:54.608813, step: 1116, loss: 0.0006592039717361331, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:54.835296, step: 1117, loss: 0.00047171846381388605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:55.057132, step: 1118, loss: 0.0006123153143562376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:55.272314, step: 1119, loss: 0.0008576171239838004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:55.493541, step: 1120, loss: 0.0005384740652516484, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:55.713597, step: 1121, loss: 0.0005908994935452938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:39:55.922934, step: 1122, loss: 0.0005598270217888057, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:56.138873, step: 1123, loss: 0.0009896311676129699, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:56.348135, step: 1124, loss: 0.0005714620347134769, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:56.563479, step: 1125, loss: 0.0006198115879669785, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:56.775865, step: 1126, loss: 0.0007230233168229461, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:56.990335, step: 1127, loss: 0.0007823624182492495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:57.198560, step: 1128, loss: 0.0004825627547688782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:57.436103, step: 1129, loss: 0.0005679180030710995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:57.648655, step: 1130, loss: 0.0009569653775542974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:57.874020, step: 1131, loss: 0.001084861229173839, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:58.098074, step: 1132, loss: 0.0008271135156974196, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:58.329719, step: 1133, loss: 0.000638830300886184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:58.555021, step: 1134, loss: 0.0007871753768995404, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:58.776814, step: 1135, loss: 0.0007944252574816346, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:59.008603, step: 1136, loss: 0.0011625406332314014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:59.237546, step: 1137, loss: 0.0009005153551697731, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:59.465301, step: 1138, loss: 0.0009347912855446339, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:59.706975, step: 1139, loss: 0.0005609682411886752, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:39:59.939123, step: 1140, loss: 0.0005490169860422611, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:00.154818, step: 1141, loss: 0.0007977616041898727, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:00.387526, step: 1142, loss: 0.0006635294994339347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:00.616275, step: 1143, loss: 0.0005717526073567569, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:00.847446, step: 1144, loss: 0.0007726030889898539, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:01.080030, step: 1145, loss: 0.000484054529806599, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:01.304934, step: 1146, loss: 0.000709039974026382, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:01.541202, step: 1147, loss: 0.000626237248070538, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:01.770357, step: 1148, loss: 0.0009054160909727216, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:01.991602, step: 1149, loss: 0.0007258601253852248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:02.216764, step: 1150, loss: 0.0007748952484689653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:02.439084, step: 1151, loss: 0.0005415839259512722, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:02.662959, step: 1152, loss: 0.000652097980491817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:02.887967, step: 1153, loss: 0.0007917809416539967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:03.119043, step: 1154, loss: 0.0006480422453023493, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:03.348118, step: 1155, loss: 0.0005219614831730723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:03.566024, step: 1156, loss: 0.00044822413474321365, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:03.792994, step: 1157, loss: 0.0004845967050641775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:03.995296, step: 1158, loss: 0.00082829047460109, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:04.205097, step: 1159, loss: 0.000528594886418432, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:04.411073, step: 1160, loss: 0.0005615695263259113, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:04.623095, step: 1161, loss: 0.0007933372398838401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:04.841552, step: 1162, loss: 0.0014098610263317823, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:05.050866, step: 1163, loss: 0.0007230543997138739, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:05.269724, step: 1164, loss: 0.0006599723128601909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:05.489257, step: 1165, loss: 0.0006682256353087723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:05.699293, step: 1166, loss: 0.0009299025405198336, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:05.910473, step: 1167, loss: 0.0004233898362144828, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:06.119643, step: 1168, loss: 0.00042232160922139883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:06.336679, step: 1169, loss: 0.0011193860555067658, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:06.560780, step: 1170, loss: 0.0008023579721339047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:06.777866, step: 1171, loss: 0.00037518044700846076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:06.986004, step: 1172, loss: 0.0006502429023385048, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:07.194109, step: 1173, loss: 0.0005997676635161042, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:07.405760, step: 1174, loss: 0.0005381216178648174, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:07.621093, step: 1175, loss: 0.0006535333814099431, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:07.831352, step: 1176, loss: 0.0005673550767824054, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:08.049236, step: 1177, loss: 0.0007155522471293807, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:08.256222, step: 1178, loss: 0.0007603055564686656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:08.459910, step: 1179, loss: 0.00036773766623809934, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:08.666389, step: 1180, loss: 0.0004663325089495629, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:08.874623, step: 1181, loss: 0.0013785738265141845, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:09.079120, step: 1182, loss: 0.0008615473052486777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:09.293717, step: 1183, loss: 0.0009215304162353277, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:09.498896, step: 1184, loss: 0.00042889604810625315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:09.728010, step: 1185, loss: 0.0004986691637896001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:09.946604, step: 1186, loss: 0.0011657243594527245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:10.161839, step: 1187, loss: 0.0007093228050507605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:10.372883, step: 1188, loss: 0.0004819415044039488, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:10.581989, step: 1189, loss: 0.0005436913343146443, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:10.797137, step: 1190, loss: 0.0007304708706215024, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:11.013895, step: 1191, loss: 0.0005321678472682834, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:11.238837, step: 1192, loss: 0.0006716061034239829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:40:11.459219, step: 1193, loss: 0.0005103857256472111, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:11.669858, step: 1194, loss: 0.0004546598647721112, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:11.889052, step: 1195, loss: 0.0008936439990065992, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:12.102019, step: 1196, loss: 0.000601267209276557, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:12.309532, step: 1197, loss: 0.0007346546626649797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:12.532072, step: 1198, loss: 0.0006104321219027042, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:12.747333, step: 1199, loss: 0.0009842485887929797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:12.975178, step: 1200, loss: 0.0005383463576436043, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:40:21.419121, step: 1200, loss: 0.5561796808854128, acc: 0.8671897435897435, auc: 0.9445307692307691, precision: 0.8798333333333335, recall: 0.8544256410256412\n",
      "2019-01-14T17:40:21.632731, step: 1201, loss: 0.00046139495680108666, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:21.862017, step: 1202, loss: 0.00048290376435033977, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:22.080013, step: 1203, loss: 0.000584756548050791, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:22.299556, step: 1204, loss: 0.0011284452630206943, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:22.522229, step: 1205, loss: 0.000546670111361891, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:22.738215, step: 1206, loss: 0.0006507016369141638, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:22.952128, step: 1207, loss: 0.0007422842318192124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:23.163130, step: 1208, loss: 0.0010408717207610607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:23.384204, step: 1209, loss: 0.0005016376962885261, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:23.619888, step: 1210, loss: 0.00043511122930794954, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:23.836023, step: 1211, loss: 0.0005480217514559627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:24.067188, step: 1212, loss: 0.0009283729596063495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:24.275743, step: 1213, loss: 0.0003896321286447346, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:24.488011, step: 1214, loss: 0.00053721375297755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:24.717415, step: 1215, loss: 0.0006164629012346268, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:24.941586, step: 1216, loss: 0.0009051240631379187, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:25.154517, step: 1217, loss: 0.0006435620598495007, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:25.372037, step: 1218, loss: 0.0004080838989466429, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:25.594148, step: 1219, loss: 0.0006288632284849882, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:25.806257, step: 1220, loss: 0.0005662668263539672, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:26.038295, step: 1221, loss: 0.0006017686100676656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:26.261022, step: 1222, loss: 0.0008207922801375389, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:26.473402, step: 1223, loss: 0.0005744315567426383, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:26.696948, step: 1224, loss: 0.0007748808711767197, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:26.913132, step: 1225, loss: 0.000522779009770602, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:27.134648, step: 1226, loss: 0.0006967111257836223, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:27.363319, step: 1227, loss: 0.0011147639015689492, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:27.577341, step: 1228, loss: 0.00043076497968286276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:27.808590, step: 1229, loss: 0.0007415114669129252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:28.031510, step: 1230, loss: 0.0011866056593135, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:28.254745, step: 1231, loss: 0.0006699939840473235, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:28.467533, step: 1232, loss: 0.0004587387084029615, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:28.683342, step: 1233, loss: 0.0009837911929935217, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:28.895085, step: 1234, loss: 0.0007379770977422595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:29.109356, step: 1235, loss: 0.0010654476936906576, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:29.332821, step: 1236, loss: 0.0005929194740019739, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:29.566310, step: 1237, loss: 0.0009828894399106503, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:29.778914, step: 1238, loss: 0.00023844178940635175, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:30.004197, step: 1239, loss: 0.00039868749445304275, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:30.219239, step: 1240, loss: 0.0007502996013499796, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:30.449666, step: 1241, loss: 0.0005589275388047099, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:30.658996, step: 1242, loss: 0.0007374325068667531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:30.877191, step: 1243, loss: 0.0005384444957599044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:31.093706, step: 1244, loss: 0.0007021211786195636, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:31.317630, step: 1245, loss: 0.0004934908938594162, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:31.530532, step: 1246, loss: 0.0004858602478634566, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:31.758418, step: 1247, loss: 0.000492518418468535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:31.982343, step: 1248, loss: 0.000784055795520544, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:40:32.216671, step: 1249, loss: 0.000510493409819901, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:32.442099, step: 1250, loss: 0.00048168544890359044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:32.667898, step: 1251, loss: 0.0003305938735138625, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:32.881012, step: 1252, loss: 0.0003913643304258585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:33.087201, step: 1253, loss: 0.0004062072839587927, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:33.321375, step: 1254, loss: 0.0007232304778881371, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:33.561978, step: 1255, loss: 0.0004594051861204207, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:33.784541, step: 1256, loss: 0.0004601737018674612, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:34.007400, step: 1257, loss: 0.0003575378214009106, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:34.230154, step: 1258, loss: 0.0006198443588800728, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:34.465313, step: 1259, loss: 0.0004753987886942923, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:34.701146, step: 1260, loss: 0.00036009904579259455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:34.932432, step: 1261, loss: 0.0005875729839317501, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:40:35.161514, step: 1262, loss: 0.0004640849947463721, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:35.384545, step: 1263, loss: 0.0009571412811055779, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:35.611079, step: 1264, loss: 0.0005451912875287235, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:35.825313, step: 1265, loss: 0.0006051945965737104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:36.046194, step: 1266, loss: 0.0005873009795323014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:36.273317, step: 1267, loss: 0.0008714295108802617, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:36.484196, step: 1268, loss: 0.0004891602438874543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:36.683800, step: 1269, loss: 0.0005582412704825401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:36.888160, step: 1270, loss: 0.0007355185807682574, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:37.095013, step: 1271, loss: 0.0004979451769031584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:37.302050, step: 1272, loss: 0.00045826000859960914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:37.537177, step: 1273, loss: 0.0004813065752387047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:37.754513, step: 1274, loss: 0.0005227668443694711, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:37.962784, step: 1275, loss: 0.0008669540984556079, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:38.184433, step: 1276, loss: 0.0007320517906919122, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:38.395794, step: 1277, loss: 0.0003601807402446866, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:38.591609, step: 1278, loss: 0.0007083894452080131, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:38.818082, step: 1279, loss: 0.0004917783080600202, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:39.032410, step: 1280, loss: 0.0004287021001800895, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:39.234892, step: 1281, loss: 0.0002225418866146356, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:39.466619, step: 1282, loss: 0.0005549992201849818, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:39.672302, step: 1283, loss: 0.0006008789641782641, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:39.874360, step: 1284, loss: 0.0005734452861361206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:40.073468, step: 1285, loss: 0.0003854990645777434, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:40.280430, step: 1286, loss: 0.0012384520377963781, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:40.503972, step: 1287, loss: 0.0004307073540985584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:40.716639, step: 1288, loss: 0.00041381968185305595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:40.942607, step: 1289, loss: 0.00043094196007587016, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:41.159931, step: 1290, loss: 0.000668462656904012, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:41.389714, step: 1291, loss: 0.0007497402257286012, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:41.613196, step: 1292, loss: 0.0003698995860759169, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:41.825966, step: 1293, loss: 0.0005678395973518491, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:42.046763, step: 1294, loss: 0.0005644270568154752, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:42.273666, step: 1295, loss: 0.000669812667183578, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:42.486719, step: 1296, loss: 0.000297734746709466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:42.717604, step: 1297, loss: 0.0006373417563736439, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:42.930386, step: 1298, loss: 0.00037145393434911966, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:43.155239, step: 1299, loss: 0.00029088405426591635, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:43.380437, step: 1300, loss: 0.000480074028018862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:40:51.839411, step: 1300, loss: 0.5679965882729261, acc: 0.8687871794871793, auc: 0.9434461538461539, precision: 0.8848948717948716, recall: 0.851697435897436\n",
      "2019-01-14T17:40:52.049361, step: 1301, loss: 0.0005836267955601215, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:52.263597, step: 1302, loss: 0.0003373779763933271, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:52.478659, step: 1303, loss: 0.0007388474186882377, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:52.702799, step: 1304, loss: 0.000700602657161653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:52.919906, step: 1305, loss: 0.0006018547574058175, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:53.155563, step: 1306, loss: 0.00035976740764454007, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:53.377796, step: 1307, loss: 0.0006756221409887075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:53.601827, step: 1308, loss: 0.0006434692186303437, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:53.817225, step: 1309, loss: 0.0003879795840475708, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:54.032428, step: 1310, loss: 0.0006020534783601761, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:54.256918, step: 1311, loss: 0.0005301358760334551, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:54.475272, step: 1312, loss: 0.0011291144182905555, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:54.688418, step: 1313, loss: 0.0005000867531634867, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:54.913652, step: 1314, loss: 0.0006086322828195989, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:55.125088, step: 1315, loss: 0.000585361325647682, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:55.355271, step: 1316, loss: 0.0003509715315885842, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:55.593372, step: 1317, loss: 0.0004184414865449071, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:55.822378, step: 1318, loss: 0.0001920897193485871, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:56.047617, step: 1319, loss: 0.0005744786467403173, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:56.259290, step: 1320, loss: 0.0005189959774725139, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:56.483298, step: 1321, loss: 0.00033161736791953444, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:56.702428, step: 1322, loss: 0.0005975739913992584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:56.928891, step: 1323, loss: 0.00045661343028768897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:57.162759, step: 1324, loss: 0.000555172679014504, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:57.392706, step: 1325, loss: 0.0003152833669446409, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:57.607703, step: 1326, loss: 0.0005613558460026979, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:57.829636, step: 1327, loss: 0.00046894344268366694, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:58.056296, step: 1328, loss: 0.0004096926422789693, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:58.289015, step: 1329, loss: 0.0004873582220170647, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:58.513627, step: 1330, loss: 0.00041988829616457224, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:40:58.725830, step: 1331, loss: 0.0006508742226287723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:58.948654, step: 1332, loss: 0.00030878910911269486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:59.168959, step: 1333, loss: 0.000474242347991094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:59.390431, step: 1334, loss: 0.0005237223813310266, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:59.612611, step: 1335, loss: 0.0004099541692994535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:40:59.825248, step: 1336, loss: 0.00033359654480591416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:00.045788, step: 1337, loss: 0.0005129019846208394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:00.276449, step: 1338, loss: 0.0005906494916416705, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:00.507305, step: 1339, loss: 0.00045492753270082176, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:00.720988, step: 1340, loss: 0.0004081152437720448, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:00.943338, step: 1341, loss: 0.0005138367996551096, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:01.161620, step: 1342, loss: 0.000599449675064534, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:01.370878, step: 1343, loss: 0.0003877272247336805, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:01.588838, step: 1344, loss: 0.0002616005949676037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:01.814768, step: 1345, loss: 0.0004726633196696639, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:02.058989, step: 1346, loss: 0.0006516617140732706, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:02.282948, step: 1347, loss: 0.0006386438035406172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:02.506869, step: 1348, loss: 0.0003863394376821816, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:02.728458, step: 1349, loss: 0.00039400835521519184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:02.950732, step: 1350, loss: 0.0005980399437248707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:03.177492, step: 1351, loss: 0.0007117147324606776, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:03.404077, step: 1352, loss: 0.0003867928171530366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:03.625219, step: 1353, loss: 0.00038951850729063153, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:03.849153, step: 1354, loss: 0.0006471285596489906, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:04.074857, step: 1355, loss: 0.0005916692898608744, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:04.287368, step: 1356, loss: 0.00034909782698377967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:04.512023, step: 1357, loss: 0.0006333186756819487, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:04.740274, step: 1358, loss: 0.0005065873847343028, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:04.956091, step: 1359, loss: 0.000346978020388633, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:05.177448, step: 1360, loss: 0.0006074216798879206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:05.405424, step: 1361, loss: 0.0006535642896778882, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:05.631280, step: 1362, loss: 0.00035278633004054427, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:05.845864, step: 1363, loss: 0.0003110409597866237, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:06.085993, step: 1364, loss: 0.00040598816121928394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:06.297870, step: 1365, loss: 0.0005218132864683867, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:06.536558, step: 1366, loss: 0.000357708428055048, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:06.765002, step: 1367, loss: 0.0006482508615590632, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:06.993922, step: 1368, loss: 0.0005129485507495701, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:07.204552, step: 1369, loss: 0.0004851225530728698, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:07.426289, step: 1370, loss: 0.00043889664812013507, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:07.635918, step: 1371, loss: 0.0005040254909545183, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:07.857558, step: 1372, loss: 0.0004374319687485695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:08.079991, step: 1373, loss: 0.0005185395712032914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:08.301827, step: 1374, loss: 0.0002898504608310759, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:08.530257, step: 1375, loss: 0.000467989215394482, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:08.770900, step: 1376, loss: 0.0004413612768985331, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:08.996009, step: 1377, loss: 0.0005241259932518005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:09.208152, step: 1378, loss: 0.0006342261913232505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:09.431523, step: 1379, loss: 0.00046524315257556736, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:09.653187, step: 1380, loss: 0.0006203360971994698, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:09.875387, step: 1381, loss: 0.0003380585403647274, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:10.102386, step: 1382, loss: 0.0005189816001802683, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:10.327227, step: 1383, loss: 0.000407289044233039, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:10.544245, step: 1384, loss: 0.0006352977361530066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:10.765100, step: 1385, loss: 0.00043094553984701633, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:10.978996, step: 1386, loss: 0.00021260669745970517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:11.198955, step: 1387, loss: 0.0005782374646514654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:11.426256, step: 1388, loss: 0.0005890102474950254, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:11.646518, step: 1389, loss: 0.0003381886053830385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:11.870498, step: 1390, loss: 0.0005424309056252241, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:12.090748, step: 1391, loss: 0.0005367965204641223, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:12.313198, step: 1392, loss: 0.0005205869674682617, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:12.532510, step: 1393, loss: 0.00041330207022838295, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:12.734787, step: 1394, loss: 0.0007309331558644772, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:12.964455, step: 1395, loss: 0.0007468071999028325, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:13.174143, step: 1396, loss: 0.0004842365742661059, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:13.371396, step: 1397, loss: 0.0005444127600640059, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:13.590429, step: 1398, loss: 0.00042572003440000117, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:13.794805, step: 1399, loss: 0.0006846038158982992, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:14.013343, step: 1400, loss: 0.00038534292252734303, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:41:22.493404, step: 1400, loss: 0.5781745849511563, acc: 0.8703871794871796, auc: 0.943651282051282, precision: 0.8811410256410256, recall: 0.8593615384615384\n",
      "2019-01-14T17:41:22.699905, step: 1401, loss: 0.0003889158251695335, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:22.922957, step: 1402, loss: 0.0005008548032492399, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:23.133000, step: 1403, loss: 0.00042955056414939463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:23.362510, step: 1404, loss: 0.00033051069476641715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-14T17:41:23.599414, step: 1405, loss: 0.00042832503095269203, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:23.828530, step: 1406, loss: 0.0003468276700004935, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:24.052692, step: 1407, loss: 0.0003260426165070385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:24.280288, step: 1408, loss: 0.0006003656890243292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:24.507538, step: 1409, loss: 0.00026533915661275387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:24.723943, step: 1410, loss: 0.0004180106916464865, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:24.956809, step: 1411, loss: 0.00029660595464520156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:25.181183, step: 1412, loss: 0.0003282182151451707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:25.415725, step: 1413, loss: 0.0004128370783291757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:25.657214, step: 1414, loss: 0.0007163272239267826, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:25.888833, step: 1415, loss: 0.00048789838911034167, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:26.126991, step: 1416, loss: 0.0005540333222597837, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:26.351725, step: 1417, loss: 0.0003273982438258827, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:26.575883, step: 1418, loss: 0.000292887503746897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:26.812279, step: 1419, loss: 0.0007477890467271209, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:27.038434, step: 1420, loss: 0.000520071480423212, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:27.257004, step: 1421, loss: 0.0003482622851151973, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:27.483822, step: 1422, loss: 0.0005474586505442858, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:27.737092, step: 1423, loss: 0.00024280522484332323, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:27.966491, step: 1424, loss: 0.0005260233301669359, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:28.191759, step: 1425, loss: 0.0005572809022851288, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:28.416563, step: 1426, loss: 0.00031516500166617334, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:28.628790, step: 1427, loss: 0.00034249896998517215, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:28.858291, step: 1428, loss: 0.0004639144754037261, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:29.071882, step: 1429, loss: 0.0003054331464227289, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:29.282285, step: 1430, loss: 0.00026434348546899855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:29.505263, step: 1431, loss: 0.0002517913526389748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:29.728944, step: 1432, loss: 0.00036703303339891136, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:29.957442, step: 1433, loss: 0.0002646524226292968, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:30.169775, step: 1434, loss: 0.00039989460492506623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:30.387070, step: 1435, loss: 0.00047775404527783394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:30.609984, step: 1436, loss: 0.0005578234558925033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:30.831026, step: 1437, loss: 0.0005025013233534992, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:31.048803, step: 1438, loss: 0.0004769850929733366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:31.272931, step: 1439, loss: 0.0005734386504627764, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:31.493988, step: 1440, loss: 0.0005500578554347157, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:31.721882, step: 1441, loss: 0.0003527699154801667, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:31.949237, step: 1442, loss: 0.0002320511848665774, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:32.171854, step: 1443, loss: 0.0004129204899072647, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:32.401103, step: 1444, loss: 0.00043012990499846637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:32.628960, step: 1445, loss: 0.00041969813173636794, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:32.859993, step: 1446, loss: 0.0003808238252531737, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:33.083062, step: 1447, loss: 0.00016430778487119824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:33.295952, step: 1448, loss: 0.0005768559058196843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:33.516792, step: 1449, loss: 0.0003242954262532294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:33.740468, step: 1450, loss: 0.00040909519884735346, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:33.964149, step: 1451, loss: 0.00024981205933727324, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:34.182319, step: 1452, loss: 0.0003323393175378442, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:34.404422, step: 1453, loss: 0.00040749169420450926, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:34.630430, step: 1454, loss: 0.00045201322063803673, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:34.858088, step: 1455, loss: 0.0005341981304809451, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:35.070316, step: 1456, loss: 0.0003017402486875653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:35.293416, step: 1457, loss: 0.00048563405289314687, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:35.516985, step: 1458, loss: 0.0005520544364117086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:35.744402, step: 1459, loss: 0.00026115175569429994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:35.960696, step: 1460, loss: 0.0003376907843630761, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:36.184562, step: 1461, loss: 0.00029883813112974167, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:36.408801, step: 1462, loss: 0.00021444447338581085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:36.624386, step: 1463, loss: 0.0005963643197901547, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:36.834877, step: 1464, loss: 0.0003157823230139911, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:37.063908, step: 1465, loss: 0.0003821636491920799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:37.286325, step: 1466, loss: 0.0002877239021472633, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:37.528395, step: 1467, loss: 0.00043186728726141155, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:37.746988, step: 1468, loss: 0.00033906198223121464, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:37.974275, step: 1469, loss: 0.0003092692350037396, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:41:38.195142, step: 1470, loss: 0.00048099696869030595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:38.423209, step: 1471, loss: 0.000317645026370883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:38.637039, step: 1472, loss: 0.0003479375736787915, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:38.856734, step: 1473, loss: 0.0005590292275883257, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:39.083826, step: 1474, loss: 0.0005216295248828828, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:39.306642, step: 1475, loss: 0.0004275880637578666, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:39.536260, step: 1476, loss: 0.00029149404144845903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:39.737988, step: 1477, loss: 0.0003706288989633322, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:39.956630, step: 1478, loss: 0.0004799171583727002, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:40.181568, step: 1479, loss: 0.0002564071037340909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:40.393090, step: 1480, loss: 0.00034970263368450105, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:40.609076, step: 1481, loss: 0.00039680133340880275, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:40.819493, step: 1482, loss: 0.00028279420803301036, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:41.026994, step: 1483, loss: 0.0003822904545813799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:41.251586, step: 1484, loss: 0.0003602101351134479, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:41.484404, step: 1485, loss: 0.00027092243544757366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:41.722358, step: 1486, loss: 0.00036963593447580934, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:41.945134, step: 1487, loss: 0.00042528146877884865, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:42.164811, step: 1488, loss: 0.0004918318591080606, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:42.384529, step: 1489, loss: 0.00038181652780622244, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:42.608160, step: 1490, loss: 0.000297127990052104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:42.819923, step: 1491, loss: 0.0004972477909177542, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:43.043643, step: 1492, loss: 0.00017256358114536852, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:43.260257, step: 1493, loss: 0.0004630877810996026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:43.479867, step: 1494, loss: 0.000323980551911518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:43.703611, step: 1495, loss: 0.00027293924358673394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:43.924270, step: 1496, loss: 0.00046016171108931303, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:44.143698, step: 1497, loss: 0.00036349831498228014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:44.351937, step: 1498, loss: 0.00027833072817884386, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:44.567301, step: 1499, loss: 0.0002869191230274737, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:44.789304, step: 1500, loss: 0.00028229301096871495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-14T17:41:53.364200, step: 1500, loss: 0.5821613302597632, acc: 0.8707999999999999, auc: 0.9456179487179486, precision: 0.8838871794871793, recall: 0.8553128205128209\n",
      "2019-01-14T17:41:53.581335, step: 1501, loss: 0.00030334776965901256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:53.805320, step: 1502, loss: 0.00037576467730104923, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:54.030378, step: 1503, loss: 0.0002251219266327098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:54.242761, step: 1504, loss: 0.0007541757076978683, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:54.466361, step: 1505, loss: 0.00041186041198670864, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:54.684173, step: 1506, loss: 0.0007539645885117352, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:54.897256, step: 1507, loss: 0.0004295899416320026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:55.135597, step: 1508, loss: 0.0003775992663577199, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:55.361858, step: 1509, loss: 0.0004186679143458605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:55.573671, step: 1510, loss: 0.00029324242495931685, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:55.797148, step: 1511, loss: 0.0007296258118003607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:56.005530, step: 1512, loss: 0.0002527515171095729, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:56.227092, step: 1513, loss: 0.00034411324304528534, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:56.445423, step: 1514, loss: 0.000286009453702718, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:56.668949, step: 1515, loss: 0.0001982853573281318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:56.890707, step: 1516, loss: 0.0005322153447195888, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:57.102997, step: 1517, loss: 0.00046676734928041697, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:57.329696, step: 1518, loss: 0.0005444273119792342, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:57.556098, step: 1519, loss: 0.00039209151873365045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:57.790450, step: 1520, loss: 0.0005851112073287368, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:58.016381, step: 1521, loss: 0.00024820154067128897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:58.241697, step: 1522, loss: 0.0003471768577583134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:58.454878, step: 1523, loss: 0.00028646388091146946, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:58.695375, step: 1524, loss: 0.00022496463498100638, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:58.907471, step: 1525, loss: 0.0006429087370634079, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:59.127663, step: 1526, loss: 0.00036756074405275285, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:59.352355, step: 1527, loss: 0.00042116022086702287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:59.567105, step: 1528, loss: 0.0005645225755870342, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:41:59.790883, step: 1529, loss: 0.00037326078745536506, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:00.027997, step: 1530, loss: 0.0008001062087714672, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:00.252727, step: 1531, loss: 0.0005770253483206034, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:00.478694, step: 1532, loss: 0.0002188368234783411, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:00.702500, step: 1533, loss: 0.0003905448829755187, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:00.920143, step: 1534, loss: 0.00027936784317716956, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:01.149915, step: 1535, loss: 0.000376200710888952, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:01.379493, step: 1536, loss: 0.0005600282456725836, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:01.610738, step: 1537, loss: 0.0005429005832411349, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:01.823077, step: 1538, loss: 0.0003003657911904156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-14T17:42:02.048674, step: 1539, loss: 0.000473079620860517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:02.276827, step: 1540, loss: 0.00030003493884578347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:02.509889, step: 1541, loss: 0.0005028191953897476, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:02.743692, step: 1542, loss: 0.000595231307670474, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:02.963707, step: 1543, loss: 0.00041080074151977897, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:03.161071, step: 1544, loss: 0.00044510929728858173, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:03.371591, step: 1545, loss: 0.0003769984468817711, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:03.577502, step: 1546, loss: 0.0003927599755115807, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:03.791379, step: 1547, loss: 0.00036345107946544886, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:04.019119, step: 1548, loss: 0.0004279219720046967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:04.233378, step: 1549, loss: 0.0003985227085649967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:04.449109, step: 1550, loss: 0.0004524518735706806, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:04.670318, step: 1551, loss: 0.0004479234921745956, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:04.866830, step: 1552, loss: 0.00041337008588016033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:05.078081, step: 1553, loss: 0.00039711385034024715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:05.286773, step: 1554, loss: 0.00044133508345112205, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:05.517470, step: 1555, loss: 0.00030175087158568203, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:05.743644, step: 1556, loss: 0.00043449224904179573, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:05.957744, step: 1557, loss: 0.0005181645974516869, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:06.180854, step: 1558, loss: 0.000631424249149859, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:06.402450, step: 1559, loss: 0.00037874607369303703, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-14T17:42:06.625274, step: 1560, loss: 0.00024284656683448702, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "embeddedPosition = fixedPositionEmbedding(config.batchSize, config.sequenceLength)\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Transformer/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: 1.0,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Transformer/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(transformer.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(transformer.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(transformer.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
