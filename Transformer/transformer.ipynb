{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    filters = 128  # 内层一维卷积核的数量，外层卷积核的数量应该等于embeddingSize\n",
    "    numHeads = 8  # Attention 的头数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成位置嵌入\n",
    "def positionEmbedding(batchSize, sequenceLen):\n",
    "    embeddedPosition = []\n",
    "    for batch in range(batchSize):\n",
    "        x = []\n",
    "        for step in range(sequenceLen):\n",
    "            a = np.zeros(sequenceLen)\n",
    "            a[step] = 1\n",
    "            x.append(a)\n",
    "        embeddedPosition.append(x)\n",
    "    \n",
    "    return np.array(embeddedPosition, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.embeddedPosition = tf.placeholder(tf.float32, [None, config.sequenceLength, config.sequenceLength], name=\"embeddedPosition\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embedded = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            self.embeddedWords = tf.concat([self.embedded, self.embeddedPosition], -1)\n",
    "        \n",
    "            \n",
    "        with tf.name_scope(\"transformer\"):\n",
    "            \n",
    "            # 维度[batch_size, sequence_length, embedding_size]\n",
    "            multiHeadAtt = self.multiheadAttention(queries=self.embeddedWords, keys=self.embeddedWords)\n",
    "            # 维度[batch_size, sequence_length, embedding_size]\n",
    "            outputs = self.feedForward(multiHeadAtt, [config.model.filters, config.model.embeddingSize + config.sequenceLength])\n",
    "            outputs = tf.reshape(outputs, [-1, config.sequenceLength * (config.model.embeddingSize + config.sequenceLength)])\n",
    "\n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def layerNormalization(self, inputs, epsilon=1e-8, scope=\"layerNorm\", reuse=None):\n",
    "        # 本质上可以看作是BN层\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            inputsShape = inputs.get_shape() # [batch_size, sequence_length, embedding_size]\n",
    "            paramsShape = inputsShape[-1:]\n",
    "  \n",
    "            # 在最后的维度上计算输入的数据的均值和方差\n",
    "            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "            beta = tf.Variable(tf.zeros(paramsShape))\n",
    "            gamma = tf.Variable(tf.ones(paramsShape))\n",
    "            normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "            outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def multiheadAttention(self, queries, keys, numUnits=None, numHeads=8, causality=False, scope=\"multiheadAttention\", reuse=None):\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "                numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "            # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "            # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "            # Q, K, V的维度都是[batch_size, sequence_length, embedding_size]\n",
    "            Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "            K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "            V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "            \n",
    "            # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "            # Q, K, V 的维度都是[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "            Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "            K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "            V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "            \n",
    "            # 计算keys和queries之间的点积，维度[batch_size * numHeads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "            similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "            \n",
    "            # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "            scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "            \n",
    "            # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "            # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "            # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "            # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "            # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "            \n",
    "            # 将每一时序上的向量中的值相加取平均值\n",
    "            keyMasks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # 维度[batch_size, time_step]\n",
    "            \n",
    "            # 利用tf，tile进行张量扩张， 维度[batch_size * numHeads, keys_len] keys_len = keys 的序列长度\n",
    "            keyMasks = tf.tile(keyMasks, [numHeads, 1]) \n",
    "            \n",
    "            # 增加一个维度，并进行扩张，得到维度[batch_size * numHeads, queries_len, keys_len]\n",
    "            keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "            \n",
    "            # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "            paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "            \n",
    "            # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "            # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "            maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * numHeads, queries_len, key_len]\n",
    "            \n",
    "            # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "            # Decoder是生成模型，主要用在语言生成中\n",
    "            if causality:\n",
    "                diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "                tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "                masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "                paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "                maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * numHeads, queries_len, keys_len]\n",
    "\n",
    "            # 通过softmax计算权重系数，维度 [batch_size * numHeads, queries_len, keys_len]\n",
    "            weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "            # 加权和得到输出值, 维度[batch_size * numHeads, sequence_length, embedding_size/numHeads]\n",
    "            outputs = tf.matmul(weights, V_)\n",
    "            \n",
    "            # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, sequence_length, embedding_size]\n",
    "            outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "\n",
    "            # 建立残差连接\n",
    "            outputs += queries\n",
    "            # normalization 层\n",
    "            outputs = self.layerNormalization(outputs)\n",
    "            return outputs\n",
    "\n",
    "    def feedForward(self, inputs, filters, scope=\"multiheadAttention\", reuse=None):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # 内层\n",
    "            params = {\"inputs\": inputs, \"filters\": filters[0], \"kernel_size\": 1,\n",
    "                      \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "            # 外层\n",
    "            params = {\"inputs\": outputs, \"filters\": filters[1], \"kernel_size\": 1,\n",
    "                      \"activation\": None, \"use_bias\": True}\n",
    "            \n",
    "            # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "            # 维度[batch_size, sequence_length, embedding_size]\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "            \n",
    "            # 残差连接\n",
    "            outputs += inputs\n",
    "\n",
    "            # 归一化处理\n",
    "            outputs = self.layerNormalization(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense/kernel:0/grad/hist is illegal; using multiheadAttention/dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense/kernel:0/grad/sparsity is illegal; using multiheadAttention/dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense/bias:0/grad/hist is illegal; using multiheadAttention/dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense/bias:0/grad/sparsity is illegal; using multiheadAttention/dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_1/kernel:0/grad/hist is illegal; using multiheadAttention/dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_1/kernel:0/grad/sparsity is illegal; using multiheadAttention/dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_1/bias:0/grad/hist is illegal; using multiheadAttention/dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_1/bias:0/grad/sparsity is illegal; using multiheadAttention/dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_2/kernel:0/grad/hist is illegal; using multiheadAttention/dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_2/kernel:0/grad/sparsity is illegal; using multiheadAttention/dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_2/bias:0/grad/hist is illegal; using multiheadAttention/dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/dense_2/bias:0/grad/sparsity is illegal; using multiheadAttention/dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention/layerNorm/Variable:0/grad/hist is illegal; using transformer/multiheadAttention/layerNorm/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention/layerNorm/Variable:0/grad/sparsity is illegal; using transformer/multiheadAttention/layerNorm/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention/layerNorm/Variable_1:0/grad/hist is illegal; using transformer/multiheadAttention/layerNorm/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention/layerNorm/Variable_1:0/grad/sparsity is illegal; using transformer/multiheadAttention/layerNorm/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/kernel:0/grad/hist is illegal; using multiheadAttention/conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/kernel:0/grad/sparsity is illegal; using multiheadAttention/conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/bias:0/grad/hist is illegal; using multiheadAttention/conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/bias:0/grad/sparsity is illegal; using multiheadAttention/conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/kernel:0/grad/hist is illegal; using multiheadAttention/conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/kernel:0/grad/sparsity is illegal; using multiheadAttention/conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/bias:0/grad/hist is illegal; using multiheadAttention/conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/bias:0/grad/sparsity is illegal; using multiheadAttention/conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention_1/layerNorm/Variable:0/grad/hist is illegal; using transformer/multiheadAttention_1/layerNorm/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention_1/layerNorm/Variable:0/grad/sparsity is illegal; using transformer/multiheadAttention_1/layerNorm/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention_1/layerNorm/Variable_1:0/grad/hist is illegal; using transformer/multiheadAttention_1/layerNorm/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name transformer/multiheadAttention_1/layerNorm/Variable_1:0/grad/sparsity is illegal; using transformer/multiheadAttention_1/layerNorm/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Transformer/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-07T15:28:51.239169, step: 1, loss: 1.0996532440185547, acc: 0.4844, auc: 0.4059, precision: 0.0, recall: 0.0\n",
      "2019-01-07T15:28:51.453576, step: 2, loss: 25.251907348632812, acc: 0.4844, auc: 0.5039, precision: 0.4844, recall: 1.0\n",
      "2019-01-07T15:28:51.641938, step: 3, loss: 13.734840393066406, acc: 0.4922, auc: 0.4208, precision: 0.4922, recall: 1.0\n",
      "2019-01-07T15:28:51.845332, step: 4, loss: 2.7334370613098145, acc: 0.4922, auc: 0.4713, precision: 0.0, recall: 0.0\n",
      "2019-01-07T15:28:52.051695, step: 5, loss: 3.656604290008545, acc: 0.5391, auc: 0.4503, precision: 0.0, recall: 0.0\n",
      "2019-01-07T15:28:52.264781, step: 6, loss: 1.508324384689331, acc: 0.4766, auc: 0.486, precision: 0.4818, recall: 0.8413\n",
      "2019-01-07T15:28:52.480661, step: 7, loss: 1.6156209707260132, acc: 0.5391, auc: 0.4765, precision: 0.5463, recall: 0.8551\n",
      "2019-01-07T15:28:52.695435, step: 8, loss: 1.785152792930603, acc: 0.4844, auc: 0.515, precision: 0.6, recall: 0.0448\n",
      "2019-01-07T15:28:52.899309, step: 9, loss: 1.2430219650268555, acc: 0.4766, auc: 0.4934, precision: 0.4762, recall: 0.1515\n",
      "2019-01-07T15:28:53.119129, step: 10, loss: 1.902457594871521, acc: 0.5156, auc: 0.5411, precision: 0.5159, recall: 0.9848\n",
      "2019-01-07T15:28:53.322659, step: 11, loss: 1.2278790473937988, acc: 0.5703, auc: 0.5554, precision: 0.5682, recall: 0.7463\n",
      "2019-01-07T15:28:53.544511, step: 12, loss: 1.7477996349334717, acc: 0.5078, auc: 0.5277, precision: 0.5833, recall: 0.1077\n",
      "2019-01-07T15:28:53.753516, step: 13, loss: 1.772986888885498, acc: 0.4688, auc: 0.4917, precision: 0.8462, recall: 0.1429\n",
      "2019-01-07T15:28:53.960800, step: 14, loss: 1.8274016380310059, acc: 0.5703, auc: 0.4875, precision: 0.5678, recall: 0.9437\n",
      "2019-01-07T15:28:54.173539, step: 15, loss: 2.5362300872802734, acc: 0.4453, auc: 0.5821, precision: 0.4333, recall: 0.9455\n",
      "2019-01-07T15:28:54.372466, step: 16, loss: 1.21439528465271, acc: 0.5234, auc: 0.5453, precision: 0.4821, recall: 0.4576\n",
      "2019-01-07T15:28:54.574596, step: 17, loss: 2.6289682388305664, acc: 0.4766, auc: 0.5453, precision: 0.6667, recall: 0.0294\n",
      "2019-01-07T15:28:54.788186, step: 18, loss: 1.7945910692214966, acc: 0.6016, auc: 0.5723, precision: 0.75, recall: 0.0566\n",
      "2019-01-07T15:28:55.002021, step: 19, loss: 1.2423632144927979, acc: 0.5, auc: 0.5277, precision: 0.5455, recall: 0.3529\n",
      "2019-01-07T15:28:55.204948, step: 20, loss: 2.2807068824768066, acc: 0.4609, auc: 0.5355, precision: 0.464, recall: 0.9667\n",
      "2019-01-07T15:28:55.407335, step: 21, loss: 1.4188320636749268, acc: 0.5938, auc: 0.6468, precision: 0.5714, recall: 0.9855\n",
      "2019-01-07T15:28:55.620350, step: 22, loss: 0.9603779315948486, acc: 0.5469, auc: 0.607, precision: 0.5769, recall: 0.4545\n",
      "2019-01-07T15:28:55.830672, step: 23, loss: 1.7173852920532227, acc: 0.5, auc: 0.5275, precision: 0.3333, recall: 0.0492\n",
      "2019-01-07T15:28:56.057869, step: 24, loss: 1.558422565460205, acc: 0.5391, auc: 0.5371, precision: 0.6667, recall: 0.0968\n",
      "2019-01-07T15:28:56.271315, step: 25, loss: 0.9029677510261536, acc: 0.6016, auc: 0.6283, precision: 0.5902, recall: 0.5806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:28:56.481019, step: 26, loss: 1.6133065223693848, acc: 0.4688, auc: 0.5696, precision: 0.4649, recall: 0.8833\n",
      "2019-01-07T15:28:56.691030, step: 27, loss: 0.9087508916854858, acc: 0.625, auc: 0.6146, precision: 0.6477, recall: 0.7703\n",
      "2019-01-07T15:28:56.894477, step: 28, loss: 0.8405101299285889, acc: 0.5781, auc: 0.6136, precision: 0.5641, recall: 0.3729\n",
      "2019-01-07T15:28:57.105792, step: 29, loss: 1.3453201055526733, acc: 0.5312, auc: 0.621, precision: 0.7143, recall: 0.0794\n",
      "2019-01-07T15:28:57.308600, step: 30, loss: 1.050517201423645, acc: 0.5703, auc: 0.5919, precision: 0.619, recall: 0.2167\n",
      "2019-01-07T15:28:57.523163, step: 31, loss: 0.7605966925621033, acc: 0.6328, auc: 0.7153, precision: 0.5616, recall: 0.7321\n",
      "2019-01-07T15:28:57.736532, step: 32, loss: 1.038586139678955, acc: 0.5625, auc: 0.6392, precision: 0.5125, recall: 0.7069\n",
      "2019-01-07T15:28:57.945155, step: 33, loss: 0.8473740816116333, acc: 0.5859, auc: 0.6155, precision: 0.6604, recall: 0.5\n",
      "2019-01-07T15:28:58.163001, step: 34, loss: 0.9244452118873596, acc: 0.5703, auc: 0.6119, precision: 0.7059, recall: 0.3478\n",
      "2019-01-07T15:28:58.361888, step: 35, loss: 0.8984057307243347, acc: 0.5625, auc: 0.5668, precision: 0.5778, recall: 0.4127\n",
      "2019-01-07T15:28:58.578324, step: 36, loss: 0.6767746210098267, acc: 0.6406, auc: 0.7226, precision: 0.6308, recall: 0.6508\n",
      "2019-01-07T15:28:58.791610, step: 37, loss: 0.7823083996772766, acc: 0.6016, auc: 0.6409, precision: 0.6, recall: 0.6094\n",
      "2019-01-07T15:28:59.005509, step: 38, loss: 0.6877690553665161, acc: 0.6172, auc: 0.7101, precision: 0.6364, recall: 0.459\n",
      "2019-01-07T15:28:59.218321, step: 39, loss: 0.6268690824508667, acc: 0.6484, auc: 0.7924, precision: 0.8571, recall: 0.3692\n",
      "2019-01-07T15:28:59.425578, step: 40, loss: 0.6514815092086792, acc: 0.6797, auc: 0.7656, precision: 0.7143, recall: 0.5645\n",
      "2019-01-07T15:28:59.647975, step: 41, loss: 0.8992202281951904, acc: 0.5781, auc: 0.6138, precision: 0.5833, recall: 0.5469\n",
      "2019-01-07T15:28:59.864429, step: 42, loss: 0.8342380523681641, acc: 0.6406, auc: 0.6662, precision: 0.6667, recall: 0.6061\n",
      "2019-01-07T15:29:00.078505, step: 43, loss: 0.5855117440223694, acc: 0.7266, auc: 0.7737, precision: 0.8478, recall: 0.5821\n",
      "2019-01-07T15:29:00.281733, step: 44, loss: 0.6892085075378418, acc: 0.6641, auc: 0.7319, precision: 0.7568, recall: 0.4516\n",
      "2019-01-07T15:29:00.484656, step: 45, loss: 0.7181583642959595, acc: 0.6719, auc: 0.7192, precision: 0.6739, recall: 0.5345\n",
      "2019-01-07T15:29:00.708044, step: 46, loss: 0.6680701971054077, acc: 0.6719, auc: 0.7379, precision: 0.6038, recall: 0.6038\n",
      "2019-01-07T15:29:00.920821, step: 47, loss: 0.7359660863876343, acc: 0.6094, auc: 0.7188, precision: 0.6667, recall: 0.4375\n",
      "2019-01-07T15:29:01.134323, step: 48, loss: 0.5711116194725037, acc: 0.6797, auc: 0.8059, precision: 0.7037, recall: 0.6032\n",
      "2019-01-07T15:29:01.347454, step: 49, loss: 0.5554993152618408, acc: 0.7422, auc: 0.8149, precision: 0.7273, recall: 0.7619\n",
      "2019-01-07T15:29:01.549204, step: 50, loss: 0.6681599617004395, acc: 0.6875, auc: 0.7676, precision: 0.7241, recall: 0.6364\n",
      "2019-01-07T15:29:01.762165, step: 51, loss: 0.5483719706535339, acc: 0.7266, auc: 0.8189, precision: 0.7907, recall: 0.5667\n",
      "2019-01-07T15:29:01.969973, step: 52, loss: 0.5373345613479614, acc: 0.7344, auc: 0.8187, precision: 0.75, recall: 0.5893\n",
      "2019-01-07T15:29:02.185960, step: 53, loss: 0.5660156607627869, acc: 0.7109, auc: 0.7958, precision: 0.7917, recall: 0.5846\n",
      "2019-01-07T15:29:02.401083, step: 54, loss: 0.6461547613143921, acc: 0.6875, auc: 0.7787, precision: 0.6508, recall: 0.6949\n",
      "2019-01-07T15:29:02.604180, step: 55, loss: 0.6330192685127258, acc: 0.6875, auc: 0.7815, precision: 0.8235, recall: 0.4516\n",
      "2019-01-07T15:29:02.809873, step: 56, loss: 0.6932878494262695, acc: 0.6562, auc: 0.7429, precision: 0.6724, recall: 0.6094\n",
      "2019-01-07T15:29:03.030645, step: 57, loss: 0.5121971964836121, acc: 0.7891, auc: 0.8529, precision: 0.7619, recall: 0.8\n",
      "2019-01-07T15:29:03.245350, step: 58, loss: 0.6169431209564209, acc: 0.6719, auc: 0.809, precision: 0.8409, recall: 0.5139\n",
      "2019-01-07T15:29:03.457181, step: 59, loss: 0.6733827590942383, acc: 0.6797, auc: 0.7754, precision: 0.7302, recall: 0.6571\n",
      "2019-01-07T15:29:03.669397, step: 60, loss: 0.7456704378128052, acc: 0.7578, auc: 0.8421, precision: 0.6512, recall: 0.9825\n",
      "2019-01-07T15:29:03.889337, step: 61, loss: 0.47561365365982056, acc: 0.7734, auc: 0.8648, precision: 0.9394, recall: 0.5345\n",
      "2019-01-07T15:29:04.104206, step: 62, loss: 0.7428056597709656, acc: 0.6641, auc: 0.8488, precision: 0.8846, recall: 0.3651\n",
      "2019-01-07T15:29:04.305317, step: 63, loss: 0.5782392621040344, acc: 0.6875, auc: 0.8383, precision: 0.84, recall: 0.5676\n",
      "2019-01-07T15:29:04.522976, step: 64, loss: 1.1280783414840698, acc: 0.625, auc: 0.863, precision: 0.5676, recall: 1.0\n",
      "2019-01-07T15:29:04.737978, step: 65, loss: 0.6578325033187866, acc: 0.7344, auc: 0.8524, precision: 0.6667, recall: 0.9032\n",
      "2019-01-07T15:29:04.952272, step: 66, loss: 1.235602617263794, acc: 0.5391, auc: 0.8199, precision: 0.9, recall: 0.1343\n",
      "2019-01-07T15:29:05.166295, step: 67, loss: 0.7003491520881653, acc: 0.6953, auc: 0.8737, precision: 0.875, recall: 0.2745\n",
      "2019-01-07T15:29:05.379165, step: 68, loss: 0.6223076581954956, acc: 0.7188, auc: 0.8512, precision: 0.6914, recall: 0.8358\n",
      "2019-01-07T15:29:05.589220, step: 69, loss: 0.8647351264953613, acc: 0.7188, auc: 0.844, precision: 0.6322, recall: 0.9322\n",
      "2019-01-07T15:29:05.794491, step: 70, loss: 0.622185468673706, acc: 0.7031, auc: 0.791, precision: 0.6538, recall: 0.6296\n",
      "2019-01-07T15:29:05.999405, step: 71, loss: 0.8849072456359863, acc: 0.6094, auc: 0.8805, precision: 0.9286, recall: 0.2097\n",
      "2019-01-07T15:29:06.216034, step: 72, loss: 0.623558759689331, acc: 0.7422, auc: 0.8902, precision: 0.9062, recall: 0.4915\n",
      "2019-01-07T15:29:06.431355, step: 73, loss: 0.5494721531867981, acc: 0.7969, auc: 0.8957, precision: 0.7407, recall: 0.9231\n",
      "2019-01-07T15:29:06.633925, step: 74, loss: 0.7307599782943726, acc: 0.7422, auc: 0.8778, precision: 0.6947, recall: 0.9429\n",
      "2019-01-07T15:29:06.836582, step: 75, loss: 0.6350679397583008, acc: 0.7109, auc: 0.8322, precision: 0.6515, recall: 0.7544\n",
      "2019-01-07T15:29:07.059063, step: 76, loss: 0.6461254358291626, acc: 0.7266, auc: 0.8819, precision: 0.9231, recall: 0.4211\n",
      "2019-01-07T15:29:07.271911, step: 77, loss: 1.0970559120178223, acc: 0.6172, auc: 0.8697, precision: 1.0, recall: 0.3\n",
      "2019-01-07T15:29:07.481181, step: 78, loss: 0.5819679498672485, acc: 0.7578, auc: 0.8542, precision: 0.75, recall: 0.7377\n",
      "2019-01-07T15:29:07.701246, step: 79, loss: 0.8244525194168091, acc: 0.7422, auc: 0.8732, precision: 0.7, recall: 0.9589\n",
      "2019-01-07T15:29:07.906221, step: 80, loss: 0.8601512908935547, acc: 0.7656, auc: 0.8452, precision: 0.7097, recall: 0.9565\n",
      "2019-01-07T15:29:08.121079, step: 81, loss: 0.5491124987602234, acc: 0.7344, auc: 0.8588, precision: 0.807, recall: 0.6667\n",
      "2019-01-07T15:29:08.318679, step: 82, loss: 0.7104713916778564, acc: 0.7422, auc: 0.8918, precision: 0.9167, recall: 0.5238\n",
      "2019-01-07T15:29:08.530617, step: 83, loss: 0.8013025522232056, acc: 0.7188, auc: 0.8441, precision: 0.9565, recall: 0.386\n",
      "2019-01-07T15:29:08.733219, step: 84, loss: 0.5013512372970581, acc: 0.7812, auc: 0.8699, precision: 0.8478, recall: 0.65\n",
      "2019-01-07T15:29:08.948430, step: 85, loss: 0.5207386016845703, acc: 0.8359, auc: 0.9175, precision: 0.8025, recall: 0.9286\n",
      "2019-01-07T15:29:09.160627, step: 86, loss: 1.0257009267807007, acc: 0.7344, auc: 0.8828, precision: 0.6596, recall: 0.9688\n",
      "2019-01-07T15:29:09.366567, step: 87, loss: 0.4621158540248871, acc: 0.8516, auc: 0.8994, precision: 0.8667, recall: 0.8254\n",
      "2019-01-07T15:29:09.589923, step: 88, loss: 0.6469624042510986, acc: 0.7578, auc: 0.8926, precision: 0.8537, recall: 0.5833\n",
      "2019-01-07T15:29:09.800463, step: 89, loss: 0.7188760638237, acc: 0.7109, auc: 0.9123, precision: 0.9231, recall: 0.4068\n",
      "2019-01-07T15:29:10.014891, step: 90, loss: 0.7259173393249512, acc: 0.7344, auc: 0.832, precision: 0.8222, recall: 0.5873\n",
      "2019-01-07T15:29:10.229950, step: 91, loss: 0.6837639808654785, acc: 0.7734, auc: 0.8411, precision: 0.7831, recall: 0.8553\n",
      "2019-01-07T15:29:10.444762, step: 92, loss: 0.9370673894882202, acc: 0.7969, auc: 0.9328, precision: 0.6835, recall: 0.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:29:10.658423, step: 93, loss: 0.4956212043762207, acc: 0.8359, auc: 0.9229, precision: 0.7571, recall: 0.9298\n",
      "2019-01-07T15:29:10.861933, step: 94, loss: 0.5124728083610535, acc: 0.7578, auc: 0.9034, precision: 0.875, recall: 0.6269\n",
      "2019-01-07T15:29:11.062429, step: 95, loss: 0.6336507797241211, acc: 0.7422, auc: 0.9277, precision: 0.9697, recall: 0.5\n",
      "2019-01-07T15:29:11.266777, step: 96, loss: 0.7290962338447571, acc: 0.75, auc: 0.8962, precision: 0.9429, recall: 0.5238\n",
      "2019-01-07T15:29:11.479876, step: 97, loss: 0.528708279132843, acc: 0.7891, auc: 0.8879, precision: 0.8033, recall: 0.7656\n",
      "2019-01-07T15:29:11.681565, step: 98, loss: 0.716462254524231, acc: 0.8047, auc: 0.9116, precision: 0.7419, recall: 0.9857\n",
      "2019-01-07T15:29:11.895444, step: 99, loss: 0.6416012048721313, acc: 0.8203, auc: 0.911, precision: 0.7753, recall: 0.9583\n",
      "2019-01-07T15:29:12.097399, step: 100, loss: 0.424837201833725, acc: 0.8359, auc: 0.9182, precision: 0.8243, recall: 0.8841\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:29:20.183763, step: 100, loss: 0.5644402847840235, acc: 0.7786384615384614, auc: 0.8987153846153845, precision: 0.8777692307692309, recall: 0.65205641025641\n",
      "2019-01-07T15:29:20.400150, step: 101, loss: 0.5433135032653809, acc: 0.7969, auc: 0.9206, precision: 0.9762, recall: 0.6212\n",
      "2019-01-07T15:29:20.613386, step: 102, loss: 0.6816868782043457, acc: 0.7891, auc: 0.9041, precision: 0.9302, recall: 0.625\n",
      "2019-01-07T15:29:20.813534, step: 103, loss: 0.5258143544197083, acc: 0.7891, auc: 0.8895, precision: 0.8182, recall: 0.7258\n",
      "2019-01-07T15:29:21.016524, step: 104, loss: 0.4507443904876709, acc: 0.8125, auc: 0.9049, precision: 0.8, recall: 0.8\n",
      "2019-01-07T15:29:21.225036, step: 105, loss: 0.8926177024841309, acc: 0.7578, auc: 0.8015, precision: 0.75, recall: 0.7846\n",
      "2019-01-07T15:29:21.426532, step: 106, loss: 0.6681678295135498, acc: 0.7969, auc: 0.8855, precision: 0.7333, recall: 0.9016\n",
      "2019-01-07T15:29:21.628694, step: 107, loss: 0.4872312545776367, acc: 0.8047, auc: 0.9211, precision: 0.9231, recall: 0.6957\n",
      "2019-01-07T15:29:21.834645, step: 108, loss: 0.6666674613952637, acc: 0.7578, auc: 0.8646, precision: 0.825, recall: 0.5789\n",
      "2019-01-07T15:29:22.034219, step: 109, loss: 0.42455804347991943, acc: 0.7969, auc: 0.9124, precision: 0.8519, recall: 0.7188\n",
      "2019-01-07T15:29:22.232275, step: 110, loss: 0.5275877714157104, acc: 0.7969, auc: 0.8751, precision: 0.8103, recall: 0.7581\n",
      "2019-01-07T15:29:22.432887, step: 111, loss: 0.6152437329292297, acc: 0.7734, auc: 0.8498, precision: 0.7794, recall: 0.791\n",
      "2019-01-07T15:29:22.642888, step: 112, loss: 0.3040842115879059, acc: 0.8672, auc: 0.9462, precision: 0.8305, recall: 0.875\n",
      "2019-01-07T15:29:22.854637, step: 113, loss: 0.6101343631744385, acc: 0.7734, auc: 0.8656, precision: 0.7705, recall: 0.7581\n",
      "2019-01-07T15:29:23.057950, step: 114, loss: 0.4665830433368683, acc: 0.8281, auc: 0.9022, precision: 0.86, recall: 0.7414\n",
      "2019-01-07T15:29:23.257818, step: 115, loss: 0.46277663111686707, acc: 0.7969, auc: 0.8973, precision: 0.84, recall: 0.7\n",
      "2019-01-07T15:29:23.471799, step: 116, loss: 0.46771296858787537, acc: 0.8281, auc: 0.9068, precision: 0.8824, recall: 0.7377\n",
      "2019-01-07T15:29:23.691228, step: 117, loss: 0.3973546028137207, acc: 0.8594, auc: 0.9179, precision: 0.8852, recall: 0.8308\n",
      "2019-01-07T15:29:23.902072, step: 118, loss: 0.3668558895587921, acc: 0.8438, auc: 0.9296, precision: 0.8667, recall: 0.8667\n",
      "2019-01-07T15:29:24.110236, step: 119, loss: 0.37874218821525574, acc: 0.8438, auc: 0.9308, precision: 0.8667, recall: 0.8667\n",
      "2019-01-07T15:29:24.313548, step: 120, loss: 0.33759206533432007, acc: 0.8359, auc: 0.9344, precision: 0.8276, recall: 0.8136\n",
      "2019-01-07T15:29:24.524401, step: 121, loss: 0.4799637198448181, acc: 0.7734, auc: 0.8804, precision: 0.7692, recall: 0.7018\n",
      "2019-01-07T15:29:24.732653, step: 122, loss: 0.4722808003425598, acc: 0.7969, auc: 0.9236, precision: 0.8793, recall: 0.7286\n",
      "2019-01-07T15:29:24.935427, step: 123, loss: 0.3885713815689087, acc: 0.8359, auc: 0.9184, precision: 0.8361, recall: 0.8226\n",
      "2019-01-07T15:29:25.142426, step: 124, loss: 0.4031916558742523, acc: 0.8359, auc: 0.9128, precision: 0.8909, recall: 0.7656\n",
      "2019-01-07T15:29:25.357767, step: 125, loss: 0.39846399426460266, acc: 0.8438, auc: 0.9222, precision: 0.8493, recall: 0.8732\n",
      "2019-01-07T15:29:25.572759, step: 126, loss: 0.45524680614471436, acc: 0.7812, auc: 0.9039, precision: 0.8448, recall: 0.7206\n",
      "2019-01-07T15:29:25.776027, step: 127, loss: 0.44020530581474304, acc: 0.8203, auc: 0.9089, precision: 0.8806, recall: 0.7973\n",
      "2019-01-07T15:29:25.979503, step: 128, loss: 0.3737315535545349, acc: 0.8516, auc: 0.9296, precision: 0.8983, recall: 0.803\n",
      "2019-01-07T15:29:26.188646, step: 129, loss: 0.5347563028335571, acc: 0.7969, auc: 0.918, precision: 0.7324, recall: 0.8814\n",
      "2019-01-07T15:29:26.388187, step: 130, loss: 0.43980634212493896, acc: 0.7812, auc: 0.899, precision: 0.8387, recall: 0.7429\n",
      "2019-01-07T15:29:26.605613, step: 131, loss: 0.3017967939376831, acc: 0.8594, auc: 0.9467, precision: 0.9016, recall: 0.8209\n",
      "2019-01-07T15:29:26.813530, step: 132, loss: 0.4963262379169464, acc: 0.7812, auc: 0.9017, precision: 0.92, recall: 0.6571\n",
      "2019-01-07T15:29:27.019092, step: 133, loss: 0.5496179461479187, acc: 0.7812, auc: 0.8647, precision: 0.7541, recall: 0.7797\n",
      "2019-01-07T15:29:27.221228, step: 134, loss: 0.30466732382774353, acc: 0.8516, auc: 0.9461, precision: 0.8889, recall: 0.8235\n",
      "2019-01-07T15:29:27.430528, step: 135, loss: 0.48725831508636475, acc: 0.8359, auc: 0.9021, precision: 0.8378, recall: 0.8732\n",
      "2019-01-07T15:29:27.634827, step: 136, loss: 0.35735809803009033, acc: 0.8672, auc: 0.9407, precision: 0.8529, recall: 0.8923\n",
      "2019-01-07T15:29:27.833759, step: 137, loss: 0.47109368443489075, acc: 0.8047, auc: 0.8958, precision: 0.7818, recall: 0.7679\n",
      "2019-01-07T15:29:28.044773, step: 138, loss: 0.4403882324695587, acc: 0.8359, auc: 0.921, precision: 0.9474, recall: 0.6545\n",
      "2019-01-07T15:29:28.255236, step: 139, loss: 0.377116858959198, acc: 0.8203, auc: 0.9432, precision: 0.8929, recall: 0.7463\n",
      "2019-01-07T15:29:28.470432, step: 140, loss: 0.6442704796791077, acc: 0.7578, auc: 0.8601, precision: 0.8036, recall: 0.6923\n",
      "2019-01-07T15:29:28.678884, step: 141, loss: 0.4867449998855591, acc: 0.8281, auc: 0.9308, precision: 0.7647, recall: 0.8966\n",
      "2019-01-07T15:29:28.896085, step: 142, loss: 0.4438745975494385, acc: 0.8516, auc: 0.9326, precision: 0.8267, recall: 0.9118\n",
      "2019-01-07T15:29:29.111970, step: 143, loss: 0.3632987141609192, acc: 0.8438, auc: 0.9351, precision: 0.9231, recall: 0.8\n",
      "2019-01-07T15:29:29.315078, step: 144, loss: 0.49462074041366577, acc: 0.7969, auc: 0.8965, precision: 0.8776, recall: 0.6825\n",
      "2019-01-07T15:29:29.517240, step: 145, loss: 0.43378326296806335, acc: 0.7812, auc: 0.9098, precision: 0.8036, recall: 0.7258\n",
      "2019-01-07T15:29:29.715834, step: 146, loss: 0.49512121081352234, acc: 0.8359, auc: 0.9065, precision: 0.7945, recall: 0.9062\n",
      "2019-01-07T15:29:29.925867, step: 147, loss: 0.43337661027908325, acc: 0.8281, auc: 0.9101, precision: 0.8, recall: 0.8276\n",
      "2019-01-07T15:29:30.125537, step: 148, loss: 0.44929251074790955, acc: 0.8047, auc: 0.9211, precision: 0.8519, recall: 0.7302\n",
      "2019-01-07T15:29:30.336399, step: 149, loss: 0.4819997549057007, acc: 0.8281, auc: 0.9059, precision: 0.8364, recall: 0.7797\n",
      "2019-01-07T15:29:30.536624, step: 150, loss: 0.6110439300537109, acc: 0.7656, auc: 0.8786, precision: 0.9, recall: 0.6429\n",
      "2019-01-07T15:29:30.746225, step: 151, loss: 0.4766521453857422, acc: 0.8359, auc: 0.9135, precision: 0.8219, recall: 0.8824\n",
      "2019-01-07T15:29:30.947508, step: 152, loss: 0.6850548982620239, acc: 0.7734, auc: 0.8998, precision: 0.6753, recall: 0.9286\n",
      "2019-01-07T15:29:31.159335, step: 153, loss: 0.41738390922546387, acc: 0.7891, auc: 0.9283, precision: 0.8462, recall: 0.7639\n",
      "2019-01-07T15:29:31.371083, step: 154, loss: 0.5983286499977112, acc: 0.7656, auc: 0.8877, precision: 0.8824, recall: 0.6522\n",
      "2019-01-07T15:29:31.570404, step: 155, loss: 0.6393560171127319, acc: 0.7734, auc: 0.9137, precision: 0.9111, recall: 0.6212\n",
      "2019-01-07T15:29:31.781245, step: 156, loss: 0.49957072734832764, acc: 0.8203, auc: 0.8983, precision: 0.8095, recall: 0.8226\n",
      "start training model\n",
      "2019-01-07T15:29:32.027583, step: 157, loss: 0.4299641251564026, acc: 0.8516, auc: 0.9666, precision: 0.8, recall: 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:29:32.231983, step: 158, loss: 0.27248311042785645, acc: 0.9062, auc: 0.9682, precision: 0.875, recall: 0.9545\n",
      "2019-01-07T15:29:32.438146, step: 159, loss: 0.37943917512893677, acc: 0.8672, auc: 0.9543, precision: 0.9423, recall: 0.7778\n",
      "2019-01-07T15:29:32.644117, step: 160, loss: 0.35967573523521423, acc: 0.75, auc: 0.9887, precision: 1.0, recall: 0.5294\n",
      "2019-01-07T15:29:32.859794, step: 161, loss: 0.1757589876651764, acc: 0.9297, auc: 0.9801, precision: 0.9474, recall: 0.9\n",
      "2019-01-07T15:29:33.057696, step: 162, loss: 0.24997590482234955, acc: 0.9297, auc: 0.9621, precision: 0.9315, recall: 0.9444\n",
      "2019-01-07T15:29:33.270960, step: 163, loss: 0.34177762269973755, acc: 0.9062, auc: 0.9811, precision: 0.8519, recall: 1.0\n",
      "2019-01-07T15:29:33.485518, step: 164, loss: 0.1952819526195526, acc: 0.9375, auc: 0.9765, precision: 0.9277, recall: 0.9747\n",
      "2019-01-07T15:29:33.696434, step: 165, loss: 0.23345084488391876, acc: 0.9141, auc: 0.9694, precision: 0.9079, recall: 0.9452\n",
      "2019-01-07T15:29:33.895311, step: 166, loss: 0.2649279236793518, acc: 0.9141, auc: 0.9694, precision: 0.9672, recall: 0.8676\n",
      "2019-01-07T15:29:34.101438, step: 167, loss: 0.2925298810005188, acc: 0.8359, auc: 0.9592, precision: 0.9038, recall: 0.746\n",
      "2019-01-07T15:29:34.324727, step: 168, loss: 0.3212897479534149, acc: 0.8984, auc: 0.9512, precision: 0.9365, recall: 0.8676\n",
      "2019-01-07T15:29:34.544431, step: 169, loss: 0.2413003146648407, acc: 0.8984, auc: 0.9679, precision: 0.9655, recall: 0.8358\n",
      "2019-01-07T15:29:34.759787, step: 170, loss: 0.21892735362052917, acc: 0.9297, auc: 0.9809, precision: 0.9091, recall: 0.9722\n",
      "2019-01-07T15:29:34.963061, step: 171, loss: 0.10407409816980362, acc: 0.9688, auc: 0.9971, precision: 0.9661, recall: 0.9661\n",
      "2019-01-07T15:29:35.171136, step: 172, loss: 0.1675061285495758, acc: 0.9297, auc: 0.9851, precision: 0.9375, recall: 0.9231\n",
      "2019-01-07T15:29:35.378634, step: 173, loss: 0.24337448179721832, acc: 0.9062, auc: 0.9659, precision: 0.9385, recall: 0.8841\n",
      "2019-01-07T15:29:35.592296, step: 174, loss: 0.1601746380329132, acc: 0.9375, auc: 0.9896, precision: 1.0, recall: 0.8961\n",
      "2019-01-07T15:29:35.790501, step: 175, loss: 0.13801446557044983, acc: 0.9141, auc: 0.9894, precision: 0.9615, recall: 0.8475\n",
      "2019-01-07T15:29:35.999666, step: 176, loss: 0.2695824205875397, acc: 0.875, auc: 0.9654, precision: 0.8909, recall: 0.8305\n",
      "2019-01-07T15:29:36.218870, step: 177, loss: 0.17252007126808167, acc: 0.9531, auc: 0.9816, precision: 0.9714, recall: 0.9444\n",
      "2019-01-07T15:29:36.420226, step: 178, loss: 0.24528290331363678, acc: 0.8984, auc: 0.972, precision: 0.9107, recall: 0.8644\n",
      "2019-01-07T15:29:36.627640, step: 179, loss: 0.2606368958950043, acc: 0.9453, auc: 0.9665, precision: 0.95, recall: 0.9344\n",
      "2019-01-07T15:29:36.843898, step: 180, loss: 0.196353942155838, acc: 0.9062, auc: 0.9744, precision: 0.918, recall: 0.8889\n",
      "2019-01-07T15:29:37.044449, step: 181, loss: 0.32614314556121826, acc: 0.8672, auc: 0.9635, precision: 0.9767, recall: 0.7241\n",
      "2019-01-07T15:29:37.261255, step: 182, loss: 0.20380601286888123, acc: 0.9141, auc: 0.9793, precision: 0.9123, recall: 0.8966\n",
      "2019-01-07T15:29:37.479009, step: 183, loss: 0.13223198056221008, acc: 0.9609, auc: 0.9885, precision: 0.9559, recall: 0.9701\n",
      "2019-01-07T15:29:37.683702, step: 184, loss: 0.3054869771003723, acc: 0.9297, auc: 0.9814, precision: 0.85, recall: 1.0\n",
      "2019-01-07T15:29:37.900599, step: 185, loss: 0.23479579389095306, acc: 0.9062, auc: 0.9713, precision: 0.9403, recall: 0.8873\n",
      "2019-01-07T15:29:38.110468, step: 186, loss: 0.3157411515712738, acc: 0.8672, auc: 0.9568, precision: 0.9545, recall: 0.7368\n",
      "2019-01-07T15:29:38.324801, step: 187, loss: 0.25418683886528015, acc: 0.8594, auc: 0.9794, precision: 0.9636, recall: 0.7681\n",
      "2019-01-07T15:29:38.549442, step: 188, loss: 0.20967014133930206, acc: 0.9141, auc: 0.9739, precision: 0.9492, recall: 0.875\n",
      "2019-01-07T15:29:38.764273, step: 189, loss: 0.2390240579843521, acc: 0.9297, auc: 0.977, precision: 0.9041, recall: 0.9706\n",
      "2019-01-07T15:29:38.977252, step: 190, loss: 0.34083542227745056, acc: 0.8594, auc: 0.9688, precision: 0.8228, recall: 0.942\n",
      "2019-01-07T15:29:39.182028, step: 191, loss: 0.18480728566646576, acc: 0.9375, auc: 0.9762, precision: 0.9483, recall: 0.9167\n",
      "2019-01-07T15:29:39.384663, step: 192, loss: 0.18032057583332062, acc: 0.9141, auc: 0.9861, precision: 0.9808, recall: 0.8361\n",
      "2019-01-07T15:29:39.601813, step: 193, loss: 0.27715957164764404, acc: 0.8672, auc: 0.9841, precision: 1.0, recall: 0.7385\n",
      "2019-01-07T15:29:39.816151, step: 194, loss: 0.3009376525878906, acc: 0.8516, auc: 0.9512, precision: 0.918, recall: 0.8\n",
      "2019-01-07T15:29:40.021214, step: 195, loss: 0.3000991642475128, acc: 0.8906, auc: 0.9569, precision: 0.8507, recall: 0.9344\n",
      "2019-01-07T15:29:40.234488, step: 196, loss: 0.27119189500808716, acc: 0.9219, auc: 0.9873, precision: 0.8714, recall: 0.9839\n",
      "2019-01-07T15:29:40.449068, step: 197, loss: 0.1737511157989502, acc: 0.9375, auc: 0.9852, precision: 0.9259, recall: 0.9259\n",
      "2019-01-07T15:29:40.665848, step: 198, loss: 0.19710460305213928, acc: 0.9062, auc: 0.9851, precision: 0.9804, recall: 0.8197\n",
      "2019-01-07T15:29:40.878685, step: 199, loss: 0.3630775809288025, acc: 0.8281, auc: 0.9675, precision: 0.9778, recall: 0.6769\n",
      "2019-01-07T15:29:41.090277, step: 200, loss: 0.20257121324539185, acc: 0.9141, auc: 0.9774, precision: 0.9552, recall: 0.8889\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:29:49.137030, step: 200, loss: 0.43156744883610654, acc: 0.8437564102564102, auc: 0.9250666666666668, precision: 0.8165846153846154, recall: 0.8923641025641024\n",
      "2019-01-07T15:29:49.334680, step: 201, loss: 0.2944427728652954, acc: 0.9062, auc: 0.9742, precision: 0.8615, recall: 0.9492\n",
      "2019-01-07T15:29:49.539090, step: 202, loss: 0.15835526585578918, acc: 0.9375, auc: 0.9953, precision: 0.9067, recall: 0.9855\n",
      "2019-01-07T15:29:49.756544, step: 203, loss: 0.16924411058425903, acc: 0.9375, auc: 0.9808, precision: 0.9844, recall: 0.9\n",
      "2019-01-07T15:29:49.960003, step: 204, loss: 0.20989598333835602, acc: 0.9141, auc: 0.9739, precision: 0.9322, recall: 0.8871\n",
      "2019-01-07T15:29:50.159978, step: 205, loss: 0.17103083431720734, acc: 0.8984, auc: 0.9831, precision: 0.9512, recall: 0.78\n",
      "2019-01-07T15:29:50.363043, step: 206, loss: 0.2355944812297821, acc: 0.9062, auc: 0.9709, precision: 0.9322, recall: 0.873\n",
      "2019-01-07T15:29:50.573458, step: 207, loss: 0.15243840217590332, acc: 0.9375, auc: 0.9861, precision: 0.95, recall: 0.9194\n",
      "2019-01-07T15:29:50.783154, step: 208, loss: 0.24266517162322998, acc: 0.9062, auc: 0.9731, precision: 0.8971, recall: 0.9242\n",
      "2019-01-07T15:29:50.982501, step: 209, loss: 0.1558208167552948, acc: 0.9453, auc: 0.9878, precision: 0.9265, recall: 0.9692\n",
      "2019-01-07T15:29:51.206929, step: 210, loss: 0.15709668397903442, acc: 0.9297, auc: 0.9867, precision: 0.931, recall: 0.9153\n",
      "2019-01-07T15:29:51.421365, step: 211, loss: 0.25153648853302, acc: 0.8828, auc: 0.9752, precision: 1.0, recall: 0.75\n",
      "2019-01-07T15:29:51.622724, step: 212, loss: 0.23825684189796448, acc: 0.8984, auc: 0.9743, precision: 0.9615, recall: 0.8197\n",
      "2019-01-07T15:29:51.838591, step: 213, loss: 0.22610998153686523, acc: 0.9375, auc: 0.9682, precision: 0.9677, recall: 0.9091\n",
      "2019-01-07T15:29:52.052774, step: 214, loss: 0.2021910846233368, acc: 0.9375, auc: 0.9888, precision: 0.8889, recall: 1.0\n",
      "2019-01-07T15:29:52.266108, step: 215, loss: 0.2691643238067627, acc: 0.9297, auc: 0.9707, precision: 0.9206, recall: 0.9355\n",
      "2019-01-07T15:29:52.479041, step: 216, loss: 0.11564282327890396, acc: 0.9609, auc: 0.9939, precision: 1.0, recall: 0.9231\n",
      "2019-01-07T15:29:52.683116, step: 217, loss: 0.2154594510793686, acc: 0.9062, auc: 0.9768, precision: 0.9808, recall: 0.8226\n",
      "2019-01-07T15:29:52.890491, step: 218, loss: 0.1998279094696045, acc: 0.8906, auc: 0.9812, precision: 0.9483, recall: 0.8333\n",
      "2019-01-07T15:29:53.106018, step: 219, loss: 0.15882553160190582, acc: 0.9297, auc: 0.9861, precision: 0.9677, recall: 0.8955\n",
      "2019-01-07T15:29:53.306367, step: 220, loss: 0.2562747001647949, acc: 0.8984, auc: 0.9732, precision: 0.8594, recall: 0.9322\n",
      "2019-01-07T15:29:53.519262, step: 221, loss: 0.25291943550109863, acc: 0.9297, auc: 0.9661, precision: 0.9057, recall: 0.9231\n",
      "2019-01-07T15:29:53.731310, step: 222, loss: 0.1649717092514038, acc: 0.9062, auc: 0.9848, precision: 0.9524, recall: 0.8696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:29:53.950559, step: 223, loss: 0.15075638890266418, acc: 0.9297, auc: 0.9887, precision: 0.9636, recall: 0.8833\n",
      "2019-01-07T15:29:54.167966, step: 224, loss: 0.18247927725315094, acc: 0.9062, auc: 0.9836, precision: 0.963, recall: 0.8387\n",
      "2019-01-07T15:29:54.371264, step: 225, loss: 0.17290566861629486, acc: 0.9375, auc: 0.9829, precision: 1.0, recall: 0.8788\n",
      "2019-01-07T15:29:54.585385, step: 226, loss: 0.27522599697113037, acc: 0.9141, auc: 0.9685, precision: 0.8857, recall: 0.9538\n",
      "2019-01-07T15:29:54.793792, step: 227, loss: 0.1898288130760193, acc: 0.9219, auc: 0.985, precision: 0.9153, recall: 0.9153\n",
      "2019-01-07T15:29:55.005103, step: 228, loss: 0.1659417450428009, acc: 0.9297, auc: 0.9841, precision: 0.9322, recall: 0.9167\n",
      "2019-01-07T15:29:55.215854, step: 229, loss: 0.2313946783542633, acc: 0.8906, auc: 0.9713, precision: 0.9091, recall: 0.8824\n",
      "2019-01-07T15:29:55.431195, step: 230, loss: 0.3610122799873352, acc: 0.8906, auc: 0.9462, precision: 0.9592, recall: 0.7966\n",
      "2019-01-07T15:29:55.651845, step: 231, loss: 0.29567018151283264, acc: 0.8906, auc: 0.9635, precision: 0.9792, recall: 0.7833\n",
      "2019-01-07T15:29:55.862425, step: 232, loss: 0.25018221139907837, acc: 0.9141, auc: 0.9675, precision: 0.9167, recall: 0.9016\n",
      "2019-01-07T15:29:56.063506, step: 233, loss: 0.2068362534046173, acc: 0.9141, auc: 0.989, precision: 0.8676, recall: 0.9672\n",
      "2019-01-07T15:29:56.278970, step: 234, loss: 0.17254918813705444, acc: 0.9453, auc: 0.9775, precision: 0.9444, recall: 0.9577\n",
      "2019-01-07T15:29:56.492287, step: 235, loss: 0.17235752940177917, acc: 0.9297, auc: 0.9823, precision: 0.8909, recall: 0.9423\n",
      "2019-01-07T15:29:56.703802, step: 236, loss: 0.24114862084388733, acc: 0.8828, auc: 0.9817, precision: 0.9796, recall: 0.7742\n",
      "2019-01-07T15:29:56.907171, step: 237, loss: 0.35006940364837646, acc: 0.8203, auc: 0.9769, precision: 1.0, recall: 0.7051\n",
      "2019-01-07T15:29:57.127510, step: 238, loss: 0.2541041076183319, acc: 0.9297, auc: 0.98, precision: 0.8889, recall: 0.9655\n",
      "2019-01-07T15:29:57.330179, step: 239, loss: 0.20222386717796326, acc: 0.9219, auc: 0.9848, precision: 0.9041, recall: 0.9565\n",
      "2019-01-07T15:29:57.550820, step: 240, loss: 0.26141834259033203, acc: 0.9062, auc: 0.9814, precision: 0.8636, recall: 0.95\n",
      "2019-01-07T15:29:57.757974, step: 241, loss: 0.2768419682979584, acc: 0.8359, auc: 0.958, precision: 0.8478, recall: 0.7358\n",
      "2019-01-07T15:29:57.960640, step: 242, loss: 0.5063297748565674, acc: 0.8516, auc: 0.9222, precision: 0.9375, recall: 0.7377\n",
      "2019-01-07T15:29:58.172884, step: 243, loss: 0.2901400327682495, acc: 0.8828, auc: 0.9689, precision: 0.9818, recall: 0.7941\n",
      "2019-01-07T15:29:58.381007, step: 244, loss: 0.16978156566619873, acc: 0.9453, auc: 0.9812, precision: 0.9683, recall: 0.9242\n",
      "2019-01-07T15:29:58.597261, step: 245, loss: 0.20237067341804504, acc: 0.9297, auc: 0.9765, precision: 0.9344, recall: 0.9194\n",
      "2019-01-07T15:29:58.810355, step: 246, loss: 0.27421319484710693, acc: 0.9062, auc: 0.9818, precision: 0.8276, recall: 0.96\n",
      "2019-01-07T15:29:59.019009, step: 247, loss: 0.2630116641521454, acc: 0.875, auc: 0.9624, precision: 0.8906, recall: 0.8636\n",
      "2019-01-07T15:29:59.236848, step: 248, loss: 0.15521003305912018, acc: 0.9297, auc: 0.9856, precision: 0.9846, recall: 0.8889\n",
      "2019-01-07T15:29:59.453212, step: 249, loss: 0.33304363489151, acc: 0.8438, auc: 0.9663, precision: 0.9756, recall: 0.678\n",
      "2019-01-07T15:29:59.666014, step: 250, loss: 0.23822544515132904, acc: 0.9141, auc: 0.9698, precision: 0.9, recall: 0.9153\n",
      "2019-01-07T15:29:59.880735, step: 251, loss: 0.2045610547065735, acc: 0.9375, auc: 0.9756, precision: 0.9394, recall: 0.9394\n",
      "2019-01-07T15:30:00.086764, step: 252, loss: 0.2413523942232132, acc: 0.8984, auc: 0.9704, precision: 0.9014, recall: 0.9143\n",
      "2019-01-07T15:30:00.291390, step: 253, loss: 0.33311617374420166, acc: 0.8828, auc: 0.9452, precision: 0.8939, recall: 0.8806\n",
      "2019-01-07T15:30:00.496772, step: 254, loss: 0.17495983839035034, acc: 0.9219, auc: 0.9844, precision: 0.9649, recall: 0.873\n",
      "2019-01-07T15:30:00.712490, step: 255, loss: 0.18299707770347595, acc: 0.9219, auc: 0.9816, precision: 0.9474, recall: 0.8852\n",
      "2019-01-07T15:30:00.933533, step: 256, loss: 0.2466869056224823, acc: 0.875, auc: 0.9751, precision: 0.9804, recall: 0.7692\n",
      "2019-01-07T15:30:01.142745, step: 257, loss: 0.23566868901252747, acc: 0.9141, auc: 0.9678, precision: 0.9, recall: 0.9153\n",
      "2019-01-07T15:30:01.342257, step: 258, loss: 0.2715524435043335, acc: 0.9219, auc: 0.9805, precision: 0.875, recall: 0.9844\n",
      "2019-01-07T15:30:01.552743, step: 259, loss: 0.26639223098754883, acc: 0.8984, auc: 0.9545, precision: 0.9437, recall: 0.8816\n",
      "2019-01-07T15:30:01.757870, step: 260, loss: 0.17905016243457794, acc: 0.9219, auc: 0.9819, precision: 0.9655, recall: 0.875\n",
      "2019-01-07T15:30:01.974491, step: 261, loss: 0.2576964795589447, acc: 0.8906, auc: 0.9722, precision: 0.9623, recall: 0.8095\n",
      "2019-01-07T15:30:02.185617, step: 262, loss: 0.19157284498214722, acc: 0.9219, auc: 0.9834, precision: 1.0, recall: 0.8438\n",
      "2019-01-07T15:30:02.403091, step: 263, loss: 0.16048139333724976, acc: 0.9375, auc: 0.9901, precision: 0.9079, recall: 0.9857\n",
      "2019-01-07T15:30:02.614630, step: 264, loss: 0.22131212055683136, acc: 0.9297, auc: 0.9753, precision: 0.913, recall: 0.9545\n",
      "2019-01-07T15:30:02.827514, step: 265, loss: 0.23417727649211884, acc: 0.9062, auc: 0.9756, precision: 0.8393, recall: 0.94\n",
      "2019-01-07T15:30:03.041396, step: 266, loss: 0.1510753631591797, acc: 0.9219, auc: 0.9902, precision: 0.9808, recall: 0.85\n",
      "2019-01-07T15:30:03.257207, step: 267, loss: 0.24184665083885193, acc: 0.8984, auc: 0.9775, precision: 0.9623, recall: 0.8226\n",
      "2019-01-07T15:30:03.472129, step: 268, loss: 0.21205365657806396, acc: 0.9062, auc: 0.9765, precision: 0.963, recall: 0.8387\n",
      "2019-01-07T15:30:03.687808, step: 269, loss: 0.16625097393989563, acc: 0.9219, auc: 0.9861, precision: 0.9344, recall: 0.9048\n",
      "2019-01-07T15:30:03.912930, step: 270, loss: 0.3258283734321594, acc: 0.8984, auc: 0.9642, precision: 0.8615, recall: 0.9333\n",
      "2019-01-07T15:30:04.130142, step: 271, loss: 0.1624995470046997, acc: 0.9141, auc: 0.9875, precision: 0.9077, recall: 0.9219\n",
      "2019-01-07T15:30:04.348711, step: 272, loss: 0.21261441707611084, acc: 0.9062, auc: 0.9696, precision: 0.9375, recall: 0.8824\n",
      "2019-01-07T15:30:04.552097, step: 273, loss: 0.21983760595321655, acc: 0.8984, auc: 0.9836, precision: 0.9804, recall: 0.8065\n",
      "2019-01-07T15:30:04.753773, step: 274, loss: 0.19404594600200653, acc: 0.9062, auc: 0.9758, precision: 0.9091, recall: 0.8772\n",
      "2019-01-07T15:30:04.956151, step: 275, loss: 0.20694340765476227, acc: 0.9297, auc: 0.9753, precision: 0.9355, recall: 0.9206\n",
      "2019-01-07T15:30:05.169810, step: 276, loss: 0.18874792754650116, acc: 0.9219, auc: 0.9825, precision: 0.9692, recall: 0.8873\n",
      "2019-01-07T15:30:05.385273, step: 277, loss: 0.20276686549186707, acc: 0.9219, auc: 0.9775, precision: 0.9206, recall: 0.9206\n",
      "2019-01-07T15:30:05.597687, step: 278, loss: 0.19779102504253387, acc: 0.9375, auc: 0.9817, precision: 0.9118, recall: 0.9688\n",
      "2019-01-07T15:30:05.815912, step: 279, loss: 0.1768646389245987, acc: 0.9375, auc: 0.9863, precision: 0.9275, recall: 0.9552\n",
      "2019-01-07T15:30:06.026879, step: 280, loss: 0.17246150970458984, acc: 0.9062, auc: 0.9829, precision: 0.9492, recall: 0.8615\n",
      "2019-01-07T15:30:06.242153, step: 281, loss: 0.2702895700931549, acc: 0.875, auc: 0.9702, precision: 0.9655, recall: 0.8\n",
      "2019-01-07T15:30:06.465466, step: 282, loss: 0.1759204864501953, acc: 0.8984, auc: 0.9801, precision: 0.9545, recall: 0.863\n",
      "2019-01-07T15:30:06.678461, step: 283, loss: 0.15177565813064575, acc: 0.9453, auc: 0.9905, precision: 0.9375, recall: 0.9524\n",
      "2019-01-07T15:30:06.898388, step: 284, loss: 0.1966818869113922, acc: 0.9141, auc: 0.9884, precision: 0.8393, recall: 0.9592\n",
      "2019-01-07T15:30:07.111451, step: 285, loss: 0.3023139536380768, acc: 0.8516, auc: 0.9607, precision: 0.9508, recall: 0.7838\n",
      "2019-01-07T15:30:07.317023, step: 286, loss: 0.27805233001708984, acc: 0.8828, auc: 0.9602, precision: 0.9516, recall: 0.831\n",
      "2019-01-07T15:30:07.526665, step: 287, loss: 0.18548136949539185, acc: 0.8984, auc: 0.9799, precision: 0.9242, recall: 0.8841\n",
      "2019-01-07T15:30:07.748553, step: 288, loss: 0.13646164536476135, acc: 0.9297, auc: 0.9904, precision: 0.918, recall: 0.9333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:30:07.957187, step: 289, loss: 0.23915952444076538, acc: 0.9062, auc: 0.97, precision: 0.8929, recall: 0.8929\n",
      "2019-01-07T15:30:08.174094, step: 290, loss: 0.2663210332393646, acc: 0.9219, auc: 0.9595, precision: 0.9231, recall: 0.8889\n",
      "2019-01-07T15:30:08.383343, step: 291, loss: 0.1638394296169281, acc: 0.9297, auc: 0.9834, precision: 0.9672, recall: 0.8939\n",
      "2019-01-07T15:30:08.590131, step: 292, loss: 0.2521619498729706, acc: 0.8828, auc: 0.9792, precision: 0.9811, recall: 0.7879\n",
      "2019-01-07T15:30:08.811257, step: 293, loss: 0.2941185235977173, acc: 0.8828, auc: 0.9576, precision: 0.9231, recall: 0.8571\n",
      "2019-01-07T15:30:09.027377, step: 294, loss: 0.1128646656870842, acc: 0.9688, auc: 0.9947, precision: 0.9608, recall: 0.9608\n",
      "2019-01-07T15:30:09.234073, step: 295, loss: 0.36064738035202026, acc: 0.8828, auc: 0.9529, precision: 0.8625, recall: 0.9452\n",
      "2019-01-07T15:30:09.438391, step: 296, loss: 0.254219651222229, acc: 0.9141, auc: 0.9699, precision: 0.8873, recall: 0.9545\n",
      "2019-01-07T15:30:09.662316, step: 297, loss: 0.1415364146232605, acc: 0.9375, auc: 0.9883, precision: 0.9649, recall: 0.9016\n",
      "2019-01-07T15:30:09.869300, step: 298, loss: 0.28683990240097046, acc: 0.875, auc: 0.9836, precision: 1.0, recall: 0.7612\n",
      "2019-01-07T15:30:10.079568, step: 299, loss: 0.16635508835315704, acc: 0.9062, auc: 0.9855, precision: 0.9615, recall: 0.8333\n",
      "2019-01-07T15:30:10.289138, step: 300, loss: 0.17990413308143616, acc: 0.8984, auc: 0.9846, precision: 0.9344, recall: 0.8636\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:30:18.308491, step: 300, loss: 0.5257421938272623, acc: 0.8363358974358974, auc: 0.9269128205128205, precision: 0.7872461538461538, recall: 0.9274846153846156\n",
      "2019-01-07T15:30:18.515607, step: 301, loss: 0.36070364713668823, acc: 0.8438, auc: 0.9725, precision: 0.7576, recall: 0.9259\n",
      "2019-01-07T15:30:18.709356, step: 302, loss: 0.23364132642745972, acc: 0.9141, auc: 0.9779, precision: 0.8904, recall: 0.9559\n",
      "2019-01-07T15:30:18.903042, step: 303, loss: 0.2074708640575409, acc: 0.9297, auc: 0.973, precision: 0.9286, recall: 0.942\n",
      "2019-01-07T15:30:19.129723, step: 304, loss: 0.25265079736709595, acc: 0.8438, auc: 0.9869, precision: 1.0, recall: 0.6552\n",
      "2019-01-07T15:30:19.344677, step: 305, loss: 0.2527792155742645, acc: 0.8906, auc: 0.9689, precision: 0.9649, recall: 0.8209\n",
      "2019-01-07T15:30:19.561489, step: 306, loss: 0.12404192239046097, acc: 0.9609, auc: 0.9912, precision: 0.9692, recall: 0.9545\n",
      "2019-01-07T15:30:19.776877, step: 307, loss: 0.2522609531879425, acc: 0.9141, auc: 0.9784, precision: 0.8571, recall: 0.9412\n",
      "2019-01-07T15:30:19.992544, step: 308, loss: 0.1430581659078598, acc: 0.9531, auc: 0.9956, precision: 0.9091, recall: 1.0\n",
      "2019-01-07T15:30:20.208542, step: 309, loss: 0.25648677349090576, acc: 0.8672, auc: 0.9734, precision: 1.0, recall: 0.7571\n",
      "2019-01-07T15:30:20.424523, step: 310, loss: 0.3113040328025818, acc: 0.9062, auc: 0.9545, precision: 0.9655, recall: 0.8485\n",
      "2019-01-07T15:30:20.635971, step: 311, loss: 0.17365075647830963, acc: 0.9219, auc: 0.9808, precision: 0.9286, recall: 0.8966\n",
      "2019-01-07T15:30:20.839758, step: 312, loss: 0.14174649119377136, acc: 0.9453, auc: 0.988, precision: 0.9718, recall: 0.9324\n",
      "start training model\n",
      "2019-01-07T15:30:21.066503, step: 313, loss: 0.060000043362379074, acc: 0.9766, auc: 0.9995, precision: 0.9692, recall: 0.9844\n",
      "2019-01-07T15:30:21.278379, step: 314, loss: 0.10941237211227417, acc: 0.9609, auc: 0.9985, precision: 0.9286, recall: 1.0\n",
      "2019-01-07T15:30:21.480747, step: 315, loss: 0.047924723476171494, acc: 0.9844, auc: 0.9997, precision: 0.9815, recall: 0.9815\n",
      "2019-01-07T15:30:21.697452, step: 316, loss: 0.056230731308460236, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9452\n",
      "2019-01-07T15:30:21.915624, step: 317, loss: 0.09383723884820938, acc: 0.9844, auc: 0.988, precision: 0.9831, recall: 0.9831\n",
      "2019-01-07T15:30:22.120425, step: 318, loss: 0.07833573967218399, acc: 0.9531, auc: 1.0, precision: 1.0, recall: 0.9048\n",
      "2019-01-07T15:30:22.350256, step: 319, loss: 0.08811835944652557, acc: 0.9375, auc: 0.9976, precision: 0.9821, recall: 0.8871\n",
      "2019-01-07T15:30:22.563139, step: 320, loss: 0.05364038050174713, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9552\n",
      "2019-01-07T15:30:22.778455, step: 321, loss: 0.09943802654743195, acc: 0.9766, auc: 0.9973, precision: 0.95, recall: 1.0\n",
      "2019-01-07T15:30:22.995283, step: 322, loss: 0.1272657811641693, acc: 0.9531, auc: 0.9972, precision: 0.9138, recall: 0.9815\n",
      "2019-01-07T15:30:23.199003, step: 323, loss: 0.06239568442106247, acc: 0.9844, auc: 0.998, precision: 0.9831, recall: 0.9831\n",
      "2019-01-07T15:30:23.401006, step: 324, loss: 0.04157296195626259, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9655\n",
      "2019-01-07T15:30:23.607925, step: 325, loss: 0.09108507633209229, acc: 0.9531, auc: 0.9985, precision: 1.0, recall: 0.8947\n",
      "2019-01-07T15:30:23.829715, step: 326, loss: 0.06360186636447906, acc: 0.9844, auc: 0.9973, precision: 0.9848, recall: 0.9848\n",
      "2019-01-07T15:30:24.048355, step: 327, loss: 0.10115395486354828, acc: 0.9453, auc: 0.9941, precision: 0.9643, recall: 0.9153\n",
      "2019-01-07T15:30:24.268789, step: 328, loss: 0.04206448793411255, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-07T15:30:24.480102, step: 329, loss: 0.037572383880615234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:24.684509, step: 330, loss: 0.04388665035367012, acc: 0.9766, auc: 0.9995, precision: 0.9867, recall: 0.9737\n",
      "2019-01-07T15:30:24.901753, step: 331, loss: 0.01751241832971573, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:25.113559, step: 332, loss: 0.05081230401992798, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2019-01-07T15:30:25.318868, step: 333, loss: 0.15201522409915924, acc: 0.9766, auc: 0.9836, precision: 0.9833, recall: 0.9672\n",
      "2019-01-07T15:30:25.521046, step: 334, loss: 0.036757104098796844, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2019-01-07T15:30:25.735435, step: 335, loss: 0.0622183158993721, acc: 0.9844, auc: 0.998, precision: 1.0, recall: 0.9623\n",
      "2019-01-07T15:30:25.953449, step: 336, loss: 0.0560050867497921, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9322\n",
      "2019-01-07T15:30:26.158737, step: 337, loss: 0.07706453651189804, acc: 0.9766, auc: 0.9956, precision: 0.9821, recall: 0.9649\n",
      "2019-01-07T15:30:26.381595, step: 338, loss: 0.040956512093544006, acc: 0.9844, auc: 0.9998, precision: 0.9857, recall: 0.9857\n",
      "2019-01-07T15:30:26.600065, step: 339, loss: 0.046407148241996765, acc: 0.9922, auc: 0.9998, precision: 0.9841, recall: 1.0\n",
      "2019-01-07T15:30:26.815721, step: 340, loss: 0.06473301351070404, acc: 0.9844, auc: 0.9985, precision: 0.971, recall: 1.0\n",
      "2019-01-07T15:30:27.022131, step: 341, loss: 0.042173851281404495, acc: 0.9922, auc: 0.9998, precision: 0.9833, recall: 1.0\n",
      "2019-01-07T15:30:27.239453, step: 342, loss: 0.0506742037832737, acc: 0.9844, auc: 0.9988, precision: 0.9841, recall: 0.9841\n",
      "2019-01-07T15:30:27.448441, step: 343, loss: 0.09083905816078186, acc: 0.9531, auc: 0.9973, precision: 1.0, recall: 0.9104\n",
      "2019-01-07T15:30:27.650084, step: 344, loss: 0.14152660965919495, acc: 0.9297, auc: 0.9954, precision: 1.0, recall: 0.8657\n",
      "2019-01-07T15:30:27.858267, step: 345, loss: 0.07422393560409546, acc: 0.9609, auc: 0.9982, precision: 0.9859, recall: 0.9459\n",
      "2019-01-07T15:30:28.062922, step: 346, loss: 0.07899507135152817, acc: 0.9609, auc: 0.9985, precision: 0.9344, recall: 0.9828\n",
      "2019-01-07T15:30:28.283659, step: 347, loss: 0.047466129064559937, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:28.488227, step: 348, loss: 0.09356947243213654, acc: 0.9766, auc: 0.999, precision: 0.9583, recall: 1.0\n",
      "2019-01-07T15:30:28.692026, step: 349, loss: 0.06309393048286438, acc: 0.9844, auc: 0.9993, precision: 0.9825, recall: 0.9825\n",
      "2019-01-07T15:30:28.906864, step: 350, loss: 0.04819723218679428, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9697\n",
      "2019-01-07T15:30:29.123048, step: 351, loss: 0.10230860114097595, acc: 0.9453, auc: 0.9998, precision: 1.0, recall: 0.8939\n",
      "2019-01-07T15:30:29.327642, step: 352, loss: 0.15590794384479523, acc: 0.8984, auc: 0.9968, precision: 1.0, recall: 0.7903\n",
      "2019-01-07T15:30:29.530847, step: 353, loss: 0.06964265555143356, acc: 0.9844, auc: 0.9961, precision: 0.9839, recall: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:30:29.741421, step: 354, loss: 0.060041483491659164, acc: 0.9844, auc: 0.9995, precision: 0.9818, recall: 0.9818\n",
      "2019-01-07T15:30:29.962235, step: 355, loss: 0.19692790508270264, acc: 0.9375, auc: 0.9968, precision: 0.8904, recall: 1.0\n",
      "2019-01-07T15:30:30.184853, step: 356, loss: 0.11583901941776276, acc: 0.9531, auc: 0.9961, precision: 0.9306, recall: 0.9853\n",
      "2019-01-07T15:30:30.385278, step: 357, loss: 0.02193405106663704, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-07T15:30:30.588391, step: 358, loss: 0.08561357855796814, acc: 0.9453, auc: 0.9998, precision: 1.0, recall: 0.8955\n",
      "2019-01-07T15:30:30.800412, step: 359, loss: 0.20979222655296326, acc: 0.8672, auc: 0.9975, precision: 1.0, recall: 0.7606\n",
      "2019-01-07T15:30:31.001333, step: 360, loss: 0.04362582042813301, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9825\n",
      "2019-01-07T15:30:31.206241, step: 361, loss: 0.04866586625576019, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9483\n",
      "2019-01-07T15:30:31.411224, step: 362, loss: 0.2040136754512787, acc: 0.9297, auc: 0.997, precision: 0.8594, recall: 1.0\n",
      "2019-01-07T15:30:31.619934, step: 363, loss: 0.11663109064102173, acc: 0.9375, auc: 0.9983, precision: 0.8971, recall: 0.9839\n",
      "2019-01-07T15:30:31.826684, step: 364, loss: 0.07054991275072098, acc: 0.9844, auc: 0.9961, precision: 0.9853, recall: 0.9853\n",
      "2019-01-07T15:30:32.032340, step: 365, loss: 0.04599740356206894, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9672\n",
      "2019-01-07T15:30:32.241397, step: 366, loss: 0.10742323845624924, acc: 0.9375, auc: 1.0, precision: 1.0, recall: 0.8491\n",
      "2019-01-07T15:30:32.444114, step: 367, loss: 0.10391436517238617, acc: 0.9297, auc: 1.0, precision: 1.0, recall: 0.8548\n",
      "2019-01-07T15:30:32.658211, step: 368, loss: 0.0466439314186573, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.971\n",
      "2019-01-07T15:30:32.869493, step: 369, loss: 0.08945740759372711, acc: 0.9766, auc: 0.9968, precision: 0.9571, recall: 1.0\n",
      "2019-01-07T15:30:33.080160, step: 370, loss: 0.06731964647769928, acc: 0.9844, auc: 1.0, precision: 0.9688, recall: 1.0\n",
      "2019-01-07T15:30:33.296109, step: 371, loss: 0.12775428593158722, acc: 0.9844, auc: 0.9956, precision: 0.9683, recall: 1.0\n",
      "2019-01-07T15:30:33.499229, step: 372, loss: 0.06645074486732483, acc: 0.9844, auc: 0.9978, precision: 1.0, recall: 0.9667\n",
      "2019-01-07T15:30:33.702890, step: 373, loss: 0.04706345871090889, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9552\n",
      "2019-01-07T15:30:33.905365, step: 374, loss: 0.06820091605186462, acc: 0.9688, auc: 1.0, precision: 1.0, recall: 0.9365\n",
      "2019-01-07T15:30:34.109286, step: 375, loss: 0.10127664357423782, acc: 0.9453, auc: 0.999, precision: 1.0, recall: 0.8923\n",
      "2019-01-07T15:30:34.314570, step: 376, loss: 0.07493814080953598, acc: 0.9688, auc: 0.9971, precision: 0.9661, recall: 0.9661\n",
      "2019-01-07T15:30:34.514253, step: 377, loss: 0.05425197631120682, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9672\n",
      "2019-01-07T15:30:34.716342, step: 378, loss: 0.05435832962393761, acc: 0.9844, auc: 0.9993, precision: 0.9859, recall: 0.9859\n",
      "2019-01-07T15:30:34.935349, step: 379, loss: 0.06216885894536972, acc: 0.9844, auc: 0.9973, precision: 0.9857, recall: 0.9857\n",
      "2019-01-07T15:30:35.141442, step: 380, loss: 0.07155798375606537, acc: 0.9609, auc: 0.9973, precision: 0.9846, recall: 0.9412\n",
      "2019-01-07T15:30:35.340906, step: 381, loss: 0.05087633803486824, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9701\n",
      "2019-01-07T15:30:35.548353, step: 382, loss: 0.05779401212930679, acc: 0.9844, auc: 0.998, precision: 0.9841, recall: 0.9841\n",
      "2019-01-07T15:30:35.751473, step: 383, loss: 0.052278343588113785, acc: 0.9766, auc: 0.9983, precision: 0.9861, recall: 0.9726\n",
      "2019-01-07T15:30:35.963726, step: 384, loss: 0.03909212723374367, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9828\n",
      "2019-01-07T15:30:36.175148, step: 385, loss: 0.09149202704429626, acc: 0.9609, auc: 0.996, precision: 0.9444, recall: 0.9623\n",
      "2019-01-07T15:30:36.380213, step: 386, loss: 0.056947991251945496, acc: 0.9922, auc: 0.9958, precision: 1.0, recall: 0.9821\n",
      "2019-01-07T15:30:36.595457, step: 387, loss: 0.02554447576403618, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-07T15:30:36.811689, step: 388, loss: 0.03682727739214897, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9672\n",
      "2019-01-07T15:30:37.018677, step: 389, loss: 0.049307793378829956, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9818\n",
      "2019-01-07T15:30:37.222543, step: 390, loss: 0.03566564619541168, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-01-07T15:30:37.435518, step: 391, loss: 0.06680496037006378, acc: 0.9844, auc: 1.0, precision: 0.963, recall: 1.0\n",
      "2019-01-07T15:30:37.645937, step: 392, loss: 0.07749897241592407, acc: 0.9609, auc: 0.9973, precision: 1.0, recall: 0.9138\n",
      "2019-01-07T15:30:37.860416, step: 393, loss: 0.04355359822511673, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9839\n",
      "2019-01-07T15:30:38.063168, step: 394, loss: 0.016332747414708138, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:38.274344, step: 395, loss: 0.07859796285629272, acc: 0.9766, auc: 0.9973, precision: 1.0, recall: 0.9545\n",
      "2019-01-07T15:30:38.493613, step: 396, loss: 0.07092070579528809, acc: 0.9609, auc: 0.9975, precision: 0.9459, recall: 0.9859\n",
      "2019-01-07T15:30:38.696733, step: 397, loss: 0.01243941392749548, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:38.910051, step: 398, loss: 0.05845240503549576, acc: 0.9922, auc: 0.9933, precision: 1.0, recall: 0.9828\n",
      "2019-01-07T15:30:39.121978, step: 399, loss: 0.033835191279649734, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-07T15:30:39.326476, step: 400, loss: 0.021205272525548935, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:30:47.467570, step: 400, loss: 0.4110001352352974, acc: 0.8607794871794872, auc: 0.9352128205128204, precision: 0.8659384615384615, recall: 0.8579230769230768\n",
      "2019-01-07T15:30:47.676420, step: 401, loss: 0.08204467594623566, acc: 0.9922, auc: 0.9951, precision: 1.0, recall: 0.9836\n",
      "2019-01-07T15:30:47.896160, step: 402, loss: 0.04941464215517044, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9565\n",
      "2019-01-07T15:30:48.111435, step: 403, loss: 0.08100011944770813, acc: 0.9688, auc: 0.9968, precision: 1.0, recall: 0.9403\n",
      "2019-01-07T15:30:48.314889, step: 404, loss: 0.02988709881901741, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-07T15:30:48.517321, step: 405, loss: 0.041581664234399796, acc: 0.9844, auc: 0.999, precision: 0.9855, recall: 0.9855\n",
      "2019-01-07T15:30:48.727081, step: 406, loss: 0.02474270947277546, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-01-07T15:30:48.950038, step: 407, loss: 0.04557151347398758, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9846\n",
      "2019-01-07T15:30:49.163826, step: 408, loss: 0.03504680469632149, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9851\n",
      "2019-01-07T15:30:49.376701, step: 409, loss: 0.038400955498218536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:49.591064, step: 410, loss: 0.042684249579906464, acc: 0.9844, auc: 1.0, precision: 0.9726, recall: 1.0\n",
      "2019-01-07T15:30:49.805093, step: 411, loss: 0.06611573696136475, acc: 0.9609, auc: 0.9983, precision: 0.9818, recall: 0.931\n",
      "2019-01-07T15:30:50.020700, step: 412, loss: 0.020462878048419952, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:50.231647, step: 413, loss: 0.060406412929296494, acc: 0.9688, auc: 0.9993, precision: 1.0, recall: 0.9385\n",
      "2019-01-07T15:30:50.444745, step: 414, loss: 0.05153379589319229, acc: 0.9922, auc: 0.9973, precision: 1.0, recall: 0.9859\n",
      "2019-01-07T15:30:50.649827, step: 415, loss: 0.07193703949451447, acc: 0.9688, auc: 0.998, precision: 0.9828, recall: 0.95\n",
      "2019-01-07T15:30:50.853731, step: 416, loss: 0.05465780943632126, acc: 0.9766, auc: 1.0, precision: 0.9444, recall: 1.0\n",
      "2019-01-07T15:30:51.061791, step: 417, loss: 0.06357163190841675, acc: 0.9844, auc: 0.9988, precision: 0.9661, recall: 1.0\n",
      "2019-01-07T15:30:51.263261, step: 418, loss: 0.04136870056390762, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9844\n",
      "2019-01-07T15:30:51.475370, step: 419, loss: 0.039564892649650574, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:30:51.687145, step: 420, loss: 0.05020885914564133, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9714\n",
      "2019-01-07T15:30:51.890869, step: 421, loss: 0.07690086960792542, acc: 0.9688, auc: 0.9968, precision: 1.0, recall: 0.9394\n",
      "2019-01-07T15:30:52.108592, step: 422, loss: 0.05627736449241638, acc: 0.9844, auc: 0.9978, precision: 1.0, recall: 0.9701\n",
      "2019-01-07T15:30:52.309602, step: 423, loss: 0.054304659366607666, acc: 0.9688, auc: 0.9987, precision: 0.962, recall: 0.987\n",
      "2019-01-07T15:30:52.515941, step: 424, loss: 0.07606889307498932, acc: 0.9688, auc: 0.9995, precision: 0.9452, recall: 1.0\n",
      "2019-01-07T15:30:52.714139, step: 425, loss: 0.08476990461349487, acc: 0.9766, auc: 0.9983, precision: 0.9649, recall: 0.9821\n",
      "2019-01-07T15:30:52.925363, step: 426, loss: 0.0829215794801712, acc: 0.9688, auc: 0.9946, precision: 0.9836, recall: 0.9524\n",
      "2019-01-07T15:30:53.142145, step: 427, loss: 0.08419971913099289, acc: 0.9609, auc: 0.998, precision: 0.9815, recall: 0.9298\n",
      "2019-01-07T15:30:53.354822, step: 428, loss: 0.038930684328079224, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-07T15:30:53.570397, step: 429, loss: 0.06429396569728851, acc: 0.9609, auc: 0.999, precision: 1.0, recall: 0.9194\n",
      "2019-01-07T15:30:53.774606, step: 430, loss: 0.06183629482984543, acc: 0.9766, auc: 0.9973, precision: 0.9706, recall: 0.9851\n",
      "2019-01-07T15:30:53.987257, step: 431, loss: 0.04926827549934387, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9839\n",
      "2019-01-07T15:30:54.188255, step: 432, loss: 0.043850481510162354, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-07T15:30:54.401735, step: 433, loss: 0.0579063780605793, acc: 0.9766, auc: 0.9988, precision: 0.9677, recall: 0.9836\n",
      "2019-01-07T15:30:54.615954, step: 434, loss: 0.0899907797574997, acc: 0.9766, auc: 0.9918, precision: 0.9859, recall: 0.9722\n",
      "2019-01-07T15:30:54.816739, step: 435, loss: 0.04075084254145622, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9697\n",
      "2019-01-07T15:30:55.029140, step: 436, loss: 0.05367427319288254, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9492\n",
      "2019-01-07T15:30:55.246211, step: 437, loss: 0.06849878281354904, acc: 0.9766, auc: 0.9985, precision: 0.9855, recall: 0.9714\n",
      "2019-01-07T15:30:55.461990, step: 438, loss: 0.04915539175271988, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9855\n",
      "2019-01-07T15:30:55.687814, step: 439, loss: 0.05901565030217171, acc: 0.9844, auc: 0.9993, precision: 0.9722, recall: 1.0\n",
      "2019-01-07T15:30:55.893864, step: 440, loss: 0.04423005133867264, acc: 0.9922, auc: 0.999, precision: 0.9828, recall: 1.0\n",
      "2019-01-07T15:30:56.112788, step: 441, loss: 0.04399555176496506, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9726\n",
      "2019-01-07T15:30:56.330791, step: 442, loss: 0.07021303474903107, acc: 0.9766, auc: 0.998, precision: 0.9841, recall: 0.9688\n",
      "2019-01-07T15:30:56.545159, step: 443, loss: 0.031805649399757385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:56.762080, step: 444, loss: 0.038364361971616745, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9833\n",
      "2019-01-07T15:30:56.979635, step: 445, loss: 0.08331470936536789, acc: 0.9688, auc: 0.9958, precision: 0.9833, recall: 0.9516\n",
      "2019-01-07T15:30:57.183197, step: 446, loss: 0.055056072771549225, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9605\n",
      "2019-01-07T15:30:57.398179, step: 447, loss: 0.03440346196293831, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:57.605637, step: 448, loss: 0.03445318713784218, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-01-07T15:30:57.814953, step: 449, loss: 0.06556680053472519, acc: 0.9688, auc: 0.9978, precision: 0.9688, recall: 0.9688\n",
      "2019-01-07T15:30:58.032221, step: 450, loss: 0.035988129675388336, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9688\n",
      "2019-01-07T15:30:58.244328, step: 451, loss: 0.06406670808792114, acc: 0.9844, auc: 0.9988, precision: 0.9839, recall: 0.9839\n",
      "2019-01-07T15:30:58.451817, step: 452, loss: 0.019962258636951447, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:58.668586, step: 453, loss: 0.04641403257846832, acc: 0.9922, auc: 0.999, precision: 0.9855, recall: 1.0\n",
      "2019-01-07T15:30:58.884016, step: 454, loss: 0.04688705503940582, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9672\n",
      "2019-01-07T15:30:59.087475, step: 455, loss: 0.10531862080097198, acc: 0.9531, auc: 0.9929, precision: 0.9863, recall: 0.9351\n",
      "2019-01-07T15:30:59.291863, step: 456, loss: 0.05714143440127373, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9718\n",
      "2019-01-07T15:30:59.511315, step: 457, loss: 0.02797049656510353, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-07T15:30:59.723809, step: 458, loss: 0.03216671943664551, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:30:59.930024, step: 459, loss: 0.052936457097530365, acc: 0.9844, auc: 0.998, precision: 0.9851, recall: 0.9851\n",
      "2019-01-07T15:31:00.150263, step: 460, loss: 0.04532717168331146, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9701\n",
      "2019-01-07T15:31:00.351438, step: 461, loss: 0.07656685262918472, acc: 0.9688, auc: 0.9968, precision: 0.9833, recall: 0.9516\n",
      "2019-01-07T15:31:00.560083, step: 462, loss: 0.04999694973230362, acc: 0.9766, auc: 0.9993, precision: 0.9855, recall: 0.9714\n",
      "2019-01-07T15:31:00.775257, step: 463, loss: 0.03203834220767021, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2019-01-07T15:31:00.973419, step: 464, loss: 0.05913645401597023, acc: 0.9766, auc: 0.9978, precision: 0.9841, recall: 0.9688\n",
      "2019-01-07T15:31:01.177765, step: 465, loss: 0.01924244686961174, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:01.394334, step: 466, loss: 0.06905561685562134, acc: 0.9844, auc: 0.9939, precision: 0.9701, recall: 1.0\n",
      "2019-01-07T15:31:01.611783, step: 467, loss: 0.037278912961483, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9844\n",
      "2019-01-07T15:31:01.821429, step: 468, loss: 0.0518999919295311, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9571\n",
      "start training model\n",
      "2019-01-07T15:31:02.056773, step: 469, loss: 0.01548752374947071, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:02.269994, step: 470, loss: 0.015062826685607433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:02.469461, step: 471, loss: 0.019008157774806023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:02.686328, step: 472, loss: 0.009792122058570385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:02.888201, step: 473, loss: 0.027140848338603973, acc: 0.9844, auc: 0.9998, precision: 0.9828, recall: 0.9828\n",
      "2019-01-07T15:31:03.112916, step: 474, loss: 0.010956734418869019, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:03.332291, step: 475, loss: 0.013470778241753578, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:03.548188, step: 476, loss: 0.01173422485589981, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-07T15:31:03.756293, step: 477, loss: 0.0153126185759902, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:03.962412, step: 478, loss: 0.017574060708284378, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:04.171653, step: 479, loss: 0.014347248710691929, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:04.375146, step: 480, loss: 0.010222642682492733, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:04.587940, step: 481, loss: 0.018589548766613007, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-07T15:31:04.799474, step: 482, loss: 0.011360464617609978, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:05.001330, step: 483, loss: 0.024203874170780182, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:05.202046, step: 484, loss: 0.0225834958255291, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9818\n",
      "2019-01-07T15:31:05.410892, step: 485, loss: 0.014131762087345123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:05.617459, step: 486, loss: 0.019583025947213173, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-07T15:31:05.831655, step: 487, loss: 0.012050904333591461, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:31:06.046571, step: 488, loss: 0.011589892208576202, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:06.263429, step: 489, loss: 0.008744997903704643, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:06.477754, step: 490, loss: 0.0125470831990242, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:06.680542, step: 491, loss: 0.01925300806760788, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-01-07T15:31:06.894692, step: 492, loss: 0.00914047472178936, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:07.111874, step: 493, loss: 0.008918143808841705, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:07.335008, step: 494, loss: 0.007298437878489494, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:07.536767, step: 495, loss: 0.012919397093355656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:07.754013, step: 496, loss: 0.014875390566885471, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:07.964074, step: 497, loss: 0.007459120824933052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:08.182066, step: 498, loss: 0.007282956037670374, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:08.395408, step: 499, loss: 0.010345615446567535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:08.602116, step: 500, loss: 0.02277793362736702, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:31:16.768926, step: 500, loss: 0.4412503991371546, acc: 0.8631846153846154, auc: 0.9390230769230771, precision: 0.8766641025641022, recall: 0.8473512820512821\n",
      "2019-01-07T15:31:16.980680, step: 501, loss: 0.008159800432622433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:17.197319, step: 502, loss: 0.006133199669420719, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:17.397974, step: 503, loss: 0.008200864307582378, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:17.609946, step: 504, loss: 0.008634515106678009, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:17.819152, step: 505, loss: 0.007190410979092121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:18.034034, step: 506, loss: 0.03074846789240837, acc: 0.9844, auc: 0.9995, precision: 0.9718, recall: 1.0\n",
      "2019-01-07T15:31:18.250217, step: 507, loss: 0.012259948998689651, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:18.463444, step: 508, loss: 0.008522750809788704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:18.689268, step: 509, loss: 0.010022690519690514, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-07T15:31:18.905369, step: 510, loss: 0.014184217900037766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:19.120021, step: 511, loss: 0.007451208308339119, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:19.336919, step: 512, loss: 0.009955798275768757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:19.550739, step: 513, loss: 0.006879053544253111, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:19.770942, step: 514, loss: 0.015508662909269333, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-07T15:31:19.983351, step: 515, loss: 0.012574229389429092, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:20.195679, step: 516, loss: 0.005831949878484011, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:20.410034, step: 517, loss: 0.01179849449545145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:20.624275, step: 518, loss: 0.0057596853002905846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:20.827708, step: 519, loss: 0.006381182465702295, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:21.036129, step: 520, loss: 0.005055639427155256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:21.257847, step: 521, loss: 0.023555036634206772, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-01-07T15:31:21.470908, step: 522, loss: 0.015871714800596237, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-07T15:31:21.673517, step: 523, loss: 0.003574386239051819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:21.874222, step: 524, loss: 0.014461925253272057, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-07T15:31:22.088663, step: 525, loss: 0.00999049935489893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:22.310692, step: 526, loss: 0.01534230075776577, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:22.530394, step: 527, loss: 0.01593654975295067, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:22.746350, step: 528, loss: 0.01105877012014389, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:22.960351, step: 529, loss: 0.010266096331179142, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:23.177280, step: 530, loss: 0.012214153073728085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:23.392978, step: 531, loss: 0.013563346117734909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:23.596952, step: 532, loss: 0.014295889995992184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:23.804673, step: 533, loss: 0.00949870329350233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:24.020138, step: 534, loss: 0.005153007805347443, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:24.243684, step: 535, loss: 0.006697357632219791, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:24.447169, step: 536, loss: 0.00803943257778883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:24.660481, step: 537, loss: 0.017204582691192627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:24.864825, step: 538, loss: 0.010983835905790329, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-07T15:31:25.075193, step: 539, loss: 0.015884237363934517, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-07T15:31:25.292759, step: 540, loss: 0.004756065085530281, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:25.503387, step: 541, loss: 0.010907001793384552, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:25.704663, step: 542, loss: 0.014468688517808914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:25.908884, step: 543, loss: 0.024225987493991852, acc: 0.9922, auc: 1.0, precision: 0.9861, recall: 1.0\n",
      "2019-01-07T15:31:26.108479, step: 544, loss: 0.024104155600070953, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-01-07T15:31:26.318692, step: 545, loss: 0.01573864184319973, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-07T15:31:26.529063, step: 546, loss: 0.024846266955137253, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-07T15:31:26.742960, step: 547, loss: 0.08002950996160507, acc: 0.9922, auc: 0.9901, precision: 1.0, recall: 0.9828\n",
      "2019-01-07T15:31:26.958421, step: 548, loss: 0.030073370784521103, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-07T15:31:27.173778, step: 549, loss: 0.013559648767113686, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-07T15:31:27.393332, step: 550, loss: 0.008465668186545372, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:27.608335, step: 551, loss: 0.020561732351779938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:27.821368, step: 552, loss: 0.010647542774677277, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:28.029733, step: 553, loss: 0.014255782589316368, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-01-07T15:31:28.231321, step: 554, loss: 0.008804116398096085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:28.448204, step: 555, loss: 0.0061276135966181755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:28.663431, step: 556, loss: 0.014673957601189613, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-01-07T15:31:28.865877, step: 557, loss: 0.010588540695607662, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:31:29.075034, step: 558, loss: 0.021716414019465446, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-01-07T15:31:29.291567, step: 559, loss: 0.015498099848628044, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-01-07T15:31:29.492327, step: 560, loss: 0.012912798672914505, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-07T15:31:29.707129, step: 561, loss: 0.012985581532120705, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:29.922254, step: 562, loss: 0.025852235034108162, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2019-01-07T15:31:30.128628, step: 563, loss: 0.015313624404370785, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:30.333686, step: 564, loss: 0.014451135881245136, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:30.544466, step: 565, loss: 0.004996602889150381, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:30.752126, step: 566, loss: 0.00533738499507308, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:30.955277, step: 567, loss: 0.01084078848361969, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:31.174169, step: 568, loss: 0.0062388526275753975, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:31.391519, step: 569, loss: 0.012596475891768932, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-07T15:31:31.608242, step: 570, loss: 0.019100476056337357, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-07T15:31:31.822695, step: 571, loss: 0.006726439576596022, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:32.031089, step: 572, loss: 0.007192505523562431, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:32.229613, step: 573, loss: 0.0072148521430790424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:32.413860, step: 574, loss: 0.012035813182592392, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:32.615496, step: 575, loss: 0.005658978596329689, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:32.813608, step: 576, loss: 0.012674303725361824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:33.029172, step: 577, loss: 0.01660831645131111, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:33.251917, step: 578, loss: 0.011211968958377838, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:33.462930, step: 579, loss: 0.011722970753908157, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-01-07T15:31:33.664428, step: 580, loss: 0.010323756374418736, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:33.874243, step: 581, loss: 0.045991312712430954, acc: 0.9922, auc: 0.998, precision: 0.9836, recall: 1.0\n",
      "2019-01-07T15:31:34.088024, step: 582, loss: 0.00899004191160202, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:34.290716, step: 583, loss: 0.008902325294911861, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:34.517536, step: 584, loss: 0.005613934248685837, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:34.751003, step: 585, loss: 0.022976461797952652, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-07T15:31:34.966650, step: 586, loss: 0.0073531558737158775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:35.195515, step: 587, loss: 0.011020101606845856, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:35.409848, step: 588, loss: 0.020578838884830475, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-07T15:31:35.611762, step: 589, loss: 0.0081576406955719, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:35.815818, step: 590, loss: 0.008555018343031406, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:36.027664, step: 591, loss: 0.010106062516570091, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:36.233958, step: 592, loss: 0.01282154768705368, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:36.438405, step: 593, loss: 0.010583489201962948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:36.649413, step: 594, loss: 0.009282818995416164, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:36.860720, step: 595, loss: 0.006529957056045532, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:37.071116, step: 596, loss: 0.006449943874031305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:37.280723, step: 597, loss: 0.006757702212780714, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:37.493833, step: 598, loss: 0.01054004393517971, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:37.697700, step: 599, loss: 0.00886859092861414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:37.916368, step: 600, loss: 0.007045595906674862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:31:45.798120, step: 600, loss: 0.448812913054075, acc: 0.8665897435897437, auc: 0.9408230769230772, precision: 0.8717102564102561, recall: 0.8629384615384614\n",
      "2019-01-07T15:31:45.993545, step: 601, loss: 0.009147805161774158, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:46.193627, step: 602, loss: 0.01711602509021759, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-07T15:31:46.408511, step: 603, loss: 0.013852713629603386, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-07T15:31:46.609552, step: 604, loss: 0.011869321577250957, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:46.824123, step: 605, loss: 0.009443899616599083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:47.032405, step: 606, loss: 0.021378040313720703, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-01-07T15:31:47.237302, step: 607, loss: 0.10426625609397888, acc: 0.9844, auc: 0.9907, precision: 0.9636, recall: 1.0\n",
      "2019-01-07T15:31:47.438393, step: 608, loss: 0.005655231419950724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:47.644787, step: 609, loss: 0.010752072557806969, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-07T15:31:47.860835, step: 610, loss: 0.013322364538908005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:48.070046, step: 611, loss: 0.003999271895736456, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:48.287217, step: 612, loss: 0.0073916176334023476, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:48.489965, step: 613, loss: 0.004980491008609533, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:48.696343, step: 614, loss: 0.012929407879710197, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:48.919333, step: 615, loss: 0.00676302332431078, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:49.140049, step: 616, loss: 0.00405730539932847, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:49.336739, step: 617, loss: 0.007464979775249958, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:49.549345, step: 618, loss: 0.02019897662103176, acc: 0.9922, auc: 1.0, precision: 0.9846, recall: 1.0\n",
      "2019-01-07T15:31:49.748432, step: 619, loss: 0.006474865600466728, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:49.954276, step: 620, loss: 0.009713980369269848, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:50.156539, step: 621, loss: 0.004912564996629953, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:50.371135, step: 622, loss: 0.011571623384952545, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-07T15:31:50.588657, step: 623, loss: 0.008993078023195267, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:50.803133, step: 624, loss: 0.006917483638972044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:31:51.026992, step: 625, loss: 0.003893741872161627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:51.228870, step: 626, loss: 0.005572161637246609, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:31:51.428394, step: 627, loss: 0.004017024300992489, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:51.634814, step: 628, loss: 0.003398990025743842, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:51.839441, step: 629, loss: 0.00634545274078846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:52.041840, step: 630, loss: 0.0061856829561293125, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:52.242008, step: 631, loss: 0.00411286111921072, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:52.441345, step: 632, loss: 0.009295741096138954, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-07T15:31:52.642771, step: 633, loss: 0.002649373607710004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:52.854905, step: 634, loss: 0.003586476668715477, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:53.055640, step: 635, loss: 0.002829187549650669, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:53.258699, step: 636, loss: 0.0033488799817860126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:53.461668, step: 637, loss: 0.00884790439158678, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-07T15:31:53.679549, step: 638, loss: 0.003949346020817757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:53.894507, step: 639, loss: 0.0036933994852006435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:54.108543, step: 640, loss: 0.004219529684633017, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:54.324239, step: 641, loss: 0.0025781341828405857, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:54.538072, step: 642, loss: 0.003303367644548416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:54.752104, step: 643, loss: 0.004941918421536684, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:54.963065, step: 644, loss: 0.0033611636608839035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:55.157434, step: 645, loss: 0.0034959069453179836, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:55.367490, step: 646, loss: 0.004343697801232338, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:55.576242, step: 647, loss: 0.003265703795477748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:55.782959, step: 648, loss: 0.0026749828830361366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:55.986007, step: 649, loss: 0.0036579149309545755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:56.203232, step: 650, loss: 0.00264170253649354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:56.404872, step: 651, loss: 0.0053354231640696526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:56.621304, step: 652, loss: 0.004396352451294661, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:56.834802, step: 653, loss: 0.00256438460201025, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:57.036686, step: 654, loss: 0.003478897735476494, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:57.234340, step: 655, loss: 0.006708433851599693, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:57.429308, step: 656, loss: 0.006023414433002472, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:57.636743, step: 657, loss: 0.006259027402848005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:57.847455, step: 658, loss: 0.002749532461166382, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:58.067356, step: 659, loss: 0.0023969048634171486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:58.265184, step: 660, loss: 0.0027863329742103815, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:58.462935, step: 661, loss: 0.004279282875359058, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:58.667991, step: 662, loss: 0.0023470125161111355, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:58.884634, step: 663, loss: 0.003495234064757824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:59.088015, step: 664, loss: 0.0029757472220808268, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:59.301747, step: 665, loss: 0.004409844521433115, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:59.513495, step: 666, loss: 0.006372732575982809, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:59.717323, step: 667, loss: 0.0033144429326057434, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:31:59.923057, step: 668, loss: 0.0060207173228263855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:00.122863, step: 669, loss: 0.0049900030717253685, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:00.322743, step: 670, loss: 0.003918909002095461, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:00.529576, step: 671, loss: 0.003159796819090843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:00.748074, step: 672, loss: 0.004154837690293789, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:00.954386, step: 673, loss: 0.003759579034522176, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:01.168824, step: 674, loss: 0.007198565639555454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:01.370520, step: 675, loss: 0.05327056348323822, acc: 0.9922, auc: 0.9953, precision: 0.9857, recall: 1.0\n",
      "2019-01-07T15:32:01.581956, step: 676, loss: 0.003460770472884178, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:01.785728, step: 677, loss: 0.0030209070537239313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:01.990700, step: 678, loss: 0.004100598394870758, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:02.203718, step: 679, loss: 0.005992070306092501, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:02.420291, step: 680, loss: 0.004816357512027025, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:02.632119, step: 681, loss: 0.004268118180334568, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:02.834356, step: 682, loss: 0.0024345084093511105, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:03.047545, step: 683, loss: 0.0034782537259161472, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:03.247859, step: 684, loss: 0.005946035496890545, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:03.462413, step: 685, loss: 0.0027758809737861156, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:03.678600, step: 686, loss: 0.0034514665603637695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:03.890466, step: 687, loss: 0.004208963830024004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:04.096410, step: 688, loss: 0.00404193252325058, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:04.304436, step: 689, loss: 0.010791812092065811, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:04.507967, step: 690, loss: 0.004840692970901728, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:04.711248, step: 691, loss: 0.0030797466170042753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:04.926019, step: 692, loss: 0.0038706574123352766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:05.130088, step: 693, loss: 0.004160262644290924, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:05.341970, step: 694, loss: 0.0034867690410465, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:05.551009, step: 695, loss: 0.004767144098877907, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:05.763112, step: 696, loss: 0.004016820341348648, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:05.982008, step: 697, loss: 0.003018635790795088, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:32:06.198907, step: 698, loss: 0.0035108469892293215, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:06.403180, step: 699, loss: 0.003979366738349199, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:06.605978, step: 700, loss: 0.005354780703783035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:32:14.680075, step: 700, loss: 0.4616419267960084, acc: 0.8645846153846154, auc: 0.9413256410256409, precision: 0.8728461538461537, recall: 0.8567564102564103\n",
      "2019-01-07T15:32:14.882936, step: 701, loss: 0.002394675277173519, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:15.095620, step: 702, loss: 0.004171614535152912, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:15.299183, step: 703, loss: 0.003875317517668009, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:15.512612, step: 704, loss: 0.003625973127782345, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:15.727705, step: 705, loss: 0.0018318024231120944, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:15.933541, step: 706, loss: 0.003257918171584606, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:16.141886, step: 707, loss: 0.0017648732755333185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:16.347250, step: 708, loss: 0.003618922783061862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:16.563861, step: 709, loss: 0.003939674701541662, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:16.767520, step: 710, loss: 0.0035045500844717026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:16.968975, step: 711, loss: 0.0036844718270003796, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:17.187868, step: 712, loss: 0.0024080027360469103, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:17.391436, step: 713, loss: 0.0026981020346283913, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:17.594044, step: 714, loss: 0.0028392085805535316, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:17.808798, step: 715, loss: 0.003966925665736198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:18.023220, step: 716, loss: 0.004842429421842098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:18.226149, step: 717, loss: 0.0026377257890999317, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:18.431877, step: 718, loss: 0.0023415731266140938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:18.636156, step: 719, loss: 0.0018393266946077347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:18.850112, step: 720, loss: 0.0025413185358047485, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:19.051865, step: 721, loss: 0.0015928667271509767, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:19.254605, step: 722, loss: 0.0028767383191734552, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:19.471594, step: 723, loss: 0.003065078519284725, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:19.677070, step: 724, loss: 0.004161020740866661, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:19.893523, step: 725, loss: 0.003682510694488883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:20.110167, step: 726, loss: 0.0022403630428016186, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:20.307917, step: 727, loss: 0.002629738301038742, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:20.517697, step: 728, loss: 0.0020267460495233536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:20.730320, step: 729, loss: 0.003383997594937682, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:20.942477, step: 730, loss: 0.00204632175154984, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:21.159754, step: 731, loss: 0.003639765316620469, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:21.380579, step: 732, loss: 0.004160817246884108, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:21.593593, step: 733, loss: 0.004090818110853434, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:21.799148, step: 734, loss: 0.0025002162437886, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:22.009589, step: 735, loss: 0.0032510049641132355, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:22.235085, step: 736, loss: 0.006077640689909458, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:22.443274, step: 737, loss: 0.0022042817436158657, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:22.657692, step: 738, loss: 0.0026871010195463896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:22.876023, step: 739, loss: 0.0030339292716234922, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:23.091423, step: 740, loss: 0.004670613445341587, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:23.301913, step: 741, loss: 0.0019527662079781294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:23.515228, step: 742, loss: 0.0035458779893815517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:23.731569, step: 743, loss: 0.002858601277694106, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:23.952770, step: 744, loss: 0.002947083441540599, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:24.155794, step: 745, loss: 0.003693055361509323, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:24.369484, step: 746, loss: 0.002558366395533085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:24.579503, step: 747, loss: 0.0018530972301959991, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:24.798896, step: 748, loss: 0.002306173788383603, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:25.001413, step: 749, loss: 0.0019540158100426197, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:25.200266, step: 750, loss: 0.002807382494211197, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:25.411497, step: 751, loss: 0.003724845126271248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:25.627456, step: 752, loss: 0.002178808208554983, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:25.827615, step: 753, loss: 0.001611974323168397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:26.025345, step: 754, loss: 0.002918363781645894, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:26.236105, step: 755, loss: 0.0037183051463216543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:26.430570, step: 756, loss: 0.0016822050092741847, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:26.634948, step: 757, loss: 0.0033452098723500967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:26.828528, step: 758, loss: 0.0034356119576841593, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:27.029653, step: 759, loss: 0.0015758215449750423, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:27.218470, step: 760, loss: 0.004845501855015755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:27.409516, step: 761, loss: 0.002904062159359455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:27.617701, step: 762, loss: 0.00309800379909575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:27.833293, step: 763, loss: 0.003428270108997822, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:28.037324, step: 764, loss: 0.002779040951281786, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:28.240492, step: 765, loss: 0.0032355268485844135, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:28.444580, step: 766, loss: 0.0023915611673146486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:28.661719, step: 767, loss: 0.005073141772300005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:32:28.875277, step: 768, loss: 0.002027044538408518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:29.091867, step: 769, loss: 0.003580498043447733, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:29.318726, step: 770, loss: 0.002835288643836975, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:29.532923, step: 771, loss: 0.0015112579567357898, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:29.757156, step: 772, loss: 0.0067199114710092545, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-01-07T15:32:29.973969, step: 773, loss: 0.0022553864400833845, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:30.189312, step: 774, loss: 0.0019076198805123568, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:30.408933, step: 775, loss: 0.0016426063375547528, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:30.622840, step: 776, loss: 0.008357692509889603, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:30.826579, step: 777, loss: 0.001956329448148608, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:31.032679, step: 778, loss: 0.0036183148622512817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:31.236742, step: 779, loss: 0.0016366824274882674, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:31.447323, step: 780, loss: 0.0035821115598082542, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:32:31.669470, step: 781, loss: 0.0029113925993442535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:31.884466, step: 782, loss: 0.0019094647141173482, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:32.106194, step: 783, loss: 0.0018676643958315253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:32.325191, step: 784, loss: 0.0017833095043897629, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:32.530116, step: 785, loss: 0.004685091786086559, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:32.742887, step: 786, loss: 0.0014712987467646599, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:32.966461, step: 787, loss: 0.0018177818274125457, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:33.169554, step: 788, loss: 0.001917407033033669, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:33.368294, step: 789, loss: 0.0018117123981937766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:33.586856, step: 790, loss: 0.001917900750413537, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:33.798563, step: 791, loss: 0.0015301862731575966, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:34.008547, step: 792, loss: 0.0019971360452473164, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:34.224162, step: 793, loss: 0.002035491168498993, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:34.441165, step: 794, loss: 0.0013561531668528914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:34.647628, step: 795, loss: 0.002397531643509865, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:34.866427, step: 796, loss: 0.001160259242169559, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:35.082820, step: 797, loss: 0.002987456973642111, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:35.300233, step: 798, loss: 0.0009836240205913782, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:35.501897, step: 799, loss: 0.0027525066398084164, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:35.714044, step: 800, loss: 0.0023037667851895094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:32:43.771959, step: 800, loss: 0.48556383832907063, acc: 0.8639846153846156, auc: 0.9421641025641023, precision: 0.883297435897436, recall: 0.8425102564102565\n",
      "2019-01-07T15:32:43.969083, step: 801, loss: 0.0020556000526994467, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:44.183090, step: 802, loss: 0.002894459292292595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:44.399070, step: 803, loss: 0.002269031712785363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:44.603360, step: 804, loss: 0.001823338447138667, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:44.819315, step: 805, loss: 0.0014364748494699597, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:45.021986, step: 806, loss: 0.001423681853339076, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:45.239258, step: 807, loss: 0.0018933366518467665, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:45.452959, step: 808, loss: 0.001896156114526093, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:45.666535, step: 809, loss: 0.002654200652614236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:45.876587, step: 810, loss: 0.002684015315026045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:46.076644, step: 811, loss: 0.0011797519400715828, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:46.279586, step: 812, loss: 0.014128251932561398, acc: 0.9922, auc: 1.0, precision: 0.9853, recall: 1.0\n",
      "2019-01-07T15:32:46.483326, step: 813, loss: 0.0016683011781424284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:46.694049, step: 814, loss: 0.0015413216315209866, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:46.896772, step: 815, loss: 0.0022425162605941296, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:47.112074, step: 816, loss: 0.0019702566787600517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:47.323538, step: 817, loss: 0.0019198679365217686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:47.526859, step: 818, loss: 0.00151031871791929, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:47.729716, step: 819, loss: 0.002334623597562313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:47.927923, step: 820, loss: 0.0011013807961717248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:48.126853, step: 821, loss: 0.0017667287029325962, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:48.320571, step: 822, loss: 0.0032622283324599266, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:48.527858, step: 823, loss: 0.0013645858271047473, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:48.738419, step: 824, loss: 0.0016887218225747347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:48.950275, step: 825, loss: 0.0014130007475614548, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:49.162916, step: 826, loss: 0.0013642098056152463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:49.373119, step: 827, loss: 0.00530521385371685, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:49.587606, step: 828, loss: 0.0013932804577052593, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:49.794719, step: 829, loss: 0.001958391396328807, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:50.009257, step: 830, loss: 0.001727780094370246, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:50.224694, step: 831, loss: 0.0024942904710769653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:50.428588, step: 832, loss: 0.0027033723890781403, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:50.623634, step: 833, loss: 0.002687138970941305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:50.832640, step: 834, loss: 0.0032709045335650444, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:51.029055, step: 835, loss: 0.001690183999016881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:51.228563, step: 836, loss: 0.0034310342743992805, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:51.428534, step: 837, loss: 0.001512160524725914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:32:51.638855, step: 838, loss: 0.002891058102250099, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:51.842920, step: 839, loss: 0.001075861742720008, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:52.058354, step: 840, loss: 0.003033209126442671, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:52.275110, step: 841, loss: 0.0021249870769679546, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:52.499263, step: 842, loss: 0.001813477952964604, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:52.713991, step: 843, loss: 0.0017381197540089488, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:52.917638, step: 844, loss: 0.00279303384013474, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:53.133045, step: 845, loss: 0.001073359977453947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:53.333502, step: 846, loss: 0.002265842631459236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:53.547052, step: 847, loss: 0.0014318476896733046, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:53.749969, step: 848, loss: 0.0016278225230053067, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:53.961731, step: 849, loss: 0.0014333853032439947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:54.165120, step: 850, loss: 0.002004846464842558, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:54.372065, step: 851, loss: 0.0017268602969124913, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:54.589788, step: 852, loss: 0.0023470711894333363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:54.808746, step: 853, loss: 0.0015847717877477407, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:55.023317, step: 854, loss: 0.002487249206751585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:55.225466, step: 855, loss: 0.0014521901030093431, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:55.432434, step: 856, loss: 0.001432916964404285, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:55.633879, step: 857, loss: 0.0016594984335824847, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:55.848055, step: 858, loss: 0.001960110617801547, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:56.054811, step: 859, loss: 0.0031899232417345047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:56.270007, step: 860, loss: 0.0022972403094172478, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:56.483374, step: 861, loss: 0.0018552730325609446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:56.687399, step: 862, loss: 0.0013827683869749308, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:56.891018, step: 863, loss: 0.0014528846368193626, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:57.106085, step: 864, loss: 0.0013882422354072332, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:57.314438, step: 865, loss: 0.002469373168423772, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:57.519128, step: 866, loss: 0.0017048604786396027, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:57.734030, step: 867, loss: 0.0013237937819212675, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:57.946517, step: 868, loss: 0.001147169852629304, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:58.146378, step: 869, loss: 0.0011105183511972427, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:58.354931, step: 870, loss: 0.0016353174578398466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:58.558580, step: 871, loss: 0.0026515598874539137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:58.762713, step: 872, loss: 0.0010151817696169019, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:58.964759, step: 873, loss: 0.001425083726644516, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:59.176580, step: 874, loss: 0.002201917115598917, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:59.390006, step: 875, loss: 0.0016646828735247254, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:59.590186, step: 876, loss: 0.0018787386361509562, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:32:59.791303, step: 877, loss: 0.0012954575940966606, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:00.002705, step: 878, loss: 0.0009786647278815508, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:00.204837, step: 879, loss: 0.002520116278901696, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:00.409828, step: 880, loss: 0.0014957982348278165, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:00.610907, step: 881, loss: 0.0013945958344265819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:00.840718, step: 882, loss: 0.0016550872242078185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:01.045769, step: 883, loss: 0.0016383638139814138, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:01.263661, step: 884, loss: 0.0015357235679402947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:01.479871, step: 885, loss: 0.0013890552800148726, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:01.696281, step: 886, loss: 0.002455146750435233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:01.898994, step: 887, loss: 0.0012524400372058153, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:02.109251, step: 888, loss: 0.0021635843440890312, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:02.320735, step: 889, loss: 0.0011922242119908333, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:02.523002, step: 890, loss: 0.002757444977760315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:02.727456, step: 891, loss: 0.0016674313228577375, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:02.941015, step: 892, loss: 0.0014461744576692581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:03.155223, step: 893, loss: 0.002534072380512953, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:03.370065, step: 894, loss: 0.001214848831295967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:03.571459, step: 895, loss: 0.0015055083204060793, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:03.786309, step: 896, loss: 0.001294778659939766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:03.993836, step: 897, loss: 0.0016380331944674253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:04.199371, step: 898, loss: 0.0015108876395970583, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:04.405763, step: 899, loss: 0.0016834167763590813, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:04.608202, step: 900, loss: 0.0007929282728582621, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:33:12.668324, step: 900, loss: 0.5058319530425928, acc: 0.8673846153846153, auc: 0.9431974358974358, precision: 0.8718282051282049, recall: 0.8636230769230769\n",
      "2019-01-07T15:33:12.883276, step: 901, loss: 0.0008947538444772363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:13.093549, step: 902, loss: 0.0015522493049502373, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:13.294427, step: 903, loss: 0.0014835144393146038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:13.494862, step: 904, loss: 0.0014689883682876825, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:13.696536, step: 905, loss: 0.001717273611575365, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:13.901990, step: 906, loss: 0.0008970933267846704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:14.108756, step: 907, loss: 0.0016266322927549481, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:33:14.324932, step: 908, loss: 0.002206678269430995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:14.527818, step: 909, loss: 0.0010343999601900578, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:14.729890, step: 910, loss: 0.00134219229221344, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:14.943630, step: 911, loss: 0.0018518270226195455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:15.148592, step: 912, loss: 0.0023404050152748823, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:15.353729, step: 913, loss: 0.0013275931123644114, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:15.556663, step: 914, loss: 0.0017891895258799195, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:15.771464, step: 915, loss: 0.001272409106604755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:15.969368, step: 916, loss: 0.0015570706455036998, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:16.180525, step: 917, loss: 0.0015235544415190816, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:16.395068, step: 918, loss: 0.0013629777822643518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:16.596586, step: 919, loss: 0.0024437103420495987, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:16.811234, step: 920, loss: 0.0019821247551590204, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:17.031254, step: 921, loss: 0.001355442451313138, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:17.230282, step: 922, loss: 0.0010730637004598975, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:17.435681, step: 923, loss: 0.0018404059810563922, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:17.644804, step: 924, loss: 0.0010082488879561424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:17.846884, step: 925, loss: 0.001282076584175229, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:18.052188, step: 926, loss: 0.001448582042939961, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:18.259720, step: 927, loss: 0.00127131724730134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:18.476554, step: 928, loss: 0.0013219501124694943, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:18.679186, step: 929, loss: 0.000989328371360898, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:18.892413, step: 930, loss: 0.002250479767099023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:19.093682, step: 931, loss: 0.0013668639585375786, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:19.295282, step: 932, loss: 0.001271621324121952, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:19.497347, step: 933, loss: 0.00175725226290524, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:19.710634, step: 934, loss: 0.001130906748585403, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:19.915340, step: 935, loss: 0.0015481360023841262, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:20.126933, step: 936, loss: 0.0015192610444501042, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:33:20.363072, step: 937, loss: 0.0011929093161597848, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:20.562488, step: 938, loss: 0.001833736547268927, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:20.763000, step: 939, loss: 0.0012351558543741703, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:20.974449, step: 940, loss: 0.0008798639755696058, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:21.176692, step: 941, loss: 0.0017580441199243069, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:21.379813, step: 942, loss: 0.0007853044080547988, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:21.593112, step: 943, loss: 0.0010700399288907647, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:21.791452, step: 944, loss: 0.0015184767544269562, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:21.995530, step: 945, loss: 0.001361502567306161, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:22.208094, step: 946, loss: 0.0008872436010278761, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:22.408846, step: 947, loss: 0.001346464967355132, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:22.619690, step: 948, loss: 0.0010795802809298038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:22.817670, step: 949, loss: 0.0013432916020974517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:23.027070, step: 950, loss: 0.0010189095046371222, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:23.220458, step: 951, loss: 0.0007389804814010859, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:23.420384, step: 952, loss: 0.0011884872801601887, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:23.620076, step: 953, loss: 0.0010546226985752583, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:23.833165, step: 954, loss: 0.0011255390709266067, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:24.047172, step: 955, loss: 0.000963687663897872, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:24.247954, step: 956, loss: 0.0011519461404532194, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:24.462997, step: 957, loss: 0.0009015690302476287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:24.677653, step: 958, loss: 0.0011188945500180125, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:24.877686, step: 959, loss: 0.0007216908852569759, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:25.086905, step: 960, loss: 0.0013383558252826333, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:25.300567, step: 961, loss: 0.0017580280546098948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:25.515153, step: 962, loss: 0.0008396626217290759, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:25.716589, step: 963, loss: 0.0006520887254737318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:25.922409, step: 964, loss: 0.0009733201004564762, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:26.136919, step: 965, loss: 0.0009252533782273531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:26.341955, step: 966, loss: 0.0015790730249136686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:26.545252, step: 967, loss: 0.0007840853650122881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:26.759210, step: 968, loss: 0.0010218460811302066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:26.965755, step: 969, loss: 0.0011242568725720048, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:27.168119, step: 970, loss: 0.0009176534949801862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:27.374484, step: 971, loss: 0.0010326933115720749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:27.578324, step: 972, loss: 0.001011540531180799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:27.792399, step: 973, loss: 0.002000273671001196, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:28.001432, step: 974, loss: 0.001478476682677865, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:28.213898, step: 975, loss: 0.0013235399965196848, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:28.416746, step: 976, loss: 0.0007568769506178796, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:28.635807, step: 977, loss: 0.0011943841818720102, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:28.841946, step: 978, loss: 0.0006962116458453238, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:29.044464, step: 979, loss: 0.001006794162094593, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:33:29.250622, step: 980, loss: 0.0016579055227339268, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:29.462718, step: 981, loss: 0.0016313203377649188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:29.675976, step: 982, loss: 0.0011370963184162974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:29.897939, step: 983, loss: 0.0010988509748131037, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:30.101407, step: 984, loss: 0.001031932421028614, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:30.317916, step: 985, loss: 0.0007782037137076259, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:30.522043, step: 986, loss: 0.0009678518981672823, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:30.734201, step: 987, loss: 0.000844899914227426, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:30.947190, step: 988, loss: 0.0015993566485121846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:31.156608, step: 989, loss: 0.001674793311394751, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:31.373593, step: 990, loss: 0.0012708967551589012, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:31.574014, step: 991, loss: 0.0009039118885993958, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:31.789777, step: 992, loss: 0.0007144301780499518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:31.992116, step: 993, loss: 0.0015552504919469357, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:32.203917, step: 994, loss: 0.0008755564922466874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:32.412142, step: 995, loss: 0.000982625992037356, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:32.619203, step: 996, loss: 0.0011278392048552632, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:32.822774, step: 997, loss: 0.0008634472033008933, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:33.034135, step: 998, loss: 0.0016678369138389826, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:33.243227, step: 999, loss: 0.0011052240151911974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:33.456301, step: 1000, loss: 0.0009889575885608792, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:33:41.552395, step: 1000, loss: 0.5164177799836184, acc: 0.8679897435897436, auc: 0.9445076923076923, precision: 0.8782487179487176, recall: 0.8569871794871795\n",
      "2019-01-07T15:33:41.760196, step: 1001, loss: 0.0009945306228473783, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:41.964010, step: 1002, loss: 0.0014486192958429456, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:42.175821, step: 1003, loss: 0.0012216356117278337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:42.374701, step: 1004, loss: 0.0016248291358351707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:42.576096, step: 1005, loss: 0.001022634794935584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:42.781001, step: 1006, loss: 0.0008341829525306821, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:42.984071, step: 1007, loss: 0.0008422069950029254, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:43.193577, step: 1008, loss: 0.0008388710557483137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:43.407760, step: 1009, loss: 0.0007104334654286504, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:43.624279, step: 1010, loss: 0.0012380266562104225, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:43.839756, step: 1011, loss: 0.0007814861019141972, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:44.055162, step: 1012, loss: 0.000915851560421288, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:44.258869, step: 1013, loss: 0.0008975411765277386, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:44.470078, step: 1014, loss: 0.00101851811632514, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:44.679679, step: 1015, loss: 0.0008575882529839873, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:44.897400, step: 1016, loss: 0.001502537983469665, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:45.101567, step: 1017, loss: 0.0011841519735753536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:45.312058, step: 1018, loss: 0.0009349525207653642, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:45.511921, step: 1019, loss: 0.0006135793519206345, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:45.732172, step: 1020, loss: 0.0009376488160341978, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:45.942926, step: 1021, loss: 0.001037699868902564, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:46.148177, step: 1022, loss: 0.000648736022412777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:46.363759, step: 1023, loss: 0.001121407374739647, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:46.567368, step: 1024, loss: 0.000907762092538178, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:46.770397, step: 1025, loss: 0.000868107657879591, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:46.983963, step: 1026, loss: 0.0010836161673069, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:47.208426, step: 1027, loss: 0.0014979017432779074, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:47.431383, step: 1028, loss: 0.000915282522328198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:47.631120, step: 1029, loss: 0.001479674014262855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:47.845062, step: 1030, loss: 0.0006235144101083279, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:48.050299, step: 1031, loss: 0.0010754227405413985, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:48.264868, step: 1032, loss: 0.001515732379630208, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:48.480838, step: 1033, loss: 0.0006605628877878189, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:48.693909, step: 1034, loss: 0.0011239087907597423, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:48.897589, step: 1035, loss: 0.000902143947314471, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:49.114136, step: 1036, loss: 0.001188176218420267, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:49.327210, step: 1037, loss: 0.000853220815770328, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:49.532257, step: 1038, loss: 0.0012341622496023774, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:49.742938, step: 1039, loss: 0.0011483351700007915, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:49.958228, step: 1040, loss: 0.0013583195395767689, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:50.177280, step: 1041, loss: 0.0006992859998717904, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:50.398567, step: 1042, loss: 0.000781868351623416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:50.608443, step: 1043, loss: 0.0006561229238286614, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:50.810606, step: 1044, loss: 0.0008273212006315589, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:51.032577, step: 1045, loss: 0.0011801966466009617, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:51.239408, step: 1046, loss: 0.0010384103516116738, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:51.455108, step: 1047, loss: 0.0008218616712838411, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:51.668087, step: 1048, loss: 0.00103284465149045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:51.870536, step: 1049, loss: 0.0010000175097957253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:33:52.074520, step: 1050, loss: 0.0010235217632725835, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:52.284034, step: 1051, loss: 0.0008945983718149364, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:52.485768, step: 1052, loss: 0.0007572526228614151, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:52.704427, step: 1053, loss: 0.000997106428258121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:52.919825, step: 1054, loss: 0.0007593964110128582, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:53.124507, step: 1055, loss: 0.0011342688230797648, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:53.337863, step: 1056, loss: 0.0006122812628746033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:53.540070, step: 1057, loss: 0.0011408707359805703, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:53.744095, step: 1058, loss: 0.0018975016428157687, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:53.955300, step: 1059, loss: 0.0010129737202078104, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:54.158745, step: 1060, loss: 0.0009212745353579521, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:54.375605, step: 1061, loss: 0.0006632436998188496, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:54.587887, step: 1062, loss: 0.0010679233819246292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:54.794446, step: 1063, loss: 0.0008859605295583606, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:54.998488, step: 1064, loss: 0.0011049683671444654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:55.215702, step: 1065, loss: 0.000611153373029083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:55.430962, step: 1066, loss: 0.0018445753958076239, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:55.634915, step: 1067, loss: 0.0009734726045280695, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:55.839326, step: 1068, loss: 0.0008651085663586855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:56.045708, step: 1069, loss: 0.0015417335089296103, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:56.252087, step: 1070, loss: 0.0011442347895354033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:56.463019, step: 1071, loss: 0.0009126681834459305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:56.666130, step: 1072, loss: 0.0012586478842422366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:56.868615, step: 1073, loss: 0.0009793235221877694, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:57.092837, step: 1074, loss: 0.002884791698306799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:57.308675, step: 1075, loss: 0.0019218577072024345, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:57.525821, step: 1076, loss: 0.0006906103226356208, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:57.737022, step: 1077, loss: 0.0009991764090955257, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:57.950898, step: 1078, loss: 0.0005597665440291166, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:58.155635, step: 1079, loss: 0.0010938296327367425, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:58.358692, step: 1080, loss: 0.0012697007041424513, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:58.582185, step: 1081, loss: 0.0013507793191820383, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:58.797833, step: 1082, loss: 0.0008924978319555521, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:59.001322, step: 1083, loss: 0.0007259846315719187, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:59.216732, step: 1084, loss: 0.0011441294336691499, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:59.417728, step: 1085, loss: 0.0008104477310553193, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:59.627743, step: 1086, loss: 0.0007345634512603283, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:33:59.827947, step: 1087, loss: 0.0007261158898472786, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:00.038714, step: 1088, loss: 0.0012167596723884344, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:00.240418, step: 1089, loss: 0.0012842644937336445, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:00.456277, step: 1090, loss: 0.0010419207392260432, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:00.670256, step: 1091, loss: 0.0009264449472539127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:00.881983, step: 1092, loss: 0.0014360958011820912, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:34:01.118477, step: 1093, loss: 0.0006222461233846843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:01.332464, step: 1094, loss: 0.0008401032537221909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:01.533062, step: 1095, loss: 0.0007375834393315017, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:01.749776, step: 1096, loss: 0.0005130195640958846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:01.959248, step: 1097, loss: 0.000705822603777051, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:02.171580, step: 1098, loss: 0.0007347893551923335, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:02.380672, step: 1099, loss: 0.0007138852379284799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:02.582157, step: 1100, loss: 0.0009448297205381095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:34:10.587976, step: 1100, loss: 0.526580029573196, acc: 0.8699923076923076, auc: 0.9445076923076925, precision: 0.8805384615384614, recall: 0.8586384615384614\n",
      "2019-01-07T15:34:10.784882, step: 1101, loss: 0.0008366294205188751, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:10.988070, step: 1102, loss: 0.0006797179812565446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:11.191815, step: 1103, loss: 0.0005331746069714427, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:11.395486, step: 1104, loss: 0.0005543224397115409, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:11.616776, step: 1105, loss: 0.0006263611139729619, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:11.827199, step: 1106, loss: 0.000961619196459651, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:12.029934, step: 1107, loss: 0.0010002738563343883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:12.238053, step: 1108, loss: 0.0006514657288789749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:12.450719, step: 1109, loss: 0.0005643118638545275, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:12.660896, step: 1110, loss: 0.0006430213688872755, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:12.868054, step: 1111, loss: 0.0011564153246581554, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:13.071957, step: 1112, loss: 0.000872788077685982, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:13.276154, step: 1113, loss: 0.000638977624475956, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:13.486147, step: 1114, loss: 0.0009555184515193105, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:13.692166, step: 1115, loss: 0.0008910048054531217, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:13.899664, step: 1116, loss: 0.0007864368963055313, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:14.121100, step: 1117, loss: 0.0007143201073631644, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:14.331314, step: 1118, loss: 0.0008471166365779936, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:34:14.545073, step: 1119, loss: 0.0011561994906514883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:14.760000, step: 1120, loss: 0.001067543402314186, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:14.966195, step: 1121, loss: 0.0006135749281384051, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:15.183231, step: 1122, loss: 0.0007891706191003323, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:15.395067, step: 1123, loss: 0.0009103317279368639, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:15.594830, step: 1124, loss: 0.0006371932104229927, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:15.794403, step: 1125, loss: 0.00101350131444633, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:16.002756, step: 1126, loss: 0.000716016162186861, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:16.216144, step: 1127, loss: 0.0007849646499380469, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:16.432850, step: 1128, loss: 0.0006114860298112035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:16.646384, step: 1129, loss: 0.000822498113848269, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:16.854899, step: 1130, loss: 0.0005684432107955217, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:17.070359, step: 1131, loss: 0.0005996936815790832, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:17.285122, step: 1132, loss: 0.0007085757097229362, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:17.499570, step: 1133, loss: 0.0012358824023976922, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:17.706778, step: 1134, loss: 0.00043493954581208527, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:17.922408, step: 1135, loss: 0.0009690007427707314, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:18.133524, step: 1136, loss: 0.00043873197864741087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:18.346711, step: 1137, loss: 0.0011205123737454414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:18.563549, step: 1138, loss: 0.0007926871767267585, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:18.769359, step: 1139, loss: 0.001001683878712356, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:18.983367, step: 1140, loss: 0.0006310158059932292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:19.204000, step: 1141, loss: 0.0005680971662513912, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:19.406428, step: 1142, loss: 0.0014599591959267855, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:19.622754, step: 1143, loss: 0.000977956922724843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:19.839858, step: 1144, loss: 0.0014328125398606062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:20.052730, step: 1145, loss: 0.0011925719445571303, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:20.267930, step: 1146, loss: 0.0005908414022997022, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:20.482315, step: 1147, loss: 0.0004539826768450439, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:20.694212, step: 1148, loss: 0.0012572510167956352, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:20.909003, step: 1149, loss: 0.0005371420993469656, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:21.122817, step: 1150, loss: 0.0008531272178515792, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:21.327934, step: 1151, loss: 0.0004128042492084205, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:21.545816, step: 1152, loss: 0.0005128337070345879, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:21.745793, step: 1153, loss: 0.0014385483227670193, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:21.960221, step: 1154, loss: 0.0009195683524012566, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:22.175430, step: 1155, loss: 0.0010310146026313305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:22.388462, step: 1156, loss: 0.000920380000025034, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:22.598380, step: 1157, loss: 0.0005722352652810514, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:22.805073, step: 1158, loss: 0.001047400408424437, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:23.000571, step: 1159, loss: 0.0007112364401109517, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:23.210309, step: 1160, loss: 0.0005463766283355653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:23.420735, step: 1161, loss: 0.0006286611896939576, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:23.622598, step: 1162, loss: 0.0004749761719722301, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:23.829274, step: 1163, loss: 0.0008932020864449441, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:24.036540, step: 1164, loss: 0.0009479493601247668, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:24.249110, step: 1165, loss: 0.0007058276096358895, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:24.463459, step: 1166, loss: 0.0006101342150941491, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:24.679704, step: 1167, loss: 0.000693516107276082, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:24.894906, step: 1168, loss: 0.0006880075670778751, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:25.109957, step: 1169, loss: 0.0005510580958798528, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:25.344105, step: 1170, loss: 0.0008700728649273515, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:25.557678, step: 1171, loss: 0.0005398968933150172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:25.761653, step: 1172, loss: 0.0005159162683412433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:25.983929, step: 1173, loss: 0.0004991363966837525, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:26.182679, step: 1174, loss: 0.0010615310166031122, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:26.395804, step: 1175, loss: 0.000904852757230401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:26.608785, step: 1176, loss: 0.0006946856738068163, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:26.821965, step: 1177, loss: 0.0010197985684499145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:27.030206, step: 1178, loss: 0.0007114459294825792, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:27.241294, step: 1179, loss: 0.00036527158226817846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:27.441837, step: 1180, loss: 0.0006028572097420692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:27.644663, step: 1181, loss: 0.000592055672314018, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:27.844674, step: 1182, loss: 0.0007853200659155846, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:28.062882, step: 1183, loss: 0.0006617165636271238, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:28.275804, step: 1184, loss: 0.0008059188257902861, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:28.479350, step: 1185, loss: 0.0005731968558393419, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:28.683940, step: 1186, loss: 0.0005516326637007296, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:28.894364, step: 1187, loss: 0.0008387044072151184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:29.095051, step: 1188, loss: 0.002005466725677252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:29.296087, step: 1189, loss: 0.000803755596280098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:34:29.507129, step: 1190, loss: 0.001048593781888485, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:29.719605, step: 1191, loss: 0.0009266024571843445, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:29.923096, step: 1192, loss: 0.0006971589755266905, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:30.141010, step: 1193, loss: 0.0011990895727649331, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:30.358839, step: 1194, loss: 0.0007664022268727422, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:30.572115, step: 1195, loss: 0.0006623855442740023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:30.784800, step: 1196, loss: 0.0008541871793568134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:30.996850, step: 1197, loss: 0.0006313563790172338, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:31.200639, step: 1198, loss: 0.0008327895193360746, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:31.400855, step: 1199, loss: 0.0005449567688629031, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:31.624169, step: 1200, loss: 0.0004990312736481428, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:34:39.727177, step: 1200, loss: 0.5440449867493067, acc: 0.8685948717948719, auc: 0.9444923076923077, precision: 0.8733358974358973, recall: 0.8649076923076922\n",
      "2019-01-07T15:34:39.937350, step: 1201, loss: 0.0007848747773095965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:40.152500, step: 1202, loss: 0.0004982251557521522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:40.349522, step: 1203, loss: 0.0006580774206668139, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:40.563406, step: 1204, loss: 0.0008452496258541942, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:40.771758, step: 1205, loss: 0.0006113050039857626, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:40.971845, step: 1206, loss: 0.0005000564269721508, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:41.171505, step: 1207, loss: 0.0006897074053995311, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:41.373549, step: 1208, loss: 0.0006652059382759035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:41.575848, step: 1209, loss: 0.0010104845277965069, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:41.789378, step: 1210, loss: 0.0009795903461053967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:41.999053, step: 1211, loss: 0.0006353899370878935, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:42.201143, step: 1212, loss: 0.0008435735944658518, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:42.422373, step: 1213, loss: 0.00048794527538120747, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:42.639955, step: 1214, loss: 0.0005944885779172182, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:42.863003, step: 1215, loss: 0.0006179481861181557, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:43.067118, step: 1216, loss: 0.0005552766378968954, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:43.271118, step: 1217, loss: 0.0008273370331153274, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:43.481028, step: 1218, loss: 0.0010135640623047948, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:43.683239, step: 1219, loss: 0.0008953158976510167, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:43.897624, step: 1220, loss: 0.0006534444401040673, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:44.111319, step: 1221, loss: 0.0006200841162353754, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:44.324031, step: 1222, loss: 0.0007043654914014041, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:44.539285, step: 1223, loss: 0.0005464602145366371, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:44.754022, step: 1224, loss: 0.00038983451668173075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:44.954702, step: 1225, loss: 0.0006964289350435138, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:45.168753, step: 1226, loss: 0.0008246900979429483, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:45.370650, step: 1227, loss: 0.0008250119281001389, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:45.571569, step: 1228, loss: 0.0011099386028945446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:45.788755, step: 1229, loss: 0.0008629410876892507, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:46.000184, step: 1230, loss: 0.00046774197835475206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:46.219546, step: 1231, loss: 0.0008343117660842836, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:46.435601, step: 1232, loss: 0.0009383276337757707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:46.639440, step: 1233, loss: 0.0006692570168524981, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:46.855328, step: 1234, loss: 0.0005266931839287281, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:47.079626, step: 1235, loss: 0.0006484240875579417, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:47.288431, step: 1236, loss: 0.0008987101027742028, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:47.500897, step: 1237, loss: 0.001021137461066246, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:47.698739, step: 1238, loss: 0.0004189953615423292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:47.911744, step: 1239, loss: 0.0004709377244580537, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:48.114354, step: 1240, loss: 0.0007602976402267814, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:48.330417, step: 1241, loss: 0.0005633083637803793, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:48.532259, step: 1242, loss: 0.00093504146207124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:48.743400, step: 1243, loss: 0.0003620584320742637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:48.959994, step: 1244, loss: 0.0010746027110144496, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:49.170818, step: 1245, loss: 0.0005948402686044574, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:49.377575, step: 1246, loss: 0.0008397641940973699, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:49.590415, step: 1247, loss: 0.0007697545806877315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:49.809687, step: 1248, loss: 0.0005464429268613458, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:34:50.033133, step: 1249, loss: 0.0007336406270042062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:50.235414, step: 1250, loss: 0.0005320326890796423, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:50.449475, step: 1251, loss: 0.0005263620987534523, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:50.665585, step: 1252, loss: 0.0007083664531819522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:50.866822, step: 1253, loss: 0.0007012900896370411, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:51.066199, step: 1254, loss: 0.00033387145958840847, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:51.267636, step: 1255, loss: 0.00042413949267938733, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:51.471266, step: 1256, loss: 0.0005067504243925214, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:51.685825, step: 1257, loss: 0.0005746062379330397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:51.892357, step: 1258, loss: 0.0005862519610673189, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:52.095921, step: 1259, loss: 0.0006221231305971742, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:34:52.293520, step: 1260, loss: 0.0003939501184504479, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:52.502671, step: 1261, loss: 0.0005556398537009954, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:52.703717, step: 1262, loss: 0.0008131119539029896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:52.909042, step: 1263, loss: 0.0004853864083997905, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:53.138378, step: 1264, loss: 0.000532378675416112, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:53.351142, step: 1265, loss: 0.00046312014455907047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:53.564868, step: 1266, loss: 0.0006026573828421533, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:53.775414, step: 1267, loss: 0.0003769598843064159, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:53.979392, step: 1268, loss: 0.0006300652166828513, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:54.184503, step: 1269, loss: 0.00048197066644206643, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:54.397849, step: 1270, loss: 0.0005543545121327043, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:54.609964, step: 1271, loss: 0.00039667607052251697, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:54.815737, step: 1272, loss: 0.0005258952151052654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:55.022606, step: 1273, loss: 0.0007561292150057852, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:55.226741, step: 1274, loss: 0.0004515773616731167, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:55.438685, step: 1275, loss: 0.0005329463165253401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:55.657640, step: 1276, loss: 0.0004729428037535399, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:55.868571, step: 1277, loss: 0.0005462924600578845, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:56.072802, step: 1278, loss: 0.0005336446920409799, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:56.275922, step: 1279, loss: 0.0006830115453340113, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:56.477865, step: 1280, loss: 0.0006568083772435784, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:56.681348, step: 1281, loss: 0.0007823220803402364, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:56.886952, step: 1282, loss: 0.0007214707438834012, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:57.101347, step: 1283, loss: 0.0005143972812220454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:57.301962, step: 1284, loss: 0.0007567641441710293, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:57.513040, step: 1285, loss: 0.0005285178776830435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:57.725223, step: 1286, loss: 0.0005680154426954687, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:57.941132, step: 1287, loss: 0.00041255971882492304, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:58.152678, step: 1288, loss: 0.00036696018651127815, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:58.370001, step: 1289, loss: 0.0004456556052900851, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:58.591610, step: 1290, loss: 0.00048684095963835716, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:58.806074, step: 1291, loss: 0.0006216231849975884, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:58.992266, step: 1292, loss: 0.0004742612945847213, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:59.195469, step: 1293, loss: 0.0006330806063488126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:59.408316, step: 1294, loss: 0.0006081020692363381, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:59.624046, step: 1295, loss: 0.0006827784236520529, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:34:59.828713, step: 1296, loss: 0.0004914712044410408, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:00.034122, step: 1297, loss: 0.00039684673538431525, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:00.233010, step: 1298, loss: 0.0005771305877715349, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:00.427787, step: 1299, loss: 0.0004211427294649184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:00.609944, step: 1300, loss: 0.00037934823194518685, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:35:08.654247, step: 1300, loss: 0.5545504811482552, acc: 0.8693948717948717, auc: 0.945205128205128, precision: 0.8780025641025645, recall: 0.8607512820512822\n",
      "2019-01-07T15:35:08.860012, step: 1301, loss: 0.0006563609931617975, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:09.075524, step: 1302, loss: 0.000815289793536067, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:09.288218, step: 1303, loss: 0.00040482962504029274, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:09.487063, step: 1304, loss: 0.0006568181561306119, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:09.706932, step: 1305, loss: 0.000484376068925485, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:09.925391, step: 1306, loss: 0.0003282737743575126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:10.128858, step: 1307, loss: 0.0006227883277460933, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:10.351016, step: 1308, loss: 0.000529217068105936, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:10.551858, step: 1309, loss: 0.0005784134264104068, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:10.754630, step: 1310, loss: 0.0007539321668446064, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:10.959450, step: 1311, loss: 0.0005138351116329432, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:11.161371, step: 1312, loss: 0.0005932407802902162, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:11.370414, step: 1313, loss: 0.0007771799573674798, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:11.571193, step: 1314, loss: 0.00040048890514299273, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:11.788176, step: 1315, loss: 0.0007751112570986152, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:12.001201, step: 1316, loss: 0.0004221448616590351, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:12.213569, step: 1317, loss: 0.0008345654932782054, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:12.421368, step: 1318, loss: 0.0004598802188411355, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:12.629096, step: 1319, loss: 0.0005432863254100084, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:12.828372, step: 1320, loss: 0.0003402471193112433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:13.041704, step: 1321, loss: 0.0007427670061588287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:13.245454, step: 1322, loss: 0.0005960181588307023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:13.451295, step: 1323, loss: 0.0008753329748287797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:13.657075, step: 1324, loss: 0.0007041464559733868, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:13.861565, step: 1325, loss: 0.0005992717924527824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:14.072026, step: 1326, loss: 0.0004115128831472248, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:14.270821, step: 1327, loss: 0.0005404141847975552, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:14.492082, step: 1328, loss: 0.0007760896114632487, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:14.692587, step: 1329, loss: 0.0009017394622787833, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:35:14.898818, step: 1330, loss: 0.0006650331779383123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:15.108875, step: 1331, loss: 0.00031096814200282097, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:15.328672, step: 1332, loss: 0.00045247527305036783, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:15.527242, step: 1333, loss: 0.0006128564709797502, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:15.736636, step: 1334, loss: 0.0006492679822258651, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:15.939770, step: 1335, loss: 0.0005846734857186675, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:16.154174, step: 1336, loss: 0.00075154984369874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:16.366008, step: 1337, loss: 0.0005607332568615675, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:16.589355, step: 1338, loss: 0.0008520944975316525, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:16.803030, step: 1339, loss: 0.00047172181075438857, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:17.015590, step: 1340, loss: 0.0004660443519242108, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:17.225485, step: 1341, loss: 0.0006499145529232919, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:17.429198, step: 1342, loss: 0.0005650713574141264, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:17.642734, step: 1343, loss: 0.0007837652810849249, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:17.849782, step: 1344, loss: 0.0004737836425192654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:18.063459, step: 1345, loss: 0.0005418166983872652, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:18.271843, step: 1346, loss: 0.00028743367874994874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:18.451934, step: 1347, loss: 0.0005026520229876041, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:18.649847, step: 1348, loss: 0.0003955358115490526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:18.834379, step: 1349, loss: 0.000552747689653188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:19.029874, step: 1350, loss: 0.0005304688238538802, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:19.235764, step: 1351, loss: 0.0005158089334145188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:19.437079, step: 1352, loss: 0.00033188305678777397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:19.645791, step: 1353, loss: 0.0003715336206369102, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:19.848671, step: 1354, loss: 0.00036605808418244123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:20.063660, step: 1355, loss: 0.0005041001713834703, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:20.290483, step: 1356, loss: 0.0005172924138605595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:20.505155, step: 1357, loss: 0.0006054648547433317, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:20.707372, step: 1358, loss: 0.00039498740807175636, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:20.919710, step: 1359, loss: 0.0007677928078919649, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:21.129555, step: 1360, loss: 0.0008157748379744589, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:21.334974, step: 1361, loss: 0.0006219634087756276, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:21.547948, step: 1362, loss: 0.00045047630555927753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:21.763052, step: 1363, loss: 0.0005011024186387658, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:21.979845, step: 1364, loss: 0.0005196992424316704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:22.193978, step: 1365, loss: 0.0005914070643484592, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:22.412694, step: 1366, loss: 0.0004575829370878637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:22.617568, step: 1367, loss: 0.0005314314039424062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:22.831658, step: 1368, loss: 0.0004836874140892178, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:23.030018, step: 1369, loss: 0.0006048367358744144, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:23.244363, step: 1370, loss: 0.0007669721962884068, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:23.456954, step: 1371, loss: 0.0005374047323130071, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:23.658220, step: 1372, loss: 0.0006015293765813112, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:23.873242, step: 1373, loss: 0.00047437139437533915, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:24.088965, step: 1374, loss: 0.00030557409627363086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:24.291056, step: 1375, loss: 0.0005217634607106447, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:24.501803, step: 1376, loss: 0.0006814312655478716, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:24.712767, step: 1377, loss: 0.0005783507949672639, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:24.916165, step: 1378, loss: 0.000589330738876015, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:25.120537, step: 1379, loss: 0.00044558869558386505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:25.340364, step: 1380, loss: 0.000523202761542052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:25.552440, step: 1381, loss: 0.00048148061614483595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:25.754028, step: 1382, loss: 0.00035327597288414836, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:25.964832, step: 1383, loss: 0.0005397816421464086, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:26.168998, step: 1384, loss: 0.0007926913676783442, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:26.389077, step: 1385, loss: 0.00035285361809656024, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:26.595937, step: 1386, loss: 0.0006635897443629801, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:26.795708, step: 1387, loss: 0.00046830138307996094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:27.006801, step: 1388, loss: 0.0004418371827341616, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:27.208844, step: 1389, loss: 0.0005616380949504673, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:27.418005, step: 1390, loss: 0.000769833626691252, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:27.630487, step: 1391, loss: 0.0006044028559699655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:27.844376, step: 1392, loss: 0.000562138098757714, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:28.047268, step: 1393, loss: 0.0005828314460813999, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:28.261387, step: 1394, loss: 0.000606229470577091, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:28.478270, step: 1395, loss: 0.0007024381193332374, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:28.685958, step: 1396, loss: 0.0005433719488792121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:28.885734, step: 1397, loss: 0.0005534688825719059, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:29.105591, step: 1398, loss: 0.0005707432283088565, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:29.306955, step: 1399, loss: 0.0008229517261497676, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:35:29.520610, step: 1400, loss: 0.0004427750827744603, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:35:37.549059, step: 1400, loss: 0.5615845998892417, acc: 0.8695948717948718, auc: 0.9452871794871794, precision: 0.8832846153846153, recall: 0.8551641025641025\n",
      "2019-01-07T15:35:37.746724, step: 1401, loss: 0.000510662270244211, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:37.951839, step: 1402, loss: 0.0005118739791214466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:38.160862, step: 1403, loss: 0.0008077961392700672, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:38.370180, step: 1404, loss: 0.0005375779001042247, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-07T15:35:38.608247, step: 1405, loss: 0.00048589176731184125, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:38.817592, step: 1406, loss: 0.0005836407653987408, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:39.032145, step: 1407, loss: 0.0005028857267461717, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:39.238145, step: 1408, loss: 0.0003724636335391551, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:39.444109, step: 1409, loss: 0.000730764411855489, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:39.664024, step: 1410, loss: 0.00044960627565160394, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:39.865321, step: 1411, loss: 0.0004306908813305199, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:40.067148, step: 1412, loss: 0.00032292993273586035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:40.268878, step: 1413, loss: 0.0005627054488286376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:40.469451, step: 1414, loss: 0.0005254666903056204, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:40.682186, step: 1415, loss: 0.0002930560440290719, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:40.883243, step: 1416, loss: 0.00039110513171181083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:41.090205, step: 1417, loss: 0.00037220565718598664, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:41.301184, step: 1418, loss: 0.0004806217912118882, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:41.514106, step: 1419, loss: 0.0004138046351727098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:41.726239, step: 1420, loss: 0.0003344080760143697, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:41.941051, step: 1421, loss: 0.0003924426855519414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:42.145788, step: 1422, loss: 0.0006264097755774856, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:42.361199, step: 1423, loss: 0.00034191858139820397, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:42.562973, step: 1424, loss: 0.0007870462723076344, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:42.762395, step: 1425, loss: 0.0003767200978472829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:42.974824, step: 1426, loss: 0.0005242004408501089, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:43.187770, step: 1427, loss: 0.0003567171806935221, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:43.405322, step: 1428, loss: 0.0003956237924285233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:43.614519, step: 1429, loss: 0.0002866365248337388, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:43.823303, step: 1430, loss: 0.0004077047051396221, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:44.043440, step: 1431, loss: 0.00039193997508846223, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:44.264235, step: 1432, loss: 0.0002634418196976185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:44.468826, step: 1433, loss: 0.0004905464593321085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:44.679580, step: 1434, loss: 0.0004692110524047166, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:44.903220, step: 1435, loss: 0.0004927853005938232, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:45.115668, step: 1436, loss: 0.00043561088386923075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:45.324707, step: 1437, loss: 0.0006022104644216597, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:45.519160, step: 1438, loss: 0.0006673454772680998, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:45.715554, step: 1439, loss: 0.00040279122185893357, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:45.919265, step: 1440, loss: 0.00040580888162367046, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:46.131830, step: 1441, loss: 0.0006584342336282134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:46.338824, step: 1442, loss: 0.0005735702579841018, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:46.541555, step: 1443, loss: 0.0005758799961768091, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:46.743503, step: 1444, loss: 0.0004600905813276768, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:46.946994, step: 1445, loss: 0.0006633286247961223, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:47.148756, step: 1446, loss: 0.0004655943193938583, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:47.348963, step: 1447, loss: 0.0003661923110485077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:47.564227, step: 1448, loss: 0.0003744981368072331, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:47.779703, step: 1449, loss: 0.0003547059022821486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:47.980989, step: 1450, loss: 0.00046543485950678587, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:48.184535, step: 1451, loss: 0.000575569923967123, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:48.396011, step: 1452, loss: 0.0009005340980365872, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:48.601937, step: 1453, loss: 0.0004592336481437087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:48.803930, step: 1454, loss: 0.0005237017176114023, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:48.996238, step: 1455, loss: 0.0003524337662383914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:49.176037, step: 1456, loss: 0.00030889303889125586, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:49.388671, step: 1457, loss: 0.00037477887235581875, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:49.599858, step: 1458, loss: 0.0003060624294448644, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:49.800746, step: 1459, loss: 0.0005070732440799475, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:50.006262, step: 1460, loss: 0.00032559194369241595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:50.206772, step: 1461, loss: 0.0004145438433624804, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:50.426114, step: 1462, loss: 0.0003438443527556956, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:50.640240, step: 1463, loss: 0.0004940350772812963, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:50.845993, step: 1464, loss: 0.0005398510256782174, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:51.049003, step: 1465, loss: 0.00037753870128653944, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:51.259943, step: 1466, loss: 0.0003870078071486205, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:51.463588, step: 1467, loss: 0.00038261915324255824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:51.682372, step: 1468, loss: 0.000389367836760357, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:35:51.895038, step: 1469, loss: 0.0005925646401010454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:52.117311, step: 1470, loss: 0.00032878475030884147, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:52.329788, step: 1471, loss: 0.00040350761264562607, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:52.531058, step: 1472, loss: 0.0003993118880316615, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:52.741926, step: 1473, loss: 0.00026039162185043097, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:52.944243, step: 1474, loss: 0.0004996603238396347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:53.157039, step: 1475, loss: 0.0005835354095324874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:53.371230, step: 1476, loss: 0.0004425386432558298, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:53.585483, step: 1477, loss: 0.0005303866346366704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:53.804730, step: 1478, loss: 0.0003639508504420519, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:54.022906, step: 1479, loss: 0.0003598475595936179, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:54.238305, step: 1480, loss: 0.0002860550011973828, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:54.441315, step: 1481, loss: 0.00040688770241104066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:54.654224, step: 1482, loss: 0.0007458872278220952, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:54.857123, step: 1483, loss: 0.0002741391654126346, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:55.074702, step: 1484, loss: 0.00028679933166131377, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:55.286674, step: 1485, loss: 0.0005849964218214154, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:55.494730, step: 1486, loss: 0.0004919373895972967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:55.707100, step: 1487, loss: 0.00047390800318680704, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:55.921536, step: 1488, loss: 0.0003588438266888261, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:56.123191, step: 1489, loss: 0.00043319721589796245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:56.325298, step: 1490, loss: 0.00038779122405685484, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:56.524385, step: 1491, loss: 0.0005703715141862631, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:56.733479, step: 1492, loss: 0.0004392241535242647, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:56.948154, step: 1493, loss: 0.00048490517656318843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:57.145351, step: 1494, loss: 0.00046830944484099746, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:57.360476, step: 1495, loss: 0.0004389856767375022, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:57.563036, step: 1496, loss: 0.0006915137637406588, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:57.776493, step: 1497, loss: 0.00032785843359306455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:57.976615, step: 1498, loss: 0.00034600935759954154, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:58.180740, step: 1499, loss: 0.0005468536983244121, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:35:58.373925, step: 1500, loss: 0.0005037094233557582, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T15:36:06.376995, step: 1500, loss: 0.5751704214475094, acc: 0.8690000000000003, auc: 0.9458871794871797, precision: 0.8790179487179484, recall: 0.8598384615384614\n",
      "2019-01-07T15:36:06.575034, step: 1501, loss: 0.000435714318882674, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:06.779485, step: 1502, loss: 0.0003218844358343631, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:06.994456, step: 1503, loss: 0.00041769578820094466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:07.192415, step: 1504, loss: 0.000491653976496309, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:07.394422, step: 1505, loss: 0.00028939484036527574, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:07.606561, step: 1506, loss: 0.0004128539003431797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:07.806851, step: 1507, loss: 0.0006043449975550175, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:08.019532, step: 1508, loss: 0.00021611568809021264, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:08.234356, step: 1509, loss: 0.0003181065258104354, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:08.438836, step: 1510, loss: 0.00030509906355291605, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:08.655408, step: 1511, loss: 0.00037812639493495226, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:08.864215, step: 1512, loss: 0.0003391252248547971, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:09.074363, step: 1513, loss: 0.0004620634426828474, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:09.276857, step: 1514, loss: 0.0003398366388864815, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:09.490196, step: 1515, loss: 0.0003324883582536131, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:09.705768, step: 1516, loss: 0.0003043942851945758, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:09.904010, step: 1517, loss: 0.000632217968814075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:10.109353, step: 1518, loss: 0.00043688315781764686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:10.313153, step: 1519, loss: 0.0005278582684695721, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:10.527454, step: 1520, loss: 0.000356194592313841, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:10.745432, step: 1521, loss: 0.00046411697985604405, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:10.955473, step: 1522, loss: 0.00046331685734912753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:11.167516, step: 1523, loss: 0.00040768037433736026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:11.375626, step: 1524, loss: 0.0003500186139717698, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:11.583671, step: 1525, loss: 0.0003359277034178376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:11.805004, step: 1526, loss: 0.0005635296693071723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:12.008394, step: 1527, loss: 0.00046117446618154645, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:12.212356, step: 1528, loss: 0.0004087314009666443, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:12.426042, step: 1529, loss: 0.0003630738938227296, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:12.643053, step: 1530, loss: 0.000347725028404966, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:12.846100, step: 1531, loss: 0.0006417530821636319, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:13.069870, step: 1532, loss: 0.0004114698385819793, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:13.274474, step: 1533, loss: 0.0004996078787371516, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:13.490661, step: 1534, loss: 0.0004156925715506077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:13.717288, step: 1535, loss: 0.0004534393665380776, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:13.920441, step: 1536, loss: 0.0005512055358849466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:14.135707, step: 1537, loss: 0.0004558729415293783, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T15:36:14.349172, step: 1538, loss: 0.00025316531537100673, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:14.561757, step: 1539, loss: 0.0002845361887011677, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:14.783980, step: 1540, loss: 0.0003060493036173284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:14.991985, step: 1541, loss: 0.0006339049432426691, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:15.176406, step: 1542, loss: 0.0004735615220852196, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:15.373553, step: 1543, loss: 0.0004441399360075593, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:15.577360, step: 1544, loss: 0.00024531918461434543, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:15.798863, step: 1545, loss: 0.00047307321801781654, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:15.998400, step: 1546, loss: 0.000403373793233186, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:16.197814, step: 1547, loss: 0.00023960645194165409, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:16.422344, step: 1548, loss: 0.0005556383403018117, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:16.623497, step: 1549, loss: 0.0005363973323255777, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:16.836274, step: 1550, loss: 0.00042587509960867465, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:17.049063, step: 1551, loss: 0.00043875936535187066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:17.275362, step: 1552, loss: 0.0004583408299367875, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:17.479752, step: 1553, loss: 0.0004195276414975524, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:17.696254, step: 1554, loss: 0.0003777046804316342, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:17.899464, step: 1555, loss: 0.00044506031554192305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:18.116155, step: 1556, loss: 0.0004209518665447831, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:18.329088, step: 1557, loss: 0.0003491752431727946, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:18.535138, step: 1558, loss: 0.0005041962722316384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:18.740948, step: 1559, loss: 0.00032076166826300323, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-07T15:36:18.953249, step: 1560, loss: 0.00022569256543647498, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "embeddedPosition = positionEmbedding(config.batchSize, config.sequenceLength)\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        transformer = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(transformer.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", transformer.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Transformer/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: config.model.dropoutKeepProb,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              transformer.inputX: batchX,\n",
    "              transformer.inputY: batchY,\n",
    "              transformer.dropoutKeepProb: 1.0,\n",
    "              transformer.embeddedPosition: embeddedPosition\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, transformer.loss, transformer.predictions, transformer.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Transformer/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(transformer.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(transformer.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(transformer.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
