{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = 128  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 180  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 180)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 180)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型构建\n",
    "\n",
    "class Transformer(object):\n",
    "    \"\"\"\n",
    "    Transformer Encoder 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        ma = self.multiheadAttention(queries=self.embeddedWords, keys=self.embeddedWords)\n",
    "        outputs = self.feedForward(ma, [config.model.hiddenSizes, config.model.embeddingSize])\n",
    "        outputs = tf.reshape(outputs, [-1, config.sequenceLength * config.model.embeddingSize])\n",
    "        \n",
    "        outputSize = outputs.get_shape()[-1].value\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(outputs, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "            \n",
    "    def layerNormalization(self, inputs, epsilon=1e-8, scope=\"ln\", reuse=None):\n",
    "        # 本质上可以看作是BN层\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            inputsShape = inputs.get_shape() # [batch_size, time_step, hidden_size]\n",
    "            print(inputsShape)\n",
    "            paramsShape = inputsShape[-1:]\n",
    "            print(paramsShape)\n",
    "            # 在最后的维度上计算输入的数据的均值和方差\n",
    "            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "            beta = tf.Variable(tf.zeros(paramsShape))\n",
    "            gamma = tf.Variable(tf.ones(paramsShape))\n",
    "            normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n",
    "            outputs = gamma * normalized + beta\n",
    "\n",
    "        return outputs\n",
    "            \n",
    "    def multiheadAttention(self, queries, keys, numUnits=None, numHeads=8, causality=False, scope=\"multihead_attention\", reuse=None):\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            if numUnits is None:  # 若是没传入值，直接去输入数据的最后一维，即embedding size.\n",
    "                numUnits = queries.get_shape().as_list()[-1]\n",
    "\n",
    "            # tf.layers.dense可以做多维tensor数据的非线性映射，在计算self-Attention时，一定要对这三个值进行非线性映射，\n",
    "            # 其实这一步就是论文中Multi-Head Attention中的对分割后的数据进行权重映射的步骤，我们在这里先映射后分割，原则上是一样的。\n",
    "            # Q, K, V的维度都是[batch_size, time_step, hidden_size]\n",
    "            Q = tf.layers.dense(queries, numUnits, activation=tf.nn.relu)\n",
    "            K = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "            V = tf.layers.dense(keys, numUnits, activation=tf.nn.relu)\n",
    "            \n",
    "            # 将数据按最后一维分割成num_heads个, 然后按照第一维拼接\n",
    "            # Q, K, V 的维度都是[batch_size * num_heads, time_step, hidden_size/num_heads]\n",
    "            Q_ = tf.concat(tf.split(Q, numHeads, axis=-1), axis=0) \n",
    "            K_ = tf.concat(tf.split(K, numHeads, axis=-1), axis=0) \n",
    "            V_ = tf.concat(tf.split(V, numHeads, axis=-1), axis=0)\n",
    "            \n",
    "            # 计算keys和queries之间的点积，维度[batch_size * num_heads, queries_len, key_len], 后两维是queries和keys的序列长度\n",
    "            similary = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))\n",
    "            \n",
    "            # 对计算的点积进行缩放处理，除以向量长度的根号值\n",
    "            scaledSimilary = similary / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "            \n",
    "            # 在我们输入的序列中会存在padding这个样的填充词，这种词应该对最终的结果是毫无帮助的，原则上说当padding都是输入0时，\n",
    "            # 计算出来的权重应该也是0，但是在transformer中引入了位置向量，当和位置向量相加之后，其值就不为0了，因此在添加位置向量\n",
    "            # 之前，我们需要将其mask为0。虽然在queries中也存在这样的填充词，但原则上模型的结果之和输入有关，而且在self-Attention中\n",
    "            # queryies = keys，因此只要一方为0，计算出的权重就为0。\n",
    "            # 具体关于key mask的介绍可以看看这里： https://github.com/Kyubyong/transformer/issues/3\n",
    "            \n",
    "            # 将每一时序上的向量中的值相加取平均值\n",
    "            keyMasks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # 维度[batch_size, time_step]\n",
    "            \n",
    "            # 利用tf，tile进行张量扩张， 维度[batch_size * num_heads, keys_len] keys_len = keys 的序列长度\n",
    "            keyMasks = tf.tile(keyMasks, [numHeads, 1]) \n",
    "            \n",
    "            # 增加一个维度，并进行扩张，得到维度[batch_size * num_heads, queries_len, keys_len]\n",
    "            keyMasks = tf.tile(tf.expand_dims(keyMasks, 1), [1, tf.shape(queries)[1], 1])\n",
    "            \n",
    "            # tf.ones_like生成元素全为1，维度和scaledSimilary相同的tensor, 然后得到负无穷大的值\n",
    "            paddings = tf.ones_like(scaledSimilary) * (-2 ** (32 + 1))\n",
    "            \n",
    "            # tf.where(condition, x, y),condition中的元素为bool值，其中对应的True用x中的元素替换，对应的False用y中的元素替换\n",
    "            # 因此condition,x,y的维度是一样的。下面就是keyMasks中的值为0就用paddings中的值替换\n",
    "            maskedSimilary = tf.where(tf.equal(keyMasks, 0), paddings, scaledSimilary) # 维度[batch_size * num_heads, queries_len, key_len]\n",
    "            \n",
    "            # 在计算当前的词时，只考虑上文，不考虑下文，出现在Transformer Decoder中。在文本分类时，可以只用Transformer Encoder。\n",
    "            # Decoder是生成模型，主要用在语言生成中\n",
    "            if causality:\n",
    "                diagVals = tf.ones_like(maskedSimilary[0, :, :])  # [queries_len, keys_len]\n",
    "                tril = tf.contrib.linalg.LinearOperatorTriL(diagVals).to_dense()  # [queries_len, keys_len]\n",
    "                masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(maskedSimilary)[0], 1, 1])  # [batch_size * num_heads, queries_len, keys_len]\n",
    "\n",
    "                paddings = tf.ones_like(masks) * (-2 ** (32 + 1))\n",
    "                maskedSimilary = tf.where(tf.equal(masks, 0), paddings, maskedSimilary)  # [batch_size * num_heads, queries_len, keys_len]\n",
    "\n",
    "            # 通过softmax计算权重系数，维度 [batch_size * num_heads, queries_len, keys_len]\n",
    "            weights = tf.nn.softmax(maskedSimilary)\n",
    "\n",
    "            # 加权和得到输出值, 维度[batch_size * num_heads, time_step, hidden_size/num_heads]\n",
    "            outputs = tf.matmul(weights, V_)\n",
    "            print(outputs)\n",
    "            \n",
    "            # 将多头Attention计算的得到的输出重组成最初的维度[batch_size, time_step, hidden_size]\n",
    "            outputs = tf.concat(tf.split(outputs, numHeads, axis=0), axis=2)\n",
    "\n",
    "            # 建立残差连接\n",
    "            outputs += queries\n",
    "            print(outputs)\n",
    "            # normalization 层\n",
    "            outputs = self.layerNormalization(outputs)\n",
    "            return outputs\n",
    "\n",
    "    def feedForward(self, inputs, numUnits=[2048, 512], scope=\"multiheadAttention\", reuse=None):\n",
    "        # 在这里的前向传播采用卷积神经网络\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # 内层\n",
    "            params = {\"inputs\": inputs, \"filters\": numUnits[0], \"kernel_size\": 1,\n",
    "                      \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "            # 外层\n",
    "            params = {\"inputs\": outputs, \"filters\": numUnits[1], \"kernel_size\": 1,\n",
    "                      \"activation\": None, \"use_bias\": True}\n",
    "            \n",
    "            # 这里用到了一维卷积，实际上卷积核尺寸还是二维的，只是只需要指定高度，宽度和embedding size的尺寸一致\n",
    "            outputs = tf.layers.conv1d(**params)\n",
    "            \n",
    "            # 残差连接\n",
    "            outputs += inputs\n",
    "\n",
    "            # 归一化处理\n",
    "            outputs = self.layerNormalization(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"multihead_attention/MatMul_1:0\", shape=(?, 180, 25), dtype=float32)\n",
      "Tensor(\"multihead_attention/add:0\", shape=(?, 180, 200), dtype=float32)\n",
      "(?, 180, 200)\n",
      "(200,)\n",
      "(?, 180, 200)\n",
      "(200,)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense/kernel:0/grad/hist is illegal; using multihead_attention/dense/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense/kernel:0/grad/sparsity is illegal; using multihead_attention/dense/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense/bias:0/grad/hist is illegal; using multihead_attention/dense/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense/bias:0/grad/sparsity is illegal; using multihead_attention/dense/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_1/kernel:0/grad/hist is illegal; using multihead_attention/dense_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_1/kernel:0/grad/sparsity is illegal; using multihead_attention/dense_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_1/bias:0/grad/hist is illegal; using multihead_attention/dense_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_1/bias:0/grad/sparsity is illegal; using multihead_attention/dense_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_2/kernel:0/grad/hist is illegal; using multihead_attention/dense_2/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_2/kernel:0/grad/sparsity is illegal; using multihead_attention/dense_2/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_2/bias:0/grad/hist is illegal; using multihead_attention/dense_2/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/dense_2/bias:0/grad/sparsity is illegal; using multihead_attention/dense_2/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/ln/Variable:0/grad/hist is illegal; using multihead_attention/ln/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/ln/Variable:0/grad/sparsity is illegal; using multihead_attention/ln/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/ln/Variable_1:0/grad/hist is illegal; using multihead_attention/ln/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multihead_attention/ln/Variable_1:0/grad/sparsity is illegal; using multihead_attention/ln/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/kernel:0/grad/hist is illegal; using multiheadAttention/conv1d/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/kernel:0/grad/sparsity is illegal; using multiheadAttention/conv1d/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/bias:0/grad/hist is illegal; using multiheadAttention/conv1d/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d/bias:0/grad/sparsity is illegal; using multiheadAttention/conv1d/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/kernel:0/grad/hist is illegal; using multiheadAttention/conv1d_1/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/kernel:0/grad/sparsity is illegal; using multiheadAttention/conv1d_1/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/bias:0/grad/hist is illegal; using multiheadAttention/conv1d_1/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/conv1d_1/bias:0/grad/sparsity is illegal; using multiheadAttention/conv1d_1/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/ln/Variable:0/grad/hist is illegal; using multiheadAttention/ln/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/ln/Variable:0/grad/sparsity is illegal; using multiheadAttention/ln/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/ln/Variable_1:0/grad/hist is illegal; using multiheadAttention/ln/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name multiheadAttention/ln/Variable_1:0/grad/sparsity is illegal; using multiheadAttention/ln/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Transformer/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-02T18:17:49.293643, step: 1, loss: 1.1020069122314453, acc: 0.4688, auc: 0.5431, precision: 0.4634, recall: 0.9661\n",
      "2019-01-02T18:17:49.481424, step: 2, loss: 10.797325134277344, acc: 0.5078, auc: 0.4325, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:49.649565, step: 3, loss: 7.114317893981934, acc: 0.5, auc: 0.532, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:49.817091, step: 4, loss: 0.9412442445755005, acc: 0.5156, auc: 0.4998, precision: 0.5357, recall: 0.2344\n",
      "2019-01-02T18:17:49.987202, step: 5, loss: 6.6749162673950195, acc: 0.4141, auc: 0.3565, precision: 0.4141, recall: 1.0\n",
      "2019-01-02T18:17:50.138089, step: 6, loss: 4.32211971282959, acc: 0.5547, auc: 0.5908, precision: 0.5547, recall: 1.0\n",
      "2019-01-02T18:17:50.296414, step: 7, loss: 1.8672963380813599, acc: 0.5156, auc: 0.5459, precision: 0.5156, recall: 1.0\n",
      "2019-01-02T18:17:50.466338, step: 8, loss: 2.211008310317993, acc: 0.4609, auc: 0.5446, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:50.645180, step: 9, loss: 3.183028221130371, acc: 0.5078, auc: 0.5228, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:50.806010, step: 10, loss: 2.568859100341797, acc: 0.4922, auc: 0.4359, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:50.977264, step: 11, loss: 1.0585259199142456, acc: 0.4766, auc: 0.4955, precision: 0.55, recall: 0.1594\n",
      "2019-01-02T18:17:51.144928, step: 12, loss: 1.9984432458877563, acc: 0.4531, auc: 0.6012, precision: 0.4444, recall: 1.0\n",
      "2019-01-02T18:17:51.312597, step: 13, loss: 2.4911749362945557, acc: 0.4922, auc: 0.5429, precision: 0.4922, recall: 1.0\n",
      "2019-01-02T18:17:51.484609, step: 14, loss: 1.6168510913848877, acc: 0.5859, auc: 0.6379, precision: 0.5806, recall: 0.9863\n",
      "2019-01-02T18:17:51.658335, step: 15, loss: 1.014129400253296, acc: 0.5547, auc: 0.6005, precision: 0.5426, recall: 0.7846\n",
      "2019-01-02T18:17:51.825989, step: 16, loss: 0.9951158761978149, acc: 0.5469, auc: 0.5848, precision: 0.7222, recall: 0.197\n",
      "2019-01-02T18:17:51.999462, step: 17, loss: 1.71140456199646, acc: 0.4766, auc: 0.5564, precision: 0.0, recall: 0.0\n",
      "2019-01-02T18:17:52.175737, step: 18, loss: 1.5681755542755127, acc: 0.5, auc: 0.4905, precision: 0.25, recall: 0.0161\n",
      "2019-01-02T18:17:52.348554, step: 19, loss: 1.2682549953460693, acc: 0.5078, auc: 0.4531, precision: 0.6364, recall: 0.1061\n",
      "2019-01-02T18:17:52.523598, step: 20, loss: 0.984054684638977, acc: 0.5156, auc: 0.5483, precision: 0.4355, recall: 0.5\n",
      "2019-01-02T18:17:52.687441, step: 21, loss: 1.1490583419799805, acc: 0.4688, auc: 0.511, precision: 0.5, recall: 0.6029\n",
      "2019-01-02T18:17:52.855294, step: 22, loss: 1.1739741563796997, acc: 0.5859, auc: 0.6245, precision: 0.5743, recall: 0.8529\n",
      "2019-01-02T18:17:53.027818, step: 23, loss: 1.1313353776931763, acc: 0.5547, auc: 0.5625, precision: 0.5204, recall: 0.8361\n",
      "2019-01-02T18:17:53.197430, step: 24, loss: 0.9065238237380981, acc: 0.5547, auc: 0.5376, precision: 0.5538, recall: 0.5625\n",
      "2019-01-02T18:17:53.374851, step: 25, loss: 0.8097536563873291, acc: 0.5547, auc: 0.6184, precision: 0.5484, recall: 0.2833\n",
      "2019-01-02T18:17:53.556281, step: 26, loss: 1.2005016803741455, acc: 0.5625, auc: 0.5513, precision: 0.4286, recall: 0.1111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:17:53.727658, step: 27, loss: 1.3452759981155396, acc: 0.5078, auc: 0.5133, precision: 0.5833, recall: 0.1077\n",
      "2019-01-02T18:17:53.898459, step: 28, loss: 1.1268305778503418, acc: 0.4922, auc: 0.5565, precision: 0.5294, recall: 0.1364\n",
      "2019-01-02T18:17:54.070868, step: 29, loss: 0.8720773458480835, acc: 0.5469, auc: 0.5438, precision: 0.5128, recall: 0.339\n",
      "2019-01-02T18:17:54.237686, step: 30, loss: 0.9207226037979126, acc: 0.5312, auc: 0.6112, precision: 0.4944, recall: 0.7458\n",
      "2019-01-02T18:17:54.403764, step: 31, loss: 1.0523834228515625, acc: 0.5547, auc: 0.581, precision: 0.5408, recall: 0.8154\n",
      "2019-01-02T18:17:54.569911, step: 32, loss: 0.8471137881278992, acc: 0.5781, auc: 0.6303, precision: 0.593, recall: 0.7286\n",
      "2019-01-02T18:17:54.751571, step: 33, loss: 0.9071395397186279, acc: 0.5547, auc: 0.5474, precision: 0.6176, recall: 0.5753\n",
      "2019-01-02T18:17:54.924164, step: 34, loss: 0.7343171238899231, acc: 0.6328, auc: 0.6574, precision: 0.6222, recall: 0.4828\n",
      "2019-01-02T18:17:55.094331, step: 35, loss: 0.9271891713142395, acc: 0.4453, auc: 0.723, precision: 0.8667, recall: 0.1585\n",
      "2019-01-02T18:17:55.262217, step: 36, loss: 0.9833047389984131, acc: 0.4922, auc: 0.5409, precision: 0.5789, recall: 0.1618\n",
      "2019-01-02T18:17:55.430786, step: 37, loss: 0.8453466892242432, acc: 0.5391, auc: 0.5449, precision: 0.6098, recall: 0.3676\n",
      "2019-01-02T18:17:55.596577, step: 38, loss: 0.9016382098197937, acc: 0.5547, auc: 0.541, precision: 0.5263, recall: 0.6557\n",
      "2019-01-02T18:17:55.760860, step: 39, loss: 0.8567665815353394, acc: 0.5469, auc: 0.6089, precision: 0.5067, recall: 0.6441\n",
      "2019-01-02T18:17:55.929730, step: 40, loss: 0.6916763782501221, acc: 0.6719, auc: 0.6865, precision: 0.6625, recall: 0.7794\n",
      "2019-01-02T18:17:56.097553, step: 41, loss: 0.6579563021659851, acc: 0.6016, auc: 0.6867, precision: 0.7381, recall: 0.4366\n",
      "2019-01-02T18:17:56.273212, step: 42, loss: 0.6745010614395142, acc: 0.5703, auc: 0.6959, precision: 0.7429, recall: 0.3611\n",
      "2019-01-02T18:17:56.444042, step: 43, loss: 0.6765422821044922, acc: 0.6172, auc: 0.6573, precision: 0.6053, recall: 0.4035\n",
      "2019-01-02T18:17:56.615126, step: 44, loss: 0.7215762734413147, acc: 0.6016, auc: 0.6092, precision: 0.6176, recall: 0.3559\n",
      "2019-01-02T18:17:56.782805, step: 45, loss: 0.722174346446991, acc: 0.5625, auc: 0.6315, precision: 0.6585, recall: 0.3913\n",
      "2019-01-02T18:17:56.958525, step: 46, loss: 0.6022288799285889, acc: 0.625, auc: 0.7436, precision: 0.7647, recall: 0.3939\n",
      "2019-01-02T18:17:57.135458, step: 47, loss: 0.681404709815979, acc: 0.6094, auc: 0.6676, precision: 0.6607, recall: 0.5441\n",
      "2019-01-02T18:17:57.307672, step: 48, loss: 0.7253731489181519, acc: 0.6484, auc: 0.7119, precision: 0.5897, recall: 0.7797\n",
      "2019-01-02T18:17:57.479879, step: 49, loss: 0.6751958131790161, acc: 0.6719, auc: 0.685, precision: 0.75, recall: 0.6\n",
      "2019-01-02T18:17:57.658479, step: 50, loss: 0.5812379121780396, acc: 0.6875, auc: 0.7741, precision: 0.8378, recall: 0.4769\n",
      "2019-01-02T18:17:57.830653, step: 51, loss: 0.7286766767501831, acc: 0.625, auc: 0.6579, precision: 0.7273, recall: 0.381\n",
      "2019-01-02T18:17:57.999597, step: 52, loss: 0.703525185585022, acc: 0.5703, auc: 0.6401, precision: 0.6829, recall: 0.4\n",
      "2019-01-02T18:17:58.172542, step: 53, loss: 0.5931313037872314, acc: 0.6562, auc: 0.7454, precision: 0.7083, recall: 0.5312\n",
      "2019-01-02T18:17:58.336612, step: 54, loss: 0.6373893022537231, acc: 0.6562, auc: 0.7293, precision: 0.6271, recall: 0.6271\n",
      "2019-01-02T18:17:58.511058, step: 55, loss: 0.6588295102119446, acc: 0.6328, auc: 0.6805, precision: 0.6591, recall: 0.4754\n",
      "2019-01-02T18:17:58.678213, step: 56, loss: 0.6079524755477905, acc: 0.6719, auc: 0.7444, precision: 0.8158, recall: 0.4697\n",
      "2019-01-02T18:17:58.849656, step: 57, loss: 0.6524233818054199, acc: 0.6094, auc: 0.6931, precision: 0.7561, recall: 0.4366\n",
      "2019-01-02T18:17:59.018059, step: 58, loss: 0.5891465544700623, acc: 0.7266, auc: 0.7768, precision: 0.7692, recall: 0.5357\n",
      "2019-01-02T18:17:59.184312, step: 59, loss: 0.6282508969306946, acc: 0.6875, auc: 0.7427, precision: 0.6271, recall: 0.6727\n",
      "2019-01-02T18:17:59.357339, step: 60, loss: 0.5261184573173523, acc: 0.6641, auc: 0.8529, precision: 0.8788, recall: 0.4265\n",
      "2019-01-02T18:17:59.525897, step: 61, loss: 0.6158952713012695, acc: 0.6406, auc: 0.7676, precision: 0.8387, recall: 0.3881\n",
      "2019-01-02T18:17:59.697572, step: 62, loss: 0.5702251195907593, acc: 0.7344, auc: 0.7927, precision: 0.7273, recall: 0.678\n",
      "2019-01-02T18:17:59.868042, step: 63, loss: 0.6175707578659058, acc: 0.6797, auc: 0.717, precision: 0.7377, recall: 0.6429\n",
      "2019-01-02T18:18:00.036904, step: 64, loss: 0.5654444694519043, acc: 0.7031, auc: 0.7886, precision: 0.7258, recall: 0.6818\n",
      "2019-01-02T18:18:00.211762, step: 65, loss: 0.5232938528060913, acc: 0.7734, auc: 0.8295, precision: 0.8077, recall: 0.6885\n",
      "2019-01-02T18:18:00.379772, step: 66, loss: 0.6208575963973999, acc: 0.6797, auc: 0.7824, precision: 0.8462, recall: 0.4853\n",
      "2019-01-02T18:18:00.535034, step: 67, loss: 0.5011467933654785, acc: 0.6875, auc: 0.8213, precision: 0.6667, recall: 0.5965\n",
      "2019-01-02T18:18:00.708028, step: 68, loss: 0.6912473440170288, acc: 0.6562, auc: 0.7304, precision: 0.7561, recall: 0.4769\n",
      "2019-01-02T18:18:00.880113, step: 69, loss: 0.5112208127975464, acc: 0.7266, auc: 0.8265, precision: 0.7667, recall: 0.6866\n",
      "2019-01-02T18:18:01.055588, step: 70, loss: 0.5292322039604187, acc: 0.7031, auc: 0.8325, precision: 0.6909, recall: 0.6441\n",
      "2019-01-02T18:18:01.208669, step: 71, loss: 0.5785961151123047, acc: 0.6953, auc: 0.8017, precision: 0.8182, recall: 0.45\n",
      "2019-01-02T18:18:01.375384, step: 72, loss: 0.4887620806694031, acc: 0.7188, auc: 0.8541, precision: 0.8409, recall: 0.5606\n",
      "2019-01-02T18:18:01.543843, step: 73, loss: 0.425134539604187, acc: 0.8203, auc: 0.8814, precision: 0.8814, recall: 0.7647\n",
      "2019-01-02T18:18:01.714249, step: 74, loss: 0.595664918422699, acc: 0.7266, auc: 0.8311, precision: 0.6712, recall: 0.8167\n",
      "2019-01-02T18:18:01.895493, step: 75, loss: 0.4974524676799774, acc: 0.8047, auc: 0.8525, precision: 0.8333, recall: 0.7692\n",
      "2019-01-02T18:18:02.062629, step: 76, loss: 0.6151461601257324, acc: 0.6562, auc: 0.8266, precision: 0.9024, recall: 0.4805\n",
      "2019-01-02T18:18:02.239671, step: 77, loss: 0.40489718317985535, acc: 0.7969, auc: 0.9019, precision: 0.9, recall: 0.7297\n",
      "2019-01-02T18:18:02.412085, step: 78, loss: 0.46051687002182007, acc: 0.8438, auc: 0.9179, precision: 0.7778, recall: 0.9333\n",
      "2019-01-02T18:18:02.582073, step: 79, loss: 0.4901474714279175, acc: 0.8281, auc: 0.893, precision: 0.8143, recall: 0.8636\n",
      "2019-01-02T18:18:02.749810, step: 80, loss: 0.44723495841026306, acc: 0.75, auc: 0.9097, precision: 0.9268, recall: 0.5672\n",
      "2019-01-02T18:18:02.908999, step: 81, loss: 0.53727787733078, acc: 0.7344, auc: 0.9051, precision: 0.9355, recall: 0.4754\n",
      "2019-01-02T18:18:03.087791, step: 82, loss: 0.5021547675132751, acc: 0.7734, auc: 0.8603, precision: 0.8095, recall: 0.75\n",
      "2019-01-02T18:18:03.265701, step: 83, loss: 0.7082918882369995, acc: 0.7266, auc: 0.8033, precision: 0.7093, recall: 0.8592\n",
      "2019-01-02T18:18:03.442193, step: 84, loss: 0.43513593077659607, acc: 0.8281, auc: 0.8973, precision: 0.8033, recall: 0.8305\n",
      "2019-01-02T18:18:03.608902, step: 85, loss: 0.6495146751403809, acc: 0.7188, auc: 0.8335, precision: 0.8684, recall: 0.5156\n",
      "2019-01-02T18:18:03.783856, step: 86, loss: 0.44511035084724426, acc: 0.7891, auc: 0.9062, precision: 0.8913, recall: 0.6508\n",
      "2019-01-02T18:18:03.956745, step: 87, loss: 0.6309042572975159, acc: 0.7344, auc: 0.8059, precision: 0.7627, recall: 0.6923\n",
      "2019-01-02T18:18:04.120577, step: 88, loss: 0.5522077083587646, acc: 0.7734, auc: 0.887, precision: 0.7083, recall: 0.8644\n",
      "2019-01-02T18:18:04.289061, step: 89, loss: 0.5136115550994873, acc: 0.7812, auc: 0.864, precision: 0.8088, recall: 0.7857\n",
      "2019-01-02T18:18:04.458309, step: 90, loss: 0.5927801132202148, acc: 0.7734, auc: 0.8493, precision: 0.7963, recall: 0.7049\n",
      "2019-01-02T18:18:04.635123, step: 91, loss: 0.38756388425827026, acc: 0.7734, auc: 0.9184, precision: 0.8696, recall: 0.6349\n",
      "2019-01-02T18:18:04.808520, step: 92, loss: 0.45279327034950256, acc: 0.7891, auc: 0.8931, precision: 0.8605, recall: 0.6379\n",
      "2019-01-02T18:18:04.978714, step: 93, loss: 0.6033801436424255, acc: 0.7188, auc: 0.8193, precision: 0.7188, recall: 0.7188\n",
      "2019-01-02T18:18:05.150175, step: 94, loss: 0.47982048988342285, acc: 0.8281, auc: 0.8864, precision: 0.8289, recall: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:18:05.303241, step: 95, loss: 0.550586998462677, acc: 0.7344, auc: 0.8386, precision: 0.75, recall: 0.7031\n",
      "2019-01-02T18:18:05.473283, step: 96, loss: 0.4618009924888611, acc: 0.8047, auc: 0.8718, precision: 0.8148, recall: 0.7458\n",
      "2019-01-02T18:18:05.647163, step: 97, loss: 0.25469762086868286, acc: 0.8828, auc: 0.9643, precision: 0.9259, recall: 0.8197\n",
      "2019-01-02T18:18:05.812290, step: 98, loss: 0.5060341954231262, acc: 0.7578, auc: 0.8921, precision: 0.8864, recall: 0.6\n",
      "2019-01-02T18:18:05.965936, step: 99, loss: 0.4369860589504242, acc: 0.7969, auc: 0.8909, precision: 0.8723, recall: 0.6721\n",
      "2019-01-02T18:18:06.134627, step: 100, loss: 0.5808737277984619, acc: 0.7734, auc: 0.8632, precision: 0.725, recall: 0.8923\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T18:18:12.837281, step: 100, loss: 0.4345000607845111, acc: 0.8233153846153843, auc: 0.8978384615384615, precision: 0.8142820512820511, recall: 0.8437974358974358\n",
      "2019-01-02T18:18:13.002278, step: 101, loss: 0.4922887086868286, acc: 0.8125, auc: 0.8823, precision: 0.7692, recall: 0.8475\n",
      "2019-01-02T18:18:13.171437, step: 102, loss: 0.45323696732521057, acc: 0.7969, auc: 0.9002, precision: 0.9038, recall: 0.6912\n",
      "2019-01-02T18:18:13.344987, step: 103, loss: 0.5502617359161377, acc: 0.7422, auc: 0.8784, precision: 0.907, recall: 0.5735\n",
      "2019-01-02T18:18:13.519217, step: 104, loss: 0.4449431300163269, acc: 0.8047, auc: 0.8722, precision: 0.814, recall: 0.6731\n",
      "2019-01-02T18:18:13.696753, step: 105, loss: 0.48987719416618347, acc: 0.8047, auc: 0.8992, precision: 0.7581, recall: 0.8246\n",
      "2019-01-02T18:18:13.873320, step: 106, loss: 0.4877365827560425, acc: 0.8125, auc: 0.886, precision: 0.8235, recall: 0.8235\n",
      "2019-01-02T18:18:14.053657, step: 107, loss: 0.419136643409729, acc: 0.7891, auc: 0.8988, precision: 0.8182, recall: 0.7258\n",
      "2019-01-02T18:18:14.218773, step: 108, loss: 0.47683513164520264, acc: 0.8125, auc: 0.875, precision: 0.8776, recall: 0.7049\n",
      "2019-01-02T18:18:14.391300, step: 109, loss: 0.5877200961112976, acc: 0.7109, auc: 0.8879, precision: 0.9062, recall: 0.4603\n",
      "2019-01-02T18:18:14.564582, step: 110, loss: 0.4190952777862549, acc: 0.8281, auc: 0.8953, precision: 0.8594, recall: 0.8088\n",
      "2019-01-02T18:18:14.743891, step: 111, loss: 0.4711148738861084, acc: 0.7969, auc: 0.8993, precision: 0.7703, recall: 0.8636\n",
      "2019-01-02T18:18:14.919530, step: 112, loss: 0.5173683166503906, acc: 0.8203, auc: 0.8833, precision: 0.7733, recall: 0.9062\n",
      "2019-01-02T18:18:15.096318, step: 113, loss: 0.3643961548805237, acc: 0.8281, auc: 0.9199, precision: 0.9298, recall: 0.7465\n",
      "2019-01-02T18:18:15.266689, step: 114, loss: 0.39704930782318115, acc: 0.7812, auc: 0.9308, precision: 0.975, recall: 0.5909\n",
      "2019-01-02T18:18:15.436191, step: 115, loss: 0.4089106619358063, acc: 0.7891, auc: 0.8988, precision: 0.8281, recall: 0.7681\n",
      "2019-01-02T18:18:15.608779, step: 116, loss: 0.43496444821357727, acc: 0.7734, auc: 0.8932, precision: 0.7463, recall: 0.8065\n",
      "2019-01-02T18:18:15.779705, step: 117, loss: 0.42892199754714966, acc: 0.8125, auc: 0.9009, precision: 0.8095, recall: 0.8095\n",
      "2019-01-02T18:18:15.947655, step: 118, loss: 0.34378278255462646, acc: 0.8594, auc: 0.9317, precision: 0.875, recall: 0.7778\n",
      "2019-01-02T18:18:16.128569, step: 119, loss: 0.4509318470954895, acc: 0.7812, auc: 0.9097, precision: 0.9231, recall: 0.5902\n",
      "2019-01-02T18:18:16.300951, step: 120, loss: 0.40882551670074463, acc: 0.8047, auc: 0.9099, precision: 0.8667, recall: 0.6724\n",
      "2019-01-02T18:18:16.473976, step: 121, loss: 0.411501407623291, acc: 0.8203, auc: 0.9103, precision: 0.7778, recall: 0.8448\n",
      "2019-01-02T18:18:16.646491, step: 122, loss: 0.36315450072288513, acc: 0.8516, auc: 0.9294, precision: 0.8525, recall: 0.8387\n",
      "2019-01-02T18:18:16.819312, step: 123, loss: 0.27073031663894653, acc: 0.8828, auc: 0.9605, precision: 0.8971, recall: 0.8841\n",
      "2019-01-02T18:18:16.992479, step: 124, loss: 0.414409875869751, acc: 0.8281, auc: 0.9071, precision: 0.8235, recall: 0.7636\n",
      "2019-01-02T18:18:17.159214, step: 125, loss: 0.4272083044052124, acc: 0.8047, auc: 0.9133, precision: 0.9535, recall: 0.6406\n",
      "2019-01-02T18:18:17.331813, step: 126, loss: 0.39012646675109863, acc: 0.8359, auc: 0.9208, precision: 0.7887, recall: 0.9032\n",
      "2019-01-02T18:18:17.498397, step: 127, loss: 0.43349888920783997, acc: 0.8281, auc: 0.8935, precision: 0.8571, recall: 0.7742\n",
      "2019-01-02T18:18:17.666605, step: 128, loss: 0.3947702944278717, acc: 0.8047, auc: 0.905, precision: 0.8305, recall: 0.7656\n",
      "2019-01-02T18:18:17.841206, step: 129, loss: 0.40183451771736145, acc: 0.8203, auc: 0.9083, precision: 0.8542, recall: 0.7193\n",
      "2019-01-02T18:18:18.012812, step: 130, loss: 0.3727739155292511, acc: 0.8359, auc: 0.9297, precision: 0.9123, recall: 0.7647\n",
      "2019-01-02T18:18:18.195824, step: 131, loss: 0.46505671739578247, acc: 0.8203, auc: 0.8755, precision: 0.8871, recall: 0.7746\n",
      "2019-01-02T18:18:18.369419, step: 132, loss: 0.35236459970474243, acc: 0.8359, auc: 0.9229, precision: 0.8525, recall: 0.8125\n",
      "2019-01-02T18:18:18.540025, step: 133, loss: 0.3614479899406433, acc: 0.8516, auc: 0.9252, precision: 0.8657, recall: 0.8529\n",
      "2019-01-02T18:18:18.714930, step: 134, loss: 0.3848956823348999, acc: 0.8203, auc: 0.9166, precision: 0.8519, recall: 0.7541\n",
      "2019-01-02T18:18:18.884818, step: 135, loss: 0.3736175000667572, acc: 0.8516, auc: 0.9145, precision: 0.8846, recall: 0.7797\n",
      "2019-01-02T18:18:19.062513, step: 136, loss: 0.3895050585269928, acc: 0.8359, auc: 0.9076, precision: 0.8679, recall: 0.7667\n",
      "2019-01-02T18:18:19.232860, step: 137, loss: 0.3710801601409912, acc: 0.7891, auc: 0.9252, precision: 0.8723, recall: 0.6613\n",
      "2019-01-02T18:18:19.400455, step: 138, loss: 0.2865296006202698, acc: 0.8281, auc: 0.944, precision: 0.8814, recall: 0.7761\n",
      "2019-01-02T18:18:19.566100, step: 139, loss: 0.5226283073425293, acc: 0.7812, auc: 0.8654, precision: 0.7937, recall: 0.7692\n",
      "2019-01-02T18:18:19.733391, step: 140, loss: 0.48978307843208313, acc: 0.7969, auc: 0.8902, precision: 0.7544, recall: 0.7818\n",
      "2019-01-02T18:18:19.897397, step: 141, loss: 0.37032461166381836, acc: 0.8203, auc: 0.9205, precision: 0.8276, recall: 0.7869\n",
      "2019-01-02T18:18:20.052634, step: 142, loss: 0.4089984893798828, acc: 0.8047, auc: 0.9369, precision: 1.0, recall: 0.5902\n",
      "2019-01-02T18:18:20.229206, step: 143, loss: 0.4685446619987488, acc: 0.8125, auc: 0.8828, precision: 0.86, recall: 0.7167\n",
      "2019-01-02T18:18:20.408297, step: 144, loss: 0.28653672337532043, acc: 0.8984, auc: 0.9538, precision: 0.8871, recall: 0.9016\n",
      "2019-01-02T18:18:20.562242, step: 145, loss: 0.3005496859550476, acc: 0.8672, auc: 0.9478, precision: 0.8852, recall: 0.8438\n",
      "2019-01-02T18:18:20.730382, step: 146, loss: 0.4085380434989929, acc: 0.8359, auc: 0.9057, precision: 0.8143, recall: 0.8769\n",
      "2019-01-02T18:18:20.899920, step: 147, loss: 0.4177629053592682, acc: 0.8281, auc: 0.9157, precision: 0.9123, recall: 0.7536\n",
      "2019-01-02T18:18:21.066283, step: 148, loss: 0.3974594473838806, acc: 0.8281, auc: 0.9054, precision: 0.8409, recall: 0.7115\n",
      "2019-01-02T18:18:21.239109, step: 149, loss: 0.30592936277389526, acc: 0.8438, auc: 0.9443, precision: 0.9231, recall: 0.75\n",
      "2019-01-02T18:18:21.403974, step: 150, loss: 0.3288554549217224, acc: 0.8359, auc: 0.9352, precision: 0.8382, recall: 0.8507\n",
      "2019-01-02T18:18:21.563712, step: 151, loss: 0.3824504017829895, acc: 0.8281, auc: 0.9296, precision: 0.8056, recall: 0.8788\n",
      "2019-01-02T18:18:21.731359, step: 152, loss: 0.3323245048522949, acc: 0.8594, auc: 0.9339, precision: 0.8909, recall: 0.8033\n",
      "2019-01-02T18:18:21.905586, step: 153, loss: 0.5488766431808472, acc: 0.7812, auc: 0.8804, precision: 0.8718, recall: 0.5965\n",
      "2019-01-02T18:18:22.074914, step: 154, loss: 0.26087096333503723, acc: 0.8516, auc: 0.9553, precision: 0.8906, recall: 0.8261\n",
      "2019-01-02T18:18:22.251853, step: 155, loss: 0.3487125635147095, acc: 0.8672, auc: 0.9521, precision: 0.8514, recall: 0.913\n",
      "2019-01-02T18:18:22.421664, step: 156, loss: 0.29432350397109985, acc: 0.8906, auc: 0.9473, precision: 0.9107, recall: 0.85\n",
      "start training model\n",
      "2019-01-02T18:18:22.616646, step: 157, loss: 0.2549252510070801, acc: 0.875, auc: 0.9607, precision: 0.8966, recall: 0.8387\n",
      "2019-01-02T18:18:22.789873, step: 158, loss: 0.23750951886177063, acc: 0.8672, auc: 0.9813, precision: 0.975, recall: 0.7091\n",
      "2019-01-02T18:18:22.970784, step: 159, loss: 0.2955279052257538, acc: 0.8516, auc: 0.9758, precision: 0.9821, recall: 0.7534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:18:23.137792, step: 160, loss: 0.25392234325408936, acc: 0.9062, auc: 0.9601, precision: 0.8961, recall: 0.9452\n",
      "2019-01-02T18:18:23.308920, step: 161, loss: 0.32933589816093445, acc: 0.8672, auc: 0.9836, precision: 0.8, recall: 0.9846\n",
      "2019-01-02T18:18:23.481893, step: 162, loss: 0.2659507095813751, acc: 0.8984, auc: 0.9633, precision: 0.8814, recall: 0.8966\n",
      "2019-01-02T18:18:23.656023, step: 163, loss: 0.18990613520145416, acc: 0.9062, auc: 0.9822, precision: 0.9655, recall: 0.8485\n",
      "2019-01-02T18:18:23.831976, step: 164, loss: 0.31363824009895325, acc: 0.8203, auc: 0.98, precision: 1.0, recall: 0.6515\n",
      "2019-01-02T18:18:24.008460, step: 165, loss: 0.3109046518802643, acc: 0.8672, auc: 0.9482, precision: 0.9412, recall: 0.7742\n",
      "2019-01-02T18:18:24.174742, step: 166, loss: 0.21396152675151825, acc: 0.8906, auc: 0.9717, precision: 0.9189, recall: 0.8947\n",
      "2019-01-02T18:18:24.329520, step: 167, loss: 0.42832863330841064, acc: 0.8672, auc: 0.9629, precision: 0.8, recall: 0.9677\n",
      "2019-01-02T18:18:24.498301, step: 168, loss: 0.3594893217086792, acc: 0.8828, auc: 0.9577, precision: 0.8182, recall: 0.9474\n",
      "2019-01-02T18:18:24.667560, step: 169, loss: 0.162643164396286, acc: 0.9141, auc: 0.9892, precision: 0.9811, recall: 0.8387\n",
      "2019-01-02T18:18:24.837528, step: 170, loss: 0.33146652579307556, acc: 0.8125, auc: 0.965, precision: 0.9792, recall: 0.6714\n",
      "2019-01-02T18:18:25.007211, step: 171, loss: 0.29463639855384827, acc: 0.8906, auc: 0.9781, precision: 1.0, recall: 0.7971\n",
      "2019-01-02T18:18:25.175457, step: 172, loss: 0.23396126925945282, acc: 0.9062, auc: 0.969, precision: 0.8955, recall: 0.9231\n",
      "2019-01-02T18:18:25.344548, step: 173, loss: 0.24187470972537994, acc: 0.8828, auc: 0.9717, precision: 0.8767, recall: 0.9143\n",
      "2019-01-02T18:18:25.511256, step: 174, loss: 0.30883944034576416, acc: 0.9219, auc: 0.9753, precision: 0.8571, recall: 0.9818\n",
      "2019-01-02T18:18:25.681265, step: 175, loss: 0.24489986896514893, acc: 0.9219, auc: 0.9651, precision: 0.8846, recall: 0.92\n",
      "2019-01-02T18:18:25.846520, step: 176, loss: 0.24050849676132202, acc: 0.8984, auc: 0.9693, precision: 0.96, recall: 0.8136\n",
      "2019-01-02T18:18:26.016297, step: 177, loss: 0.31354695558547974, acc: 0.8203, auc: 0.9729, precision: 0.9767, recall: 0.6562\n",
      "2019-01-02T18:18:26.184368, step: 178, loss: 0.3368014693260193, acc: 0.8281, auc: 0.9657, precision: 0.9787, recall: 0.6866\n",
      "2019-01-02T18:18:26.355770, step: 179, loss: 0.16391530632972717, acc: 0.9297, auc: 0.9834, precision: 0.9242, recall: 0.9385\n",
      "2019-01-02T18:18:26.528005, step: 180, loss: 0.31694018840789795, acc: 0.8594, auc: 0.9728, precision: 0.8028, recall: 0.9344\n",
      "2019-01-02T18:18:26.702129, step: 181, loss: 0.29592156410217285, acc: 0.9141, auc: 0.9779, precision: 0.8551, recall: 0.9833\n",
      "2019-01-02T18:18:26.873483, step: 182, loss: 0.1749144345521927, acc: 0.9375, auc: 0.9816, precision: 0.9286, recall: 0.9286\n",
      "2019-01-02T18:18:27.043887, step: 183, loss: 0.2761920094490051, acc: 0.8516, auc: 0.9797, precision: 0.9767, recall: 0.7\n",
      "2019-01-02T18:18:27.209862, step: 184, loss: 0.25677579641342163, acc: 0.8438, auc: 0.9829, precision: 0.9787, recall: 0.7077\n",
      "2019-01-02T18:18:27.384301, step: 185, loss: 0.179820254445076, acc: 0.9062, auc: 0.9802, precision: 0.9474, recall: 0.8571\n",
      "2019-01-02T18:18:27.559505, step: 186, loss: 0.2628956437110901, acc: 0.9219, auc: 0.9739, precision: 0.8971, recall: 0.9531\n",
      "2019-01-02T18:18:27.737715, step: 187, loss: 0.1800372302532196, acc: 0.9297, auc: 0.9798, precision: 0.9855, recall: 0.8947\n",
      "2019-01-02T18:18:27.911892, step: 188, loss: 0.3254985213279724, acc: 0.8906, auc: 0.9731, precision: 0.8312, recall: 0.9846\n",
      "2019-01-02T18:18:28.078877, step: 189, loss: 0.22047990560531616, acc: 0.9141, auc: 0.9787, precision: 0.871, recall: 0.9474\n",
      "2019-01-02T18:18:28.255558, step: 190, loss: 0.26580551266670227, acc: 0.8828, auc: 0.9604, precision: 0.9434, recall: 0.8065\n",
      "2019-01-02T18:18:28.433745, step: 191, loss: 0.2954607307910919, acc: 0.8594, auc: 0.9615, precision: 0.9737, recall: 0.6852\n",
      "2019-01-02T18:18:28.603766, step: 192, loss: 0.20158809423446655, acc: 0.9219, auc: 0.9847, precision: 1.0, recall: 0.8148\n",
      "2019-01-02T18:18:28.774291, step: 193, loss: 0.2214002013206482, acc: 0.8438, auc: 0.9936, precision: 1.0, recall: 0.7059\n",
      "2019-01-02T18:18:28.949106, step: 194, loss: 0.22029225528240204, acc: 0.8984, auc: 0.9753, precision: 0.9079, recall: 0.92\n",
      "2019-01-02T18:18:29.122390, step: 195, loss: 0.3383404612541199, acc: 0.8828, auc: 0.967, precision: 0.8194, recall: 0.9672\n",
      "2019-01-02T18:18:29.293684, step: 196, loss: 0.3924260139465332, acc: 0.8672, auc: 0.9666, precision: 0.7879, recall: 0.9455\n",
      "2019-01-02T18:18:29.467533, step: 197, loss: 0.36007457971572876, acc: 0.8516, auc: 0.9394, precision: 0.9231, recall: 0.8108\n",
      "2019-01-02T18:18:29.644725, step: 198, loss: 0.18981018662452698, acc: 0.9062, auc: 0.9787, precision: 0.931, recall: 0.871\n",
      "2019-01-02T18:18:29.815510, step: 199, loss: 0.23738044500350952, acc: 0.8906, auc: 0.9799, precision: 0.9744, recall: 0.7451\n",
      "2019-01-02T18:18:29.989161, step: 200, loss: 0.3138694763183594, acc: 0.8281, auc: 0.9809, precision: 1.0, recall: 0.6716\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T18:18:36.538090, step: 200, loss: 0.3867329030464857, acc: 0.8341358974358974, auc: 0.9249128205128204, precision: 0.8846692307692308, recall: 0.7732410256410256\n",
      "2019-01-02T18:18:36.712094, step: 201, loss: 0.2466353178024292, acc: 0.9219, auc: 0.9591, precision: 0.9474, recall: 0.8852\n",
      "2019-01-02T18:18:36.889143, step: 202, loss: 0.24294795095920563, acc: 0.9141, auc: 0.9756, precision: 0.8767, recall: 0.9697\n",
      "2019-01-02T18:18:37.064489, step: 203, loss: 0.2810596525669098, acc: 0.9062, auc: 0.9709, precision: 0.8824, recall: 0.9375\n",
      "2019-01-02T18:18:37.248114, step: 204, loss: 0.2988360524177551, acc: 0.9141, auc: 0.967, precision: 0.8696, recall: 0.9677\n",
      "2019-01-02T18:18:37.423395, step: 205, loss: 0.2371874749660492, acc: 0.8984, auc: 0.9652, precision: 0.8727, recall: 0.8889\n",
      "2019-01-02T18:18:37.603029, step: 206, loss: 0.29298821091651917, acc: 0.9062, auc: 0.9695, precision: 0.9818, recall: 0.8308\n",
      "2019-01-02T18:18:37.773097, step: 207, loss: 0.18715453147888184, acc: 0.875, auc: 0.9956, precision: 1.0, recall: 0.7647\n",
      "2019-01-02T18:18:37.950175, step: 208, loss: 0.2066478133201599, acc: 0.9297, auc: 0.9732, precision: 0.9778, recall: 0.8462\n",
      "2019-01-02T18:18:38.120191, step: 209, loss: 0.16596384346485138, acc: 0.9219, auc: 0.9851, precision: 0.9483, recall: 0.8871\n",
      "2019-01-02T18:18:38.289508, step: 210, loss: 0.15216681361198425, acc: 0.9375, auc: 0.988, precision: 0.9333, recall: 0.9333\n",
      "2019-01-02T18:18:38.463929, step: 211, loss: 0.315232515335083, acc: 0.8828, auc: 0.9684, precision: 0.8261, recall: 0.95\n",
      "2019-01-02T18:18:38.632096, step: 212, loss: 0.2687529921531677, acc: 0.8984, auc: 0.9589, precision: 0.8971, recall: 0.9104\n",
      "2019-01-02T18:18:38.816737, step: 213, loss: 0.23328638076782227, acc: 0.9141, auc: 0.9686, precision: 0.8929, recall: 0.9091\n",
      "2019-01-02T18:18:38.997887, step: 214, loss: 0.22559137642383575, acc: 0.9062, auc: 0.9722, precision: 0.9483, recall: 0.8594\n",
      "2019-01-02T18:18:39.172090, step: 215, loss: 0.2734631896018982, acc: 0.8672, auc: 0.9707, precision: 0.9767, recall: 0.7241\n",
      "2019-01-02T18:18:39.351305, step: 216, loss: 0.31298741698265076, acc: 0.8281, auc: 0.9626, precision: 0.9592, recall: 0.7015\n",
      "2019-01-02T18:18:39.526170, step: 217, loss: 0.2406773567199707, acc: 0.8906, auc: 0.9652, precision: 0.9091, recall: 0.8824\n",
      "2019-01-02T18:18:39.698831, step: 218, loss: 0.2501457929611206, acc: 0.9141, auc: 0.9831, precision: 0.8421, recall: 0.96\n",
      "2019-01-02T18:18:39.873221, step: 219, loss: 0.21696454286575317, acc: 0.9219, auc: 0.9797, precision: 0.8873, recall: 0.9692\n",
      "2019-01-02T18:18:40.056265, step: 220, loss: 0.20772379636764526, acc: 0.9141, auc: 0.9726, precision: 0.9375, recall: 0.8955\n",
      "2019-01-02T18:18:40.233805, step: 221, loss: 0.19445271790027618, acc: 0.9297, auc: 0.9734, precision: 0.95, recall: 0.9048\n",
      "2019-01-02T18:18:40.404965, step: 222, loss: 0.22381941974163055, acc: 0.9062, auc: 0.9821, precision: 1.0, recall: 0.8209\n",
      "2019-01-02T18:18:40.576394, step: 223, loss: 0.22872936725616455, acc: 0.875, auc: 0.9744, precision: 0.9655, recall: 0.8\n",
      "2019-01-02T18:18:40.745463, step: 224, loss: 0.2120395004749298, acc: 0.9375, auc: 0.969, precision: 0.9265, recall: 0.9545\n",
      "2019-01-02T18:18:40.910784, step: 225, loss: 0.18937864899635315, acc: 0.9062, auc: 0.9808, precision: 0.9265, recall: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:18:41.078412, step: 226, loss: 0.2242620438337326, acc: 0.9531, auc: 0.9768, precision: 0.9211, recall: 1.0\n",
      "2019-01-02T18:18:41.248977, step: 227, loss: 0.18481998145580292, acc: 0.9375, auc: 0.9816, precision: 0.9221, recall: 0.9726\n",
      "2019-01-02T18:18:41.417226, step: 228, loss: 0.2726507782936096, acc: 0.8906, auc: 0.9556, precision: 0.9153, recall: 0.8571\n",
      "2019-01-02T18:18:41.586255, step: 229, loss: 0.2042814940214157, acc: 0.875, auc: 0.9808, precision: 0.9672, recall: 0.8082\n",
      "2019-01-02T18:18:41.750280, step: 230, loss: 0.2316577434539795, acc: 0.9062, auc: 0.9692, precision: 0.9815, recall: 0.8281\n",
      "2019-01-02T18:18:41.918606, step: 231, loss: 0.191634863615036, acc: 0.9141, auc: 0.9788, precision: 0.9821, recall: 0.8462\n",
      "2019-01-02T18:18:42.095287, step: 232, loss: 0.20064033567905426, acc: 0.8984, auc: 0.9773, precision: 0.8676, recall: 0.9365\n",
      "2019-01-02T18:18:42.265060, step: 233, loss: 0.24074290692806244, acc: 0.9297, auc: 0.9698, precision: 0.9286, recall: 0.942\n",
      "2019-01-02T18:18:42.437669, step: 234, loss: 0.25307875871658325, acc: 0.8828, auc: 0.9621, precision: 0.8923, recall: 0.8788\n",
      "2019-01-02T18:18:42.614614, step: 235, loss: 0.15293411910533905, acc: 0.9219, auc: 0.9941, precision: 1.0, recall: 0.8592\n",
      "2019-01-02T18:18:42.786732, step: 236, loss: 0.13707029819488525, acc: 0.9453, auc: 0.9895, precision: 0.9833, recall: 0.9077\n",
      "2019-01-02T18:18:42.956259, step: 237, loss: 0.32659274339675903, acc: 0.8906, auc: 0.9508, precision: 0.8413, recall: 0.9298\n",
      "2019-01-02T18:18:43.139757, step: 238, loss: 0.15856081247329712, acc: 0.9375, auc: 0.9844, precision: 0.9667, recall: 0.9062\n",
      "2019-01-02T18:18:43.316944, step: 239, loss: 0.1981474608182907, acc: 0.9141, auc: 0.9763, precision: 0.95, recall: 0.8769\n",
      "2019-01-02T18:18:43.488184, step: 240, loss: 0.14255057275295258, acc: 0.9375, auc: 0.9856, precision: 0.9608, recall: 0.8909\n",
      "2019-01-02T18:18:43.653369, step: 241, loss: 0.12681372463703156, acc: 0.9375, auc: 0.9966, precision: 1.0, recall: 0.8806\n",
      "2019-01-02T18:18:43.817787, step: 242, loss: 0.18515247106552124, acc: 0.9375, auc: 0.9804, precision: 0.9467, recall: 0.9467\n",
      "2019-01-02T18:18:43.985358, step: 243, loss: 0.22672918438911438, acc: 0.9062, auc: 0.9686, precision: 0.9538, recall: 0.8732\n",
      "2019-01-02T18:18:44.146244, step: 244, loss: 0.16134187579154968, acc: 0.9375, auc: 0.9914, precision: 0.9077, recall: 0.9672\n",
      "2019-01-02T18:18:44.319487, step: 245, loss: 0.16867035627365112, acc: 0.9297, auc: 0.9836, precision: 0.9474, recall: 0.9\n",
      "2019-01-02T18:18:44.491678, step: 246, loss: 0.18552042543888092, acc: 0.9297, auc: 0.9773, precision: 0.9841, recall: 0.8857\n",
      "2019-01-02T18:18:44.667949, step: 247, loss: 0.19524917006492615, acc: 0.9062, auc: 0.9768, precision: 0.9032, recall: 0.9032\n",
      "2019-01-02T18:18:44.837496, step: 248, loss: 0.19193941354751587, acc: 0.8984, auc: 0.977, precision: 0.9074, recall: 0.8596\n",
      "2019-01-02T18:18:45.015347, step: 249, loss: 0.14636901021003723, acc: 0.9297, auc: 0.9897, precision: 0.9455, recall: 0.8966\n",
      "2019-01-02T18:18:45.170734, step: 250, loss: 0.17876750230789185, acc: 0.9062, auc: 0.9825, precision: 0.9844, recall: 0.8514\n",
      "2019-01-02T18:18:45.336609, step: 251, loss: 0.19912075996398926, acc: 0.9062, auc: 0.979, precision: 0.9492, recall: 0.8615\n",
      "2019-01-02T18:18:45.502784, step: 252, loss: 0.2176414430141449, acc: 0.8984, auc: 0.9731, precision: 0.9074, recall: 0.8596\n",
      "2019-01-02T18:18:45.679766, step: 253, loss: 0.16648533940315247, acc: 0.9297, auc: 0.9819, precision: 0.9508, recall: 0.9062\n",
      "2019-01-02T18:18:45.850236, step: 254, loss: 0.16658152639865875, acc: 0.9141, auc: 0.9878, precision: 0.9344, recall: 0.8906\n",
      "2019-01-02T18:18:46.005795, step: 255, loss: 0.22312931716442108, acc: 0.9219, auc: 0.9731, precision: 0.9048, recall: 0.9344\n",
      "2019-01-02T18:18:46.180912, step: 256, loss: 0.12342002242803574, acc: 0.9531, auc: 0.9921, precision: 0.9492, recall: 0.9492\n",
      "2019-01-02T18:18:46.355022, step: 257, loss: 0.2557230591773987, acc: 0.8438, auc: 0.9612, precision: 0.9322, recall: 0.7746\n",
      "2019-01-02T18:18:46.511117, step: 258, loss: 0.2562311291694641, acc: 0.8438, auc: 0.9589, precision: 0.9344, recall: 0.7808\n",
      "2019-01-02T18:18:46.681657, step: 259, loss: 0.13935360312461853, acc: 0.9531, auc: 0.9906, precision: 1.0, recall: 0.8966\n",
      "2019-01-02T18:18:46.855450, step: 260, loss: 0.22200734913349152, acc: 0.9297, auc: 0.975, precision: 0.8939, recall: 0.9672\n",
      "2019-01-02T18:18:47.025810, step: 261, loss: 0.30039429664611816, acc: 0.875, auc: 0.965, precision: 0.8125, recall: 0.9286\n",
      "2019-01-02T18:18:47.198127, step: 262, loss: 0.2259823977947235, acc: 0.9141, auc: 0.9708, precision: 0.9672, recall: 0.8676\n",
      "2019-01-02T18:18:47.367045, step: 263, loss: 0.31538814306259155, acc: 0.8672, auc: 0.9605, precision: 0.9831, recall: 0.7838\n",
      "2019-01-02T18:18:47.541909, step: 264, loss: 0.20890331268310547, acc: 0.9297, auc: 0.9729, precision: 0.9672, recall: 0.8939\n",
      "2019-01-02T18:18:47.711599, step: 265, loss: 0.3153899908065796, acc: 0.8594, auc: 0.9597, precision: 0.8158, recall: 0.9394\n",
      "2019-01-02T18:18:47.884923, step: 266, loss: 0.22101585566997528, acc: 0.9219, auc: 0.9722, precision: 0.8966, recall: 0.9286\n",
      "2019-01-02T18:18:48.056774, step: 267, loss: 0.174004465341568, acc: 0.9219, auc: 0.9826, precision: 0.963, recall: 0.8667\n",
      "2019-01-02T18:18:48.227489, step: 268, loss: 0.1492335945367813, acc: 0.9297, auc: 0.9894, precision: 0.9844, recall: 0.8873\n",
      "2019-01-02T18:18:48.394693, step: 269, loss: 0.2553650736808777, acc: 0.8672, auc: 0.9687, precision: 0.931, recall: 0.806\n",
      "2019-01-02T18:18:48.563059, step: 270, loss: 0.1484622359275818, acc: 0.9453, auc: 0.9885, precision: 0.9655, recall: 0.918\n",
      "2019-01-02T18:18:48.738124, step: 271, loss: 0.20757126808166504, acc: 0.9453, auc: 0.974, precision: 0.9851, recall: 0.9167\n",
      "2019-01-02T18:18:48.917277, step: 272, loss: 0.28067782521247864, acc: 0.9141, auc: 0.9724, precision: 0.8571, recall: 0.9836\n",
      "2019-01-02T18:18:49.100106, step: 273, loss: 0.15552127361297607, acc: 0.9297, auc: 0.9912, precision: 0.918, recall: 0.9333\n",
      "2019-01-02T18:18:49.272695, step: 274, loss: 0.09903101623058319, acc: 0.9688, auc: 0.998, precision: 0.9841, recall: 0.9538\n",
      "2019-01-02T18:18:49.445114, step: 275, loss: 0.21292860805988312, acc: 0.8906, auc: 0.988, precision: 0.9804, recall: 0.7937\n",
      "2019-01-02T18:18:49.623540, step: 276, loss: 0.2132973074913025, acc: 0.9062, auc: 0.9829, precision: 1.0, recall: 0.8333\n",
      "2019-01-02T18:18:49.800160, step: 277, loss: 0.18304041028022766, acc: 0.9531, auc: 0.9787, precision: 0.9545, recall: 0.9545\n",
      "2019-01-02T18:18:49.968574, step: 278, loss: 0.2669650912284851, acc: 0.9062, auc: 0.972, precision: 0.8615, recall: 0.9492\n",
      "2019-01-02T18:18:50.139485, step: 279, loss: 0.17490854859352112, acc: 0.9375, auc: 0.9853, precision: 0.9231, recall: 0.9524\n",
      "2019-01-02T18:18:50.311940, step: 280, loss: 0.24021616578102112, acc: 0.8828, auc: 0.9642, precision: 0.92, recall: 0.807\n",
      "2019-01-02T18:18:50.485049, step: 281, loss: 0.22300958633422852, acc: 0.9141, auc: 0.9714, precision: 0.9649, recall: 0.8594\n",
      "2019-01-02T18:18:50.659532, step: 282, loss: 0.16479933261871338, acc: 0.9453, auc: 0.9858, precision: 0.9844, recall: 0.913\n",
      "2019-01-02T18:18:50.829092, step: 283, loss: 0.302175909280777, acc: 0.8438, auc: 0.9525, precision: 0.9444, recall: 0.75\n",
      "2019-01-02T18:18:50.998820, step: 284, loss: 0.208279550075531, acc: 0.8984, auc: 0.9752, precision: 0.9, recall: 0.913\n",
      "2019-01-02T18:18:51.174238, step: 285, loss: 0.31157535314559937, acc: 0.8672, auc: 0.9504, precision: 0.8451, recall: 0.9091\n",
      "2019-01-02T18:18:51.342000, step: 286, loss: 0.2435866743326187, acc: 0.9062, auc: 0.9704, precision: 0.8906, recall: 0.9194\n",
      "2019-01-02T18:18:51.510331, step: 287, loss: 0.098830446600914, acc: 0.9766, auc: 0.9985, precision: 0.9848, recall: 0.9701\n",
      "2019-01-02T18:18:51.680872, step: 288, loss: 0.16444450616836548, acc: 0.8906, auc: 0.9911, precision: 0.9667, recall: 0.8286\n",
      "2019-01-02T18:18:51.851443, step: 289, loss: 0.2303948700428009, acc: 0.8984, auc: 0.9665, precision: 0.9492, recall: 0.8485\n",
      "2019-01-02T18:18:52.022022, step: 290, loss: 0.1929399073123932, acc: 0.9062, auc: 0.9763, precision: 0.9492, recall: 0.8615\n",
      "2019-01-02T18:18:52.195551, step: 291, loss: 0.135231152176857, acc: 0.9453, auc: 0.9904, precision: 0.9808, recall: 0.8947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:18:52.359568, step: 292, loss: 0.21591730415821075, acc: 0.9219, auc: 0.9734, precision: 0.9206, recall: 0.9206\n",
      "2019-01-02T18:18:52.527000, step: 293, loss: 0.1911626160144806, acc: 0.9375, auc: 0.9792, precision: 0.9643, recall: 0.9\n",
      "2019-01-02T18:18:52.697579, step: 294, loss: 0.19775879383087158, acc: 0.9141, auc: 0.9763, precision: 0.9423, recall: 0.8596\n",
      "2019-01-02T18:18:52.868217, step: 295, loss: 0.25683534145355225, acc: 0.8906, auc: 0.959, precision: 0.9412, recall: 0.8136\n",
      "2019-01-02T18:18:53.041812, step: 296, loss: 0.23172816634178162, acc: 0.9141, auc: 0.9692, precision: 0.95, recall: 0.8769\n",
      "2019-01-02T18:18:53.211920, step: 297, loss: 0.21315714716911316, acc: 0.9141, auc: 0.9733, precision: 0.9516, recall: 0.8806\n",
      "2019-01-02T18:18:53.388017, step: 298, loss: 0.1779269129037857, acc: 0.9375, auc: 0.9845, precision: 0.918, recall: 0.9492\n",
      "2019-01-02T18:18:53.557974, step: 299, loss: 0.15868377685546875, acc: 0.9375, auc: 0.985, precision: 0.9412, recall: 0.9412\n",
      "2019-01-02T18:18:53.729021, step: 300, loss: 0.139278382062912, acc: 0.9609, auc: 0.9883, precision: 0.9524, recall: 0.9677\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T18:19:00.370848, step: 300, loss: 0.37125690778096515, acc: 0.8429589743589745, auc: 0.9269871794871793, precision: 0.884148717948718, recall: 0.7940897435897435\n",
      "2019-01-02T18:19:00.540266, step: 301, loss: 0.19004879891872406, acc: 0.9297, auc: 0.978, precision: 0.9344, recall: 0.9194\n",
      "2019-01-02T18:19:00.714942, step: 302, loss: 0.19974593818187714, acc: 0.9219, auc: 0.9753, precision: 0.9787, recall: 0.8364\n",
      "2019-01-02T18:19:00.889752, step: 303, loss: 0.23716941475868225, acc: 0.8672, auc: 0.9812, precision: 0.9808, recall: 0.7612\n",
      "2019-01-02T18:19:01.058294, step: 304, loss: 0.17007000744342804, acc: 0.9375, auc: 0.9824, precision: 0.9538, recall: 0.9254\n",
      "2019-01-02T18:19:01.230550, step: 305, loss: 0.1604808270931244, acc: 0.9453, auc: 0.9919, precision: 0.9206, recall: 0.9667\n",
      "2019-01-02T18:19:01.401840, step: 306, loss: 0.1488623321056366, acc: 0.9609, auc: 0.9866, precision: 0.9538, recall: 0.9688\n",
      "2019-01-02T18:19:01.576490, step: 307, loss: 0.3778833746910095, acc: 0.8672, auc: 0.9323, precision: 0.8767, recall: 0.8889\n",
      "2019-01-02T18:19:01.748945, step: 308, loss: 0.20726782083511353, acc: 0.9141, auc: 0.9748, precision: 0.9464, recall: 0.8689\n",
      "2019-01-02T18:19:01.921707, step: 309, loss: 0.26665395498275757, acc: 0.8906, auc: 0.9651, precision: 0.9412, recall: 0.8136\n",
      "2019-01-02T18:19:02.093885, step: 310, loss: 0.21586540341377258, acc: 0.9219, auc: 0.9742, precision: 0.9697, recall: 0.8889\n",
      "2019-01-02T18:19:02.258864, step: 311, loss: 0.14280381798744202, acc: 0.9531, auc: 0.9895, precision: 0.9692, recall: 0.9403\n",
      "2019-01-02T18:19:02.435760, step: 312, loss: 0.16403749585151672, acc: 0.9453, auc: 0.9848, precision: 0.9333, recall: 0.9492\n",
      "start training model\n",
      "2019-01-02T18:19:02.632670, step: 313, loss: 0.03550250828266144, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:02.801832, step: 314, loss: 0.06654125452041626, acc: 0.9844, auc: 0.9983, precision: 0.9828, recall: 0.9828\n",
      "2019-01-02T18:19:02.976830, step: 315, loss: 0.03880632668733597, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:03.151034, step: 316, loss: 0.07626433670520782, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9722\n",
      "2019-01-02T18:19:03.320658, step: 317, loss: 0.09150463342666626, acc: 0.9531, auc: 0.9973, precision: 0.9839, recall: 0.9242\n",
      "2019-01-02T18:19:03.492373, step: 318, loss: 0.059582117944955826, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9833\n",
      "2019-01-02T18:19:03.663329, step: 319, loss: 0.07274722307920456, acc: 0.9766, auc: 0.9983, precision: 1.0, recall: 0.9565\n",
      "2019-01-02T18:19:03.834113, step: 320, loss: 0.07803527265787125, acc: 0.9766, auc: 0.998, precision: 1.0, recall: 0.9552\n",
      "2019-01-02T18:19:04.007994, step: 321, loss: 0.03982504457235336, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-02T18:19:04.178518, step: 322, loss: 0.06121676787734032, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-01-02T18:19:04.349581, step: 323, loss: 0.06202616170048714, acc: 0.9844, auc: 0.9998, precision: 0.9714, recall: 1.0\n",
      "2019-01-02T18:19:04.518721, step: 324, loss: 0.035299960523843765, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-02T18:19:04.694145, step: 325, loss: 0.05300223082304001, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9846\n",
      "2019-01-02T18:19:04.873919, step: 326, loss: 0.04976523667573929, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-01-02T18:19:05.045941, step: 327, loss: 0.071622334420681, acc: 0.9766, auc: 0.9978, precision: 1.0, recall: 0.9531\n",
      "2019-01-02T18:19:05.220302, step: 328, loss: 0.05582661181688309, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9697\n",
      "2019-01-02T18:19:05.391051, step: 329, loss: 0.07482177019119263, acc: 0.9688, auc: 0.9976, precision: 0.9844, recall: 0.9545\n",
      "2019-01-02T18:19:05.567985, step: 330, loss: 0.06884608417749405, acc: 0.9766, auc: 0.9983, precision: 0.9841, recall: 0.9688\n",
      "2019-01-02T18:19:05.742455, step: 331, loss: 0.09087468683719635, acc: 0.9844, auc: 0.9973, precision: 0.9718, recall: 1.0\n",
      "2019-01-02T18:19:05.923082, step: 332, loss: 0.05951067432761192, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9851\n",
      "2019-01-02T18:19:06.091605, step: 333, loss: 0.08163776248693466, acc: 0.9688, auc: 0.997, precision: 0.9815, recall: 0.9464\n",
      "2019-01-02T18:19:06.267602, step: 334, loss: 0.03761393204331398, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2019-01-02T18:19:06.434162, step: 335, loss: 0.05980989709496498, acc: 0.9844, auc: 0.9983, precision: 0.9855, recall: 0.9855\n",
      "2019-01-02T18:19:06.600112, step: 336, loss: 0.04849408194422722, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9733\n",
      "2019-01-02T18:19:06.779736, step: 337, loss: 0.12510977685451508, acc: 0.9375, auc: 0.9985, precision: 1.0, recall: 0.8769\n",
      "2019-01-02T18:19:06.950849, step: 338, loss: 0.04783819243311882, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9722\n",
      "2019-01-02T18:19:07.125368, step: 339, loss: 0.0606294684112072, acc: 0.9922, auc: 0.9983, precision: 0.9853, recall: 1.0\n",
      "2019-01-02T18:19:07.300075, step: 340, loss: 0.08160927891731262, acc: 0.9844, auc: 0.9995, precision: 0.9688, recall: 1.0\n",
      "2019-01-02T18:19:07.468289, step: 341, loss: 0.08165878057479858, acc: 0.9844, auc: 0.9998, precision: 0.9701, recall: 1.0\n",
      "2019-01-02T18:19:07.638036, step: 342, loss: 0.03456442803144455, acc: 0.9844, auc: 0.9997, precision: 0.9867, recall: 0.9867\n",
      "2019-01-02T18:19:07.808650, step: 343, loss: 0.056497082114219666, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9552\n",
      "2019-01-02T18:19:07.980474, step: 344, loss: 0.08442267030477524, acc: 0.9766, auc: 0.9958, precision: 1.0, recall: 0.9464\n",
      "2019-01-02T18:19:08.153155, step: 345, loss: 0.06184513121843338, acc: 0.9688, auc: 0.999, precision: 1.0, recall: 0.9365\n",
      "2019-01-02T18:19:08.324056, step: 346, loss: 0.048944100737571716, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9655\n",
      "2019-01-02T18:19:08.499500, step: 347, loss: 0.0315970778465271, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-02T18:19:08.674491, step: 348, loss: 0.01989854872226715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:08.845041, step: 349, loss: 0.04037901759147644, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-02T18:19:09.032844, step: 350, loss: 0.04931800067424774, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-01-02T18:19:09.203276, step: 351, loss: 0.04243016615509987, acc: 0.9844, auc: 0.9995, precision: 0.9836, recall: 0.9836\n",
      "2019-01-02T18:19:09.376090, step: 352, loss: 0.053695835173130035, acc: 0.9844, auc: 0.9949, precision: 0.9848, recall: 0.9848\n",
      "2019-01-02T18:19:09.550980, step: 353, loss: 0.05821676552295685, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9149\n",
      "2019-01-02T18:19:09.723675, step: 354, loss: 0.08120675384998322, acc: 0.9688, auc: 0.9963, precision: 0.9831, recall: 0.9508\n",
      "2019-01-02T18:19:09.905035, step: 355, loss: 0.04281849041581154, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.95\n",
      "2019-01-02T18:19:10.080094, step: 356, loss: 0.07795002311468124, acc: 0.9766, auc: 0.9972, precision: 0.98, recall: 0.9608\n",
      "2019-01-02T18:19:10.251417, step: 357, loss: 0.0925426036119461, acc: 0.9688, auc: 0.9909, precision: 0.9718, recall: 0.9718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:19:10.427304, step: 358, loss: 0.07232265174388885, acc: 0.9766, auc: 0.9995, precision: 0.9483, recall: 1.0\n",
      "2019-01-02T18:19:10.602258, step: 359, loss: 0.14428497850894928, acc: 0.9688, auc: 0.9832, precision: 0.9692, recall: 0.9692\n",
      "2019-01-02T18:19:10.773311, step: 360, loss: 0.05570978671312332, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9615\n",
      "2019-01-02T18:19:10.946128, step: 361, loss: 0.04782415181398392, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9821\n",
      "2019-01-02T18:19:11.118112, step: 362, loss: 0.06263410300016403, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.974\n",
      "2019-01-02T18:19:11.290078, step: 363, loss: 0.03565814718604088, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:11.467753, step: 364, loss: 0.036555152386426926, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-01-02T18:19:11.638296, step: 365, loss: 0.05434255674481392, acc: 0.9922, auc: 0.9995, precision: 0.9848, recall: 1.0\n",
      "2019-01-02T18:19:11.808033, step: 366, loss: 0.03572778403759003, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:11.979982, step: 367, loss: 0.088213250041008, acc: 0.9688, auc: 0.9944, precision: 0.9552, recall: 0.9846\n",
      "2019-01-02T18:19:12.150067, step: 368, loss: 0.09235142171382904, acc: 0.9766, auc: 0.9951, precision: 1.0, recall: 0.9474\n",
      "2019-01-02T18:19:12.318750, step: 369, loss: 0.038888826966285706, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2019-01-02T18:19:12.487915, step: 370, loss: 0.03026989847421646, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-01-02T18:19:12.660081, step: 371, loss: 0.07739131152629852, acc: 0.9766, auc: 0.995, precision: 1.0, recall: 0.9595\n",
      "2019-01-02T18:19:12.829304, step: 372, loss: 0.029436053708195686, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9636\n",
      "2019-01-02T18:19:12.999240, step: 373, loss: 0.06337088346481323, acc: 0.9922, auc: 0.9993, precision: 0.9851, recall: 1.0\n",
      "2019-01-02T18:19:13.173914, step: 374, loss: 0.08312343060970306, acc: 0.9844, auc: 0.9934, precision: 0.9697, recall: 1.0\n",
      "2019-01-02T18:19:13.346240, step: 375, loss: 0.0526011623442173, acc: 0.9922, auc: 0.9995, precision: 0.9841, recall: 1.0\n",
      "2019-01-02T18:19:13.524148, step: 376, loss: 0.03534530848264694, acc: 0.9844, auc: 0.9998, precision: 0.9848, recall: 0.9848\n",
      "2019-01-02T18:19:13.702736, step: 377, loss: 0.07901415228843689, acc: 0.9453, auc: 0.999, precision: 1.0, recall: 0.8923\n",
      "2019-01-02T18:19:13.881754, step: 378, loss: 0.07363676279783249, acc: 0.9688, auc: 0.9975, precision: 0.9661, recall: 0.9661\n",
      "2019-01-02T18:19:14.060019, step: 379, loss: 0.09165310859680176, acc: 0.9922, auc: 0.9907, precision: 1.0, recall: 0.9853\n",
      "2019-01-02T18:19:14.238788, step: 380, loss: 0.08426669239997864, acc: 0.9688, auc: 0.9973, precision: 0.9583, recall: 0.9857\n",
      "2019-01-02T18:19:14.410275, step: 381, loss: 0.04911435768008232, acc: 0.9844, auc: 0.9992, precision: 1.0, recall: 0.974\n",
      "2019-01-02T18:19:14.582002, step: 382, loss: 0.05256534740328789, acc: 0.9844, auc: 0.9998, precision: 0.9649, recall: 1.0\n",
      "2019-01-02T18:19:14.765453, step: 383, loss: 0.04716974496841431, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9706\n",
      "2019-01-02T18:19:14.937100, step: 384, loss: 0.10046084970235825, acc: 0.9688, auc: 0.9907, precision: 1.0, recall: 0.9333\n",
      "2019-01-02T18:19:15.109525, step: 385, loss: 0.05209886655211449, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9667\n",
      "2019-01-02T18:19:15.282346, step: 386, loss: 0.09643684327602386, acc: 0.9922, auc: 0.989, precision: 1.0, recall: 0.9839\n",
      "2019-01-02T18:19:15.453998, step: 387, loss: 0.05335497483611107, acc: 0.9922, auc: 1.0, precision: 0.9868, recall: 1.0\n",
      "2019-01-02T18:19:15.627490, step: 388, loss: 0.05934898555278778, acc: 0.9922, auc: 0.9985, precision: 0.9833, recall: 1.0\n",
      "2019-01-02T18:19:15.796804, step: 389, loss: 0.0668797418475151, acc: 0.9766, auc: 0.9988, precision: 0.9855, recall: 0.9714\n",
      "2019-01-02T18:19:15.970429, step: 390, loss: 0.04863354191184044, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9848\n",
      "2019-01-02T18:19:16.139405, step: 391, loss: 0.053583040833473206, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9831\n",
      "2019-01-02T18:19:16.313767, step: 392, loss: 0.07015237957239151, acc: 0.9609, auc: 0.998, precision: 0.9848, recall: 0.942\n",
      "2019-01-02T18:19:16.491921, step: 393, loss: 0.08918879926204681, acc: 0.9844, auc: 0.9939, precision: 0.9839, recall: 0.9839\n",
      "2019-01-02T18:19:16.656489, step: 394, loss: 0.048777565360069275, acc: 0.9922, auc: 0.9993, precision: 0.9836, recall: 1.0\n",
      "2019-01-02T18:19:16.828980, step: 395, loss: 0.07198813557624817, acc: 0.9766, auc: 0.9985, precision: 0.971, recall: 0.9853\n",
      "2019-01-02T18:19:16.998459, step: 396, loss: 0.0922781303524971, acc: 0.9766, auc: 0.9968, precision: 0.9828, recall: 0.9661\n",
      "2019-01-02T18:19:17.166983, step: 397, loss: 0.033223044127225876, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:17.339301, step: 398, loss: 0.030283119529485703, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-01-02T18:19:17.509245, step: 399, loss: 0.07886116206645966, acc: 0.9609, auc: 0.9978, precision: 0.9825, recall: 0.9333\n",
      "2019-01-02T18:19:17.677056, step: 400, loss: 0.06594409048557281, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.931\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T18:19:24.276721, step: 400, loss: 0.38063780122842544, acc: 0.8559769230769232, auc: 0.9329025641025646, precision: 0.8734410256410258, recall: 0.8367410256410256\n",
      "2019-01-02T18:19:24.442633, step: 401, loss: 0.051043521612882614, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9861\n",
      "2019-01-02T18:19:24.625108, step: 402, loss: 0.03380721062421799, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-02T18:19:24.796556, step: 403, loss: 0.06328057497739792, acc: 0.9766, auc: 1.0, precision: 0.9455, recall: 1.0\n",
      "2019-01-02T18:19:24.973537, step: 404, loss: 0.04569857567548752, acc: 0.9922, auc: 1.0, precision: 0.9855, recall: 1.0\n",
      "2019-01-02T18:19:25.147247, step: 405, loss: 0.0644499883055687, acc: 0.9766, auc: 0.9988, precision: 1.0, recall: 0.9483\n",
      "2019-01-02T18:19:25.319011, step: 406, loss: 0.030810875818133354, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-01-02T18:19:25.501809, step: 407, loss: 0.06649024784564972, acc: 0.9766, auc: 0.9988, precision: 0.9836, recall: 0.9677\n",
      "2019-01-02T18:19:25.682306, step: 408, loss: 0.041956763714551926, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9538\n",
      "2019-01-02T18:19:25.852189, step: 409, loss: 0.018324224278330803, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:26.024483, step: 410, loss: 0.03511008620262146, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:26.196200, step: 411, loss: 0.04587019979953766, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2019-01-02T18:19:26.363204, step: 412, loss: 0.03665393590927124, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9804\n",
      "2019-01-02T18:19:26.532474, step: 413, loss: 0.050662800669670105, acc: 0.9844, auc: 0.9992, precision: 1.0, recall: 0.974\n",
      "2019-01-02T18:19:26.704601, step: 414, loss: 0.03238556906580925, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:26.882067, step: 415, loss: 0.04367364943027496, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9857\n",
      "2019-01-02T18:19:27.052851, step: 416, loss: 0.038347989320755005, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-02T18:19:27.223092, step: 417, loss: 0.05204887315630913, acc: 0.9844, auc: 0.9983, precision: 0.9851, recall: 0.9851\n",
      "2019-01-02T18:19:27.394615, step: 418, loss: 0.08108469098806381, acc: 0.9844, auc: 0.9909, precision: 0.9718, recall: 1.0\n",
      "2019-01-02T18:19:27.562513, step: 419, loss: 0.040442854166030884, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9444\n",
      "2019-01-02T18:19:27.745825, step: 420, loss: 0.05921414867043495, acc: 0.9844, auc: 0.9973, precision: 0.9841, recall: 0.9841\n",
      "2019-01-02T18:19:27.917261, step: 421, loss: 0.07368819415569305, acc: 0.9844, auc: 0.9958, precision: 1.0, recall: 0.9688\n",
      "2019-01-02T18:19:28.088414, step: 422, loss: 0.03821025416254997, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-02T18:19:28.256152, step: 423, loss: 0.07941055297851562, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:19:28.426677, step: 424, loss: 0.045727893710136414, acc: 0.9922, auc: 0.9988, precision: 0.9865, recall: 1.0\n",
      "2019-01-02T18:19:28.598747, step: 425, loss: 0.045645684003829956, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-01-02T18:19:28.773276, step: 426, loss: 0.09307089447975159, acc: 0.9609, auc: 0.9956, precision: 0.9821, recall: 0.9322\n",
      "2019-01-02T18:19:28.940930, step: 427, loss: 0.050742968916893005, acc: 0.9688, auc: 0.9998, precision: 1.0, recall: 0.9286\n",
      "2019-01-02T18:19:29.107294, step: 428, loss: 0.05154186859726906, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.95\n",
      "2019-01-02T18:19:29.275032, step: 429, loss: 0.03667081519961357, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9831\n",
      "2019-01-02T18:19:29.447296, step: 430, loss: 0.048541005700826645, acc: 0.9844, auc: 0.9995, precision: 0.9853, recall: 0.9853\n",
      "2019-01-02T18:19:29.615127, step: 431, loss: 0.07922790199518204, acc: 0.9766, auc: 0.9948, precision: 1.0, recall: 0.9577\n",
      "2019-01-02T18:19:29.789044, step: 432, loss: 0.03798280656337738, acc: 0.9922, auc: 1.0, precision: 0.9844, recall: 1.0\n",
      "2019-01-02T18:19:29.963599, step: 433, loss: 0.06629770994186401, acc: 0.9844, auc: 1.0, precision: 0.9722, recall: 1.0\n",
      "2019-01-02T18:19:30.151556, step: 434, loss: 0.05277297645807266, acc: 0.9766, auc: 0.999, precision: 0.9867, recall: 0.9737\n",
      "2019-01-02T18:19:30.319989, step: 435, loss: 0.049751002341508865, acc: 0.9688, auc: 0.9998, precision: 1.0, recall: 0.9286\n",
      "2019-01-02T18:19:30.495699, step: 436, loss: 0.0460224524140358, acc: 0.9844, auc: 0.9993, precision: 0.9859, recall: 0.9859\n",
      "2019-01-02T18:19:30.670335, step: 437, loss: 0.057039663195610046, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9385\n",
      "2019-01-02T18:19:30.841879, step: 438, loss: 0.06162774935364723, acc: 0.9688, auc: 0.9995, precision: 1.0, recall: 0.9355\n",
      "2019-01-02T18:19:31.015585, step: 439, loss: 0.0479026734828949, acc: 0.9922, auc: 0.9998, precision: 0.9831, recall: 1.0\n",
      "2019-01-02T18:19:31.195669, step: 440, loss: 0.06759805232286453, acc: 0.9844, auc: 0.9977, precision: 0.9811, recall: 0.9811\n",
      "2019-01-02T18:19:31.377180, step: 441, loss: 0.036682602018117905, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-01-02T18:19:31.546995, step: 442, loss: 0.07325860857963562, acc: 0.9844, auc: 0.9993, precision: 0.9706, recall: 1.0\n",
      "2019-01-02T18:19:31.720118, step: 443, loss: 0.061687812209129333, acc: 0.9766, auc: 0.9985, precision: 0.9722, recall: 0.9859\n",
      "2019-01-02T18:19:31.896667, step: 444, loss: 0.04219171404838562, acc: 0.9766, auc: 0.9993, precision: 1.0, recall: 0.9531\n",
      "2019-01-02T18:19:32.066575, step: 445, loss: 0.05185424163937569, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9508\n",
      "2019-01-02T18:19:32.235909, step: 446, loss: 0.05859735608100891, acc: 0.9844, auc: 0.9985, precision: 1.0, recall: 0.9683\n",
      "2019-01-02T18:19:32.405807, step: 447, loss: 0.05548539385199547, acc: 0.9844, auc: 0.999, precision: 0.9667, recall: 1.0\n",
      "2019-01-02T18:19:32.580033, step: 448, loss: 0.06452342122793198, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9531\n",
      "2019-01-02T18:19:32.751930, step: 449, loss: 0.0941004529595375, acc: 0.9766, auc: 0.9892, precision: 0.9833, recall: 0.9672\n",
      "2019-01-02T18:19:32.929768, step: 450, loss: 0.038725078105926514, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9863\n",
      "2019-01-02T18:19:33.099829, step: 451, loss: 0.04715783894062042, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9839\n",
      "2019-01-02T18:19:33.271839, step: 452, loss: 0.061509132385253906, acc: 0.9766, auc: 0.998, precision: 0.9857, recall: 0.9718\n",
      "2019-01-02T18:19:33.442104, step: 453, loss: 0.041926465928554535, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:33.613721, step: 454, loss: 0.10390840470790863, acc: 0.9688, auc: 0.9946, precision: 0.9661, recall: 0.9661\n",
      "2019-01-02T18:19:33.783491, step: 455, loss: 0.03647952526807785, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9811\n",
      "2019-01-02T18:19:33.953392, step: 456, loss: 0.0429440513253212, acc: 0.9844, auc: 0.9997, precision: 1.0, recall: 0.973\n",
      "2019-01-02T18:19:34.133229, step: 457, loss: 0.06526261568069458, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9672\n",
      "2019-01-02T18:19:34.306242, step: 458, loss: 0.026841815561056137, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-01-02T18:19:34.473457, step: 459, loss: 0.09515050798654556, acc: 0.9688, auc: 0.9894, precision: 0.9851, recall: 0.9565\n",
      "2019-01-02T18:19:34.643836, step: 460, loss: 0.044372912496328354, acc: 0.9922, auc: 0.9995, precision: 0.9841, recall: 1.0\n",
      "2019-01-02T18:19:34.821458, step: 461, loss: 0.049047548323869705, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:34.990852, step: 462, loss: 0.051047272980213165, acc: 0.9922, auc: 0.9998, precision: 0.9865, recall: 1.0\n",
      "2019-01-02T18:19:35.166674, step: 463, loss: 0.07840365171432495, acc: 0.9844, auc: 0.9998, precision: 0.9733, recall: 1.0\n",
      "2019-01-02T18:19:35.343622, step: 464, loss: 0.0457465685904026, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-01-02T18:19:35.522493, step: 465, loss: 0.04806065186858177, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-01-02T18:19:35.695260, step: 466, loss: 0.11620881408452988, acc: 0.9297, auc: 1.0, precision: 1.0, recall: 0.8548\n",
      "2019-01-02T18:19:35.869129, step: 467, loss: 0.04583881422877312, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9531\n",
      "2019-01-02T18:19:36.041814, step: 468, loss: 0.021236322820186615, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-01-02T18:19:36.234346, step: 469, loss: 0.023670976981520653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:36.413115, step: 470, loss: 0.08925067633390427, acc: 0.9922, auc: 0.9936, precision: 0.9841, recall: 1.0\n",
      "2019-01-02T18:19:36.585975, step: 471, loss: 0.018322531133890152, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:36.762188, step: 472, loss: 0.016472360119223595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:36.938282, step: 473, loss: 0.01598314940929413, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:37.112516, step: 474, loss: 0.015033887699246407, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:37.280934, step: 475, loss: 0.01743653044104576, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:37.451856, step: 476, loss: 0.020624401047825813, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-01-02T18:19:37.629932, step: 477, loss: 0.02570558339357376, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:37.807174, step: 478, loss: 0.025382906198501587, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-01-02T18:19:37.975468, step: 479, loss: 0.016962839290499687, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:38.155923, step: 480, loss: 0.012172404676675797, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:38.327203, step: 481, loss: 0.025687461718916893, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:38.508416, step: 482, loss: 0.026592694222927094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:38.690320, step: 483, loss: 0.01449228823184967, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:38.858549, step: 484, loss: 0.028825074434280396, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9808\n",
      "2019-01-02T18:19:39.030399, step: 485, loss: 0.025226004421710968, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:39.204313, step: 486, loss: 0.011441560462117195, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:39.377840, step: 487, loss: 0.020546862855553627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:39.550877, step: 488, loss: 0.022764557972550392, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:39.722609, step: 489, loss: 0.02332882024347782, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-01-02T18:19:39.894752, step: 490, loss: 0.006915593519806862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:40.073432, step: 491, loss: 0.012009065598249435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T18:19:40.242977, step: 492, loss: 0.008098553866147995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:40.419691, step: 493, loss: 0.02375393733382225, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:40.589147, step: 494, loss: 0.01891218312084675, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:40.762909, step: 495, loss: 0.01117563433945179, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:40.936148, step: 496, loss: 0.010379849001765251, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:41.104146, step: 497, loss: 0.011747412383556366, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:41.269271, step: 498, loss: 0.015958203002810478, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:41.442515, step: 499, loss: 0.011247679591178894, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-01-02T18:19:41.618813, step: 500, loss: 0.014395460486412048, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = Transformer(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "#         saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
