{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 6\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 128]  # 双层LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = (len(x) - 1) // batchSize\n",
    "\n",
    "        for i in range(numBatches - 1):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTM(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "        concatedOutput = tf.concat(outputs, 2)\n",
    "        \n",
    "        # 去除最后时间步的输出作为全连接的输入\n",
    "        finalOutput = concatedOutput[:, -1, :]\n",
    "        \n",
    "        outputSize = config.model.hiddenSizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(finalOutput, [-1, outputSize])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_1/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_1/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-28T11:10:04.575614, step: 1, loss: 0.6812096238136292, acc: 0.4766, auc: 0.5318, precision: 0.6667, recall: 0.1127\n",
      "2018-12-28T11:10:05.176606, step: 2, loss: 0.709476113319397, acc: 0.5469, auc: 0.5636, precision: 0.5556, recall: 0.5385\n",
      "2018-12-28T11:10:05.812405, step: 3, loss: 0.6853252649307251, acc: 0.5156, auc: 0.5694, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:06.447824, step: 4, loss: 0.7424368858337402, acc: 0.5156, auc: 0.4453, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:07.106894, step: 5, loss: 0.7082134485244751, acc: 0.5391, auc: 0.4884, precision: 1.0, recall: 0.0328\n",
      "2018-12-28T11:10:07.741241, step: 6, loss: 0.688674807548523, acc: 0.5703, auc: 0.5513, precision: 0.3333, recall: 0.0185\n",
      "2018-12-28T11:10:08.408463, step: 7, loss: 0.702623724937439, acc: 0.4453, auc: 0.4929, precision: 0.3846, recall: 0.0735\n",
      "2018-12-28T11:10:08.974786, step: 8, loss: 0.7212009429931641, acc: 0.4922, auc: 0.4996, precision: 0.4545, recall: 0.1587\n",
      "2018-12-28T11:10:09.530058, step: 9, loss: 0.7486785054206848, acc: 0.4688, auc: 0.4569, precision: 0.375, recall: 0.2\n",
      "2018-12-28T11:10:10.117091, step: 10, loss: 0.717720627784729, acc: 0.4375, auc: 0.4571, precision: 0.4118, recall: 0.1014\n",
      "2018-12-28T11:10:10.759922, step: 11, loss: 0.7097097635269165, acc: 0.5, auc: 0.5009, precision: 0.2857, recall: 0.0328\n",
      "2018-12-28T11:10:11.410371, step: 12, loss: 0.7013558149337769, acc: 0.4766, auc: 0.5228, precision: 0.25, recall: 0.0154\n",
      "2018-12-28T11:10:12.023537, step: 13, loss: 0.7049528360366821, acc: 0.5234, auc: 0.4774, precision: 0.5, recall: 0.0328\n",
      "2018-12-28T11:10:12.586899, step: 14, loss: 0.7421671748161316, acc: 0.4453, auc: 0.3902, precision: 0.5, recall: 0.0563\n",
      "2018-12-28T11:10:13.164586, step: 15, loss: 0.7162270545959473, acc: 0.5078, auc: 0.4854, precision: 0.4167, recall: 0.082\n",
      "2018-12-28T11:10:13.733735, step: 16, loss: 0.7046998143196106, acc: 0.4531, auc: 0.514, precision: 0.5556, recall: 0.0704\n",
      "2018-12-28T11:10:14.287044, step: 17, loss: 0.7100898623466492, acc: 0.4766, auc: 0.4821, precision: 0.375, recall: 0.0462\n",
      "2018-12-28T11:10:14.819492, step: 18, loss: 0.7297284007072449, acc: 0.4297, auc: 0.4262, precision: 0.3529, recall: 0.0882\n",
      "2018-12-28T11:10:15.382019, step: 19, loss: 0.7054229974746704, acc: 0.4609, auc: 0.4634, precision: 0.75, recall: 0.12\n",
      "2018-12-28T11:10:15.967119, step: 20, loss: 0.7240671515464783, acc: 0.6484, auc: 0.5506, precision: 0.5455, recall: 0.2553\n",
      "2018-12-28T11:10:16.528198, step: 21, loss: 0.7009056210517883, acc: 0.4766, auc: 0.5021, precision: 0.5625, recall: 0.1304\n",
      "2018-12-28T11:10:17.128705, step: 22, loss: 0.6955563426017761, acc: 0.4453, auc: 0.5278, precision: 0.4, recall: 0.1194\n",
      "2018-12-28T11:10:17.708110, step: 23, loss: 0.7302250266075134, acc: 0.5, auc: 0.4279, precision: 0.3333, recall: 0.0847\n",
      "2018-12-28T11:10:18.262272, step: 24, loss: 0.704120934009552, acc: 0.5391, auc: 0.5207, precision: 0.4444, recall: 0.069\n",
      "2018-12-28T11:10:18.839493, step: 25, loss: 0.7177605032920837, acc: 0.4922, auc: 0.4292, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:19.422715, step: 26, loss: 0.7441340684890747, acc: 0.4453, auc: 0.3766, precision: 0.4, recall: 0.0286\n",
      "2018-12-28T11:10:19.974427, step: 27, loss: 0.6720489263534546, acc: 0.5391, auc: 0.6007, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:20.540577, step: 28, loss: 0.7065528035163879, acc: 0.4375, auc: 0.526, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:21.124471, step: 29, loss: 0.7034983038902283, acc: 0.5078, auc: 0.5216, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:21.693560, step: 30, loss: 0.6886258125305176, acc: 0.5547, auc: 0.5352, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:22.260158, step: 31, loss: 0.7181134223937988, acc: 0.4844, auc: 0.4419, precision: 1.0, recall: 0.0149\n",
      "2018-12-28T11:10:22.821001, step: 32, loss: 0.6956238746643066, acc: 0.5547, auc: 0.5138, precision: 0.6, recall: 0.0517\n",
      "2018-12-28T11:10:23.403299, step: 33, loss: 0.7115142941474915, acc: 0.5234, auc: 0.4564, precision: 0.6, recall: 0.0952\n",
      "2018-12-28T11:10:23.968893, step: 34, loss: 0.7329099774360657, acc: 0.5156, auc: 0.4169, precision: 0.5, recall: 0.0968\n",
      "2018-12-28T11:10:24.514413, step: 35, loss: 0.6984835863113403, acc: 0.4844, auc: 0.5275, precision: 0.5556, recall: 0.0746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:10:25.104321, step: 36, loss: 0.6969190239906311, acc: 0.5312, auc: 0.5242, precision: 0.75, recall: 0.0484\n",
      "2018-12-28T11:10:25.670206, step: 37, loss: 0.7040481567382812, acc: 0.5469, auc: 0.4711, precision: 0.75, recall: 0.05\n",
      "2018-12-28T11:10:26.245076, step: 38, loss: 0.6903712153434753, acc: 0.5234, auc: 0.5187, precision: 0.75, recall: 0.0476\n",
      "2018-12-28T11:10:26.826581, step: 39, loss: 0.6781790256500244, acc: 0.5312, auc: 0.6044, precision: 1.0, recall: 0.0164\n",
      "2018-12-28T11:10:27.362435, step: 40, loss: 0.7109823226928711, acc: 0.4219, auc: 0.4613, precision: 0.2857, recall: 0.0282\n",
      "2018-12-28T11:10:27.957127, step: 41, loss: 0.7060298919677734, acc: 0.4922, auc: 0.4846, precision: 0.8, recall: 0.0588\n",
      "2018-12-28T11:10:28.508838, step: 42, loss: 0.6989051103591919, acc: 0.4609, auc: 0.5196, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:29.096959, step: 43, loss: 0.6895378828048706, acc: 0.4219, auc: 0.5246, precision: 0.6667, recall: 0.0779\n",
      "2018-12-28T11:10:29.736931, step: 44, loss: 0.7128094434738159, acc: 0.4531, auc: 0.4668, precision: 0.375, recall: 0.0909\n",
      "2018-12-28T11:10:30.394028, step: 45, loss: 0.7140514850616455, acc: 0.5625, auc: 0.4813, precision: 0.5385, recall: 0.1228\n",
      "2018-12-28T11:10:31.047499, step: 46, loss: 0.7092388868331909, acc: 0.5156, auc: 0.4965, precision: 0.4545, recall: 0.082\n",
      "2018-12-28T11:10:31.669757, step: 47, loss: 0.7014179825782776, acc: 0.5703, auc: 0.5345, precision: 0.7, recall: 0.1186\n",
      "2018-12-28T11:10:32.264490, step: 48, loss: 0.7136346101760864, acc: 0.5, auc: 0.4735, precision: 0.25, recall: 0.0333\n",
      "2018-12-28T11:10:32.854923, step: 49, loss: 0.6973801851272583, acc: 0.5234, auc: 0.5159, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:33.449928, step: 50, loss: 0.700086772441864, acc: 0.5156, auc: 0.4916, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:34.043143, step: 51, loss: 0.7104982733726501, acc: 0.4766, auc: 0.4801, precision: 0.5, recall: 0.0149\n",
      "2018-12-28T11:10:34.604580, step: 52, loss: 0.7023853063583374, acc: 0.5469, auc: 0.467, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:35.148694, step: 53, loss: 0.7061363458633423, acc: 0.5078, auc: 0.4763, precision: 1.0, recall: 0.0156\n",
      "2018-12-28T11:10:35.808305, step: 54, loss: 0.6929821968078613, acc: 0.5391, auc: 0.5098, precision: 1.0, recall: 0.0167\n",
      "2018-12-28T11:10:36.399995, step: 55, loss: 0.7107503414154053, acc: 0.5234, auc: 0.4646, precision: 0.5, recall: 0.0164\n",
      "2018-12-28T11:10:36.974144, step: 56, loss: 0.714653730392456, acc: 0.4062, auc: 0.5167, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:37.515064, step: 57, loss: 0.6981219053268433, acc: 0.4922, auc: 0.5118, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:38.075450, step: 58, loss: 0.7133572101593018, acc: 0.5156, auc: 0.4339, precision: 1.0, recall: 0.0159\n",
      "2018-12-28T11:10:38.677438, step: 59, loss: 0.714688777923584, acc: 0.4688, auc: 0.4308, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:39.283673, step: 60, loss: 0.7069039344787598, acc: 0.5703, auc: 0.4598, precision: 0.5, recall: 0.0182\n",
      "2018-12-28T11:10:39.831853, step: 61, loss: 0.6736870408058167, acc: 0.4609, auc: 0.606, precision: 1.0, recall: 0.0548\n",
      "2018-12-28T11:10:40.432193, step: 62, loss: 0.6945196390151978, acc: 0.4531, auc: 0.5285, precision: 0.8, recall: 0.0548\n",
      "2018-12-28T11:10:41.008098, step: 63, loss: 0.7066715955734253, acc: 0.5312, auc: 0.5315, precision: 0.2857, recall: 0.0351\n",
      "2018-12-28T11:10:41.650479, step: 64, loss: 0.6960400342941284, acc: 0.4688, auc: 0.5262, precision: 0.5, recall: 0.0588\n",
      "2018-12-28T11:10:42.260485, step: 65, loss: 0.6970969438552856, acc: 0.5156, auc: 0.5261, precision: 0.6667, recall: 0.0625\n",
      "2018-12-28T11:10:42.855581, step: 66, loss: 0.6873607635498047, acc: 0.4531, auc: 0.5298, precision: 0.7143, recall: 0.0685\n",
      "2018-12-28T11:10:43.398418, step: 67, loss: 0.7196774482727051, acc: 0.5703, auc: 0.4631, precision: 0.2857, recall: 0.0385\n",
      "2018-12-28T11:10:43.955655, step: 68, loss: 0.6992889642715454, acc: 0.5312, auc: 0.532, precision: 0.75, recall: 0.0484\n",
      "2018-12-28T11:10:44.566415, step: 69, loss: 0.7203182578086853, acc: 0.5781, auc: 0.4162, precision: 0.5, recall: 0.0556\n",
      "2018-12-28T11:10:45.147949, step: 70, loss: 0.6937790513038635, acc: 0.4922, auc: 0.506, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:45.740303, step: 71, loss: 0.6958060264587402, acc: 0.5, auc: 0.5286, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:46.347291, step: 72, loss: 0.7105207443237305, acc: 0.4844, auc: 0.4438, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:47.002493, step: 73, loss: 0.7039487361907959, acc: 0.5312, auc: 0.476, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:47.556838, step: 74, loss: 0.7335430383682251, acc: 0.4531, auc: 0.3904, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:48.173709, step: 75, loss: 0.6948654651641846, acc: 0.5156, auc: 0.5156, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:48.759736, step: 76, loss: 0.7023720741271973, acc: 0.5781, auc: 0.4137, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:49.349910, step: 77, loss: 0.7046109437942505, acc: 0.5391, auc: 0.4188, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:49.929153, step: 78, loss: 0.7139872908592224, acc: 0.4922, auc: 0.4435, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:50.488853, step: 79, loss: 0.694670557975769, acc: 0.4609, auc: 0.5804, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:51.052980, step: 80, loss: 0.6910110116004944, acc: 0.5234, auc: 0.5297, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:51.614590, step: 81, loss: 0.7132189869880676, acc: 0.4375, auc: 0.4688, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:52.223463, step: 82, loss: 0.7174383401870728, acc: 0.4766, auc: 0.4201, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:52.818928, step: 83, loss: 0.7080414891242981, acc: 0.5234, auc: 0.4649, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:53.399253, step: 84, loss: 0.6936361789703369, acc: 0.4609, auc: 0.5225, precision: 0.5, recall: 0.0145\n",
      "2018-12-28T11:10:54.006955, step: 85, loss: 0.7004450559616089, acc: 0.4688, auc: 0.4673, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:54.596007, step: 86, loss: 0.7123331427574158, acc: 0.4922, auc: 0.4298, precision: 0.5, recall: 0.0462\n",
      "2018-12-28T11:10:55.194486, step: 87, loss: 0.7110958695411682, acc: 0.5156, auc: 0.4705, precision: 0.3333, recall: 0.0164\n",
      "2018-12-28T11:10:55.787112, step: 88, loss: 0.6979602575302124, acc: 0.5078, auc: 0.4738, precision: 1.0, recall: 0.0735\n",
      "2018-12-28T11:10:56.350591, step: 89, loss: 0.7172069549560547, acc: 0.5547, auc: 0.4601, precision: 0.3333, recall: 0.0179\n",
      "2018-12-28T11:10:56.890330, step: 90, loss: 0.703647255897522, acc: 0.5078, auc: 0.4799, precision: 0.5, recall: 0.0159\n",
      "2018-12-28T11:10:57.488171, step: 91, loss: 0.7008603811264038, acc: 0.5078, auc: 0.4939, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:58.055326, step: 92, loss: 0.6857970356941223, acc: 0.5, auc: 0.5869, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:58.603875, step: 93, loss: 0.6951417326927185, acc: 0.5312, auc: 0.5184, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:59.143340, step: 94, loss: 0.7039379477500916, acc: 0.5312, auc: 0.4667, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:10:59.705860, step: 95, loss: 0.6959713697433472, acc: 0.4375, auc: 0.5372, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:00.322011, step: 96, loss: 0.7066965103149414, acc: 0.4844, auc: 0.4443, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:00.933082, step: 97, loss: 0.7024493217468262, acc: 0.4922, auc: 0.4789, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:01.523661, step: 98, loss: 0.7003539800643921, acc: 0.5, auc: 0.5071, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:02.217233, step: 99, loss: 0.699806272983551, acc: 0.5078, auc: 0.4969, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:02.797236, step: 100, loss: 0.706168532371521, acc: 0.4766, auc: 0.4735, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:11:25.054809, step: 100, loss: 0.6952279837507951, acc: 0.4942421052631579, auc: 0.5343605263157896, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:25.642414, step: 101, loss: 0.6862915754318237, acc: 0.5234, auc: 0.5674, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:26.239337, step: 102, loss: 0.6927746534347534, acc: 0.4844, auc: 0.5381, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:26.799255, step: 103, loss: 0.6804683208465576, acc: 0.5156, auc: 0.5914, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:11:27.337614, step: 104, loss: 0.6819145679473877, acc: 0.5469, auc: 0.5796, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:27.919320, step: 105, loss: 0.690883994102478, acc: 0.5859, auc: 0.5215, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:28.537239, step: 106, loss: 0.6984807252883911, acc: 0.4531, auc: 0.5096, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:29.167202, step: 107, loss: 0.701971173286438, acc: 0.4844, auc: 0.4933, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:29.765474, step: 108, loss: 0.7002499103546143, acc: 0.4688, auc: 0.4804, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:30.313088, step: 109, loss: 0.6882403492927551, acc: 0.5391, auc: 0.552, precision: 1.0, recall: 0.0167\n",
      "2018-12-28T11:11:30.870182, step: 110, loss: 0.692074179649353, acc: 0.4922, auc: 0.5521, precision: 0.5, recall: 0.0154\n",
      "2018-12-28T11:11:31.480340, step: 111, loss: 0.6981812715530396, acc: 0.4688, auc: 0.4713, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:32.058076, step: 112, loss: 0.6902706623077393, acc: 0.5625, auc: 0.5528, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:32.605888, step: 113, loss: 0.6813910007476807, acc: 0.4766, auc: 0.6053, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:33.171574, step: 114, loss: 0.6915236711502075, acc: 0.5391, auc: 0.5137, precision: 1.0, recall: 0.0167\n",
      "2018-12-28T11:11:33.772263, step: 115, loss: 0.6969997882843018, acc: 0.5156, auc: 0.475, precision: 1.0, recall: 0.0159\n",
      "2018-12-28T11:11:34.327989, step: 116, loss: 0.7064288258552551, acc: 0.5391, auc: 0.4299, precision: 1.0, recall: 0.0167\n",
      "2018-12-28T11:11:34.875664, step: 117, loss: 0.6954895257949829, acc: 0.6328, auc: 0.5296, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:35.453212, step: 118, loss: 0.6993862986564636, acc: 0.5078, auc: 0.4698, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:36.046756, step: 119, loss: 0.6982238292694092, acc: 0.5312, auc: 0.4728, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:36.652011, step: 120, loss: 0.7061951160430908, acc: 0.4297, auc: 0.4954, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:37.246394, step: 121, loss: 0.6921941637992859, acc: 0.5, auc: 0.5457, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:37.807337, step: 122, loss: 0.6874638795852661, acc: 0.5156, auc: 0.5549, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:38.381041, step: 123, loss: 0.6926333904266357, acc: 0.5547, auc: 0.4885, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:38.947641, step: 124, loss: 0.6966414451599121, acc: 0.5391, auc: 0.4768, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:39.528091, step: 125, loss: 0.6940231323242188, acc: 0.5156, auc: 0.5239, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:40.199346, step: 126, loss: 0.7035350799560547, acc: 0.4922, auc: 0.474, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:40.766590, step: 127, loss: 0.7102618217468262, acc: 0.4766, auc: 0.4128, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:41.318368, step: 128, loss: 0.6905369758605957, acc: 0.5469, auc: 0.531, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:41.910967, step: 129, loss: 0.6991567015647888, acc: 0.4609, auc: 0.4871, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:42.501646, step: 130, loss: 0.7019658088684082, acc: 0.5078, auc: 0.4508, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:43.060222, step: 131, loss: 0.6977071166038513, acc: 0.4844, auc: 0.4768, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:43.733624, step: 132, loss: 0.6962393522262573, acc: 0.4141, auc: 0.4936, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:44.321251, step: 133, loss: 0.6915000677108765, acc: 0.4609, auc: 0.4979, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:44.908749, step: 134, loss: 0.6944717168807983, acc: 0.4844, auc: 0.5225, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:45.617503, step: 135, loss: 0.6867311000823975, acc: 0.4297, auc: 0.536, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:46.194333, step: 136, loss: 0.6779043078422546, acc: 0.4922, auc: 0.6074, precision: 1.0, recall: 0.0441\n",
      "2018-12-28T11:11:46.813079, step: 137, loss: 0.7032816410064697, acc: 0.4844, auc: 0.4585, precision: 0.5, recall: 0.0303\n",
      "2018-12-28T11:11:47.410981, step: 138, loss: 0.7117735147476196, acc: 0.5391, auc: 0.4596, precision: 0.6667, recall: 0.0333\n",
      "2018-12-28T11:11:47.994598, step: 139, loss: 0.6984866857528687, acc: 0.5469, auc: 0.5806, precision: 0.6667, recall: 0.0667\n",
      "2018-12-28T11:11:48.561748, step: 140, loss: 0.6834394931793213, acc: 0.4531, auc: 0.5107, precision: 0.75, recall: 0.0417\n",
      "2018-12-28T11:11:49.169758, step: 141, loss: 0.6920047402381897, acc: 0.5078, auc: 0.5442, precision: 0.6667, recall: 0.0909\n",
      "2018-12-28T11:11:49.759387, step: 142, loss: 0.7004465460777283, acc: 0.5078, auc: 0.4825, precision: 0.5, recall: 0.0317\n",
      "2018-12-28T11:11:50.317758, step: 143, loss: 0.7033668160438538, acc: 0.5156, auc: 0.4979, precision: 0.3333, recall: 0.0164\n",
      "2018-12-28T11:11:51.013842, step: 144, loss: 0.6955924034118652, acc: 0.4609, auc: 0.4873, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:51.595748, step: 145, loss: 0.6929733753204346, acc: 0.4844, auc: 0.5122, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:52.193650, step: 146, loss: 0.7037482857704163, acc: 0.5, auc: 0.4502, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:52.789161, step: 147, loss: 0.7041606307029724, acc: 0.4531, auc: 0.4369, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:53.387177, step: 148, loss: 0.6821370720863342, acc: 0.4219, auc: 0.6034, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:53.967287, step: 149, loss: 0.6922075748443604, acc: 0.5, auc: 0.5164, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:54.546063, step: 150, loss: 0.7003387212753296, acc: 0.5156, auc: 0.4545, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:55.178114, step: 151, loss: 0.6897470951080322, acc: 0.5469, auc: 0.5522, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:55.855155, step: 152, loss: 0.7045766711235046, acc: 0.4609, auc: 0.4129, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:56.502583, step: 153, loss: 0.6877671480178833, acc: 0.4531, auc: 0.5655, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:57.092942, step: 154, loss: 0.6885194778442383, acc: 0.5078, auc: 0.5375, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:11:57.692826, step: 155, loss: 0.6928894519805908, acc: 0.5234, auc: 0.5207, precision: 0.0, recall: 0.0\n",
      "start training model\n",
      "2018-12-28T11:11:58.367667, step: 156, loss: 0.6957575678825378, acc: 0.4141, auc: 0.5238, precision: 1.0, recall: 0.0132\n",
      "2018-12-28T11:11:58.980854, step: 157, loss: 0.7008877396583557, acc: 0.5547, auc: 0.4677, precision: 1.0, recall: 0.0172\n",
      "2018-12-28T11:11:59.619565, step: 158, loss: 0.694202721118927, acc: 0.4688, auc: 0.5282, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:00.249135, step: 159, loss: 0.6862777471542358, acc: 0.5234, auc: 0.5855, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:00.836127, step: 160, loss: 0.6858924627304077, acc: 0.5156, auc: 0.5892, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:01.388427, step: 161, loss: 0.6846189498901367, acc: 0.5156, auc: 0.5671, precision: 1.0, recall: 0.0312\n",
      "2018-12-28T11:12:01.970755, step: 162, loss: 0.6907437443733215, acc: 0.5234, auc: 0.5286, precision: 1.0, recall: 0.0161\n",
      "2018-12-28T11:12:02.524451, step: 163, loss: 0.697202742099762, acc: 0.4922, auc: 0.4916, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:03.100682, step: 164, loss: 0.7058748602867126, acc: 0.5703, auc: 0.4284, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:03.673819, step: 165, loss: 0.6969423294067383, acc: 0.5156, auc: 0.499, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:04.301633, step: 166, loss: 0.6952890157699585, acc: 0.4766, auc: 0.4927, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:04.866717, step: 167, loss: 0.6809672713279724, acc: 0.5234, auc: 0.6134, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:05.435420, step: 168, loss: 0.6900334358215332, acc: 0.5234, auc: 0.5258, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:06.145891, step: 169, loss: 0.7002581357955933, acc: 0.4531, auc: 0.4702, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:06.741863, step: 170, loss: 0.6948077082633972, acc: 0.5469, auc: 0.4643, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:07.294547, step: 171, loss: 0.6874376535415649, acc: 0.5234, auc: 0.5664, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:07.879544, step: 172, loss: 0.702304482460022, acc: 0.5078, auc: 0.4479, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:12:08.478171, step: 173, loss: 0.7027219533920288, acc: 0.4766, auc: 0.4509, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:09.022932, step: 174, loss: 0.7030916810035706, acc: 0.4531, auc: 0.4589, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:09.632352, step: 175, loss: 0.6936715841293335, acc: 0.4453, auc: 0.551, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:10.176251, step: 176, loss: 0.7002276182174683, acc: 0.4844, auc: 0.467, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:10.753884, step: 177, loss: 0.6916866898536682, acc: 0.5156, auc: 0.5389, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:11.362739, step: 178, loss: 0.6936594247817993, acc: 0.4844, auc: 0.5196, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:11.923454, step: 179, loss: 0.6951043605804443, acc: 0.5391, auc: 0.5097, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:12.529206, step: 180, loss: 0.6920321583747864, acc: 0.5312, auc: 0.5203, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:13.118837, step: 181, loss: 0.6865301132202148, acc: 0.5234, auc: 0.5892, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:13.723341, step: 182, loss: 0.6899546384811401, acc: 0.4922, auc: 0.5495, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:14.284338, step: 183, loss: 0.6851949691772461, acc: 0.4922, auc: 0.5663, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:14.860457, step: 184, loss: 0.7027688026428223, acc: 0.4531, auc: 0.4318, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:15.427925, step: 185, loss: 0.6933248043060303, acc: 0.5234, auc: 0.5485, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:16.014996, step: 186, loss: 0.7005778551101685, acc: 0.5469, auc: 0.501, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:16.573643, step: 187, loss: 0.6845884323120117, acc: 0.4453, auc: 0.5799, precision: 1.0, recall: 0.0139\n",
      "2018-12-28T11:12:17.137036, step: 188, loss: 0.6890778541564941, acc: 0.5078, auc: 0.5548, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:17.705708, step: 189, loss: 0.6978020071983337, acc: 0.5469, auc: 0.5079, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:18.264909, step: 190, loss: 0.6874158978462219, acc: 0.5078, auc: 0.5812, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:18.828625, step: 191, loss: 0.6997234225273132, acc: 0.5703, auc: 0.4889, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:19.409137, step: 192, loss: 0.695195198059082, acc: 0.4922, auc: 0.5226, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:19.965953, step: 193, loss: 0.7046831250190735, acc: 0.4531, auc: 0.464, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:20.539695, step: 194, loss: 0.6949388980865479, acc: 0.5234, auc: 0.5192, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:21.124333, step: 195, loss: 0.7011086940765381, acc: 0.4922, auc: 0.4686, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:21.738194, step: 196, loss: 0.6856321692466736, acc: 0.4062, auc: 0.6032, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:22.350913, step: 197, loss: 0.6972131133079529, acc: 0.5469, auc: 0.4722, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:22.941877, step: 198, loss: 0.6925457715988159, acc: 0.4688, auc: 0.5544, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:23.584338, step: 199, loss: 0.6893699765205383, acc: 0.5391, auc: 0.5554, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:24.163962, step: 200, loss: 0.7002512216567993, acc: 0.5625, auc: 0.4509, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:12:46.126203, step: 200, loss: 0.691396281907433, acc: 0.49609736842105256, auc: 0.5936842105263157, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:46.740359, step: 201, loss: 0.6909485459327698, acc: 0.5078, auc: 0.5201, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:47.337831, step: 202, loss: 0.6909886598587036, acc: 0.5625, auc: 0.4948, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:47.903321, step: 203, loss: 0.6958222389221191, acc: 0.5, auc: 0.4836, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:48.476246, step: 204, loss: 0.7062541842460632, acc: 0.4609, auc: 0.4439, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:49.030376, step: 205, loss: 0.6921259164810181, acc: 0.5312, auc: 0.5208, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:49.580723, step: 206, loss: 0.6978458762168884, acc: 0.5156, auc: 0.4868, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:50.159806, step: 207, loss: 0.7088220715522766, acc: 0.4766, auc: 0.4204, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:50.709203, step: 208, loss: 0.7010674476623535, acc: 0.5078, auc: 0.4659, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:51.295637, step: 209, loss: 0.6963814496994019, acc: 0.5078, auc: 0.5006, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:51.991850, step: 210, loss: 0.6904455423355103, acc: 0.5234, auc: 0.5128, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:52.592722, step: 211, loss: 0.6890808343887329, acc: 0.4844, auc: 0.5589, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:53.152282, step: 212, loss: 0.6927465200424194, acc: 0.4844, auc: 0.5447, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:53.745744, step: 213, loss: 0.6956342458724976, acc: 0.4922, auc: 0.5121, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:54.336135, step: 214, loss: 0.6970570087432861, acc: 0.4609, auc: 0.5109, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:54.918415, step: 215, loss: 0.7086264491081238, acc: 0.4922, auc: 0.4145, precision: 1.0, recall: 0.0152\n",
      "2018-12-28T11:12:55.510537, step: 216, loss: 0.6917940974235535, acc: 0.5156, auc: 0.5455, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:56.126897, step: 217, loss: 0.6977161169052124, acc: 0.5156, auc: 0.4954, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:56.743728, step: 218, loss: 0.7009462118148804, acc: 0.4297, auc: 0.4418, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:57.330574, step: 219, loss: 0.6892277002334595, acc: 0.5, auc: 0.5618, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:57.997827, step: 220, loss: 0.7026274800300598, acc: 0.5703, auc: 0.47, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:58.567144, step: 221, loss: 0.692404568195343, acc: 0.5078, auc: 0.5209, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:59.152289, step: 222, loss: 0.7014287114143372, acc: 0.5, auc: 0.437, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:12:59.700338, step: 223, loss: 0.6897639632225037, acc: 0.5078, auc: 0.5768, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:00.294074, step: 224, loss: 0.694267749786377, acc: 0.5391, auc: 0.5377, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:00.846863, step: 225, loss: 0.6947009563446045, acc: 0.5312, auc: 0.4968, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:01.400626, step: 226, loss: 0.689583420753479, acc: 0.4766, auc: 0.5486, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:01.951415, step: 227, loss: 0.6930983066558838, acc: 0.5312, auc: 0.4941, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:02.496848, step: 228, loss: 0.6961010098457336, acc: 0.4609, auc: 0.5085, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:03.063803, step: 229, loss: 0.6928675174713135, acc: 0.4766, auc: 0.5251, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:03.646398, step: 230, loss: 0.6909030079841614, acc: 0.4219, auc: 0.5593, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:04.194126, step: 231, loss: 0.6866657137870789, acc: 0.5078, auc: 0.6095, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:04.883492, step: 232, loss: 0.6994967460632324, acc: 0.4453, auc: 0.4566, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:05.506953, step: 233, loss: 0.6877264976501465, acc: 0.4922, auc: 0.5702, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:06.092854, step: 234, loss: 0.6963506937026978, acc: 0.4766, auc: 0.493, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:06.703091, step: 235, loss: 0.6909508109092712, acc: 0.5781, auc: 0.5723, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:07.294294, step: 236, loss: 0.695878803730011, acc: 0.5469, auc: 0.4985, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:07.892253, step: 237, loss: 0.6994246244430542, acc: 0.5703, auc: 0.4645, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:08.464410, step: 238, loss: 0.69281405210495, acc: 0.5469, auc: 0.5062, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:09.050924, step: 239, loss: 0.6881468296051025, acc: 0.5547, auc: 0.5681, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:09.597917, step: 240, loss: 0.6954793334007263, acc: 0.4375, auc: 0.5211, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:13:10.197806, step: 241, loss: 0.6856728792190552, acc: 0.5469, auc: 0.5665, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:10.844082, step: 242, loss: 0.6858155727386475, acc: 0.5, auc: 0.5745, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:11.465172, step: 243, loss: 0.6908243298530579, acc: 0.5078, auc: 0.5404, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:12.149719, step: 244, loss: 0.6916531920433044, acc: 0.5234, auc: 0.5185, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:12.726067, step: 245, loss: 0.688926100730896, acc: 0.5078, auc: 0.5558, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:13.276367, step: 246, loss: 0.6914622783660889, acc: 0.5, auc: 0.5229, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:13.860525, step: 247, loss: 0.6861144304275513, acc: 0.5234, auc: 0.5823, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:14.436833, step: 248, loss: 0.7078530788421631, acc: 0.4453, auc: 0.4692, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:15.009585, step: 249, loss: 0.6886448860168457, acc: 0.5312, auc: 0.5353, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:15.580626, step: 250, loss: 0.6879182457923889, acc: 0.4609, auc: 0.6166, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:16.192457, step: 251, loss: 0.7068368792533875, acc: 0.4531, auc: 0.4121, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:16.759355, step: 252, loss: 0.686741828918457, acc: 0.4922, auc: 0.5556, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:17.310305, step: 253, loss: 0.6891580820083618, acc: 0.5547, auc: 0.5661, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:17.892228, step: 254, loss: 0.6896384954452515, acc: 0.4453, auc: 0.5238, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:18.506619, step: 255, loss: 0.6902174949645996, acc: 0.4531, auc: 0.5172, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:19.129221, step: 256, loss: 0.6922807693481445, acc: 0.4609, auc: 0.4891, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:19.777953, step: 257, loss: 0.6996266841888428, acc: 0.5156, auc: 0.4611, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:20.381734, step: 258, loss: 0.6891676187515259, acc: 0.4375, auc: 0.4809, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:20.954995, step: 259, loss: 0.6822226643562317, acc: 0.4688, auc: 0.6027, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:21.511589, step: 260, loss: 0.6982234716415405, acc: 0.5312, auc: 0.5179, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:22.102229, step: 261, loss: 0.6954206824302673, acc: 0.4922, auc: 0.5265, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:22.689601, step: 262, loss: 0.6888586282730103, acc: 0.4453, auc: 0.5026, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:23.289003, step: 263, loss: 0.7154315710067749, acc: 0.5859, auc: 0.4644, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:23.894631, step: 264, loss: 0.6889323592185974, acc: 0.4688, auc: 0.5346, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:24.505996, step: 265, loss: 0.6872293949127197, acc: 0.5078, auc: 0.6005, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:25.164591, step: 266, loss: 0.6990179419517517, acc: 0.5547, auc: 0.5209, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:25.772352, step: 267, loss: 0.6863223910331726, acc: 0.4844, auc: 0.5555, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:26.367398, step: 268, loss: 0.6941713094711304, acc: 0.5, auc: 0.5054, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:26.947319, step: 269, loss: 0.6934225559234619, acc: 0.5234, auc: 0.5454, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:27.610866, step: 270, loss: 0.6923068761825562, acc: 0.5078, auc: 0.5302, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:28.212001, step: 271, loss: 0.6800409555435181, acc: 0.5078, auc: 0.6232, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:28.829527, step: 272, loss: 0.6863304376602173, acc: 0.5391, auc: 0.5434, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:29.427320, step: 273, loss: 0.6867535710334778, acc: 0.5625, auc: 0.5221, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:30.004373, step: 274, loss: 0.7101912498474121, acc: 0.4062, auc: 0.4557, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:30.576758, step: 275, loss: 0.6809525489807129, acc: 0.5156, auc: 0.6493, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:31.214026, step: 276, loss: 0.7054184675216675, acc: 0.3984, auc: 0.5289, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:31.798882, step: 277, loss: 0.691699743270874, acc: 0.5312, auc: 0.5007, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:32.407556, step: 278, loss: 0.67917400598526, acc: 0.5859, auc: 0.5628, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:32.996664, step: 279, loss: 0.6931259036064148, acc: 0.5234, auc: 0.5214, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:33.570021, step: 280, loss: 0.6854264736175537, acc: 0.4844, auc: 0.5525, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:34.162644, step: 281, loss: 0.6870328783988953, acc: 0.5625, auc: 0.5352, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:34.898013, step: 282, loss: 0.687943696975708, acc: 0.5234, auc: 0.5818, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:35.530269, step: 283, loss: 0.7001160383224487, acc: 0.4922, auc: 0.4625, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:36.184443, step: 284, loss: 0.688931941986084, acc: 0.5234, auc: 0.517, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:36.767842, step: 285, loss: 0.6977880001068115, acc: 0.4062, auc: 0.58, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:37.334550, step: 286, loss: 0.6844639778137207, acc: 0.4531, auc: 0.58, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:37.978908, step: 287, loss: 0.6938819885253906, acc: 0.4375, auc: 0.5092, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:38.612944, step: 288, loss: 0.700234591960907, acc: 0.4844, auc: 0.4611, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:39.272909, step: 289, loss: 0.6866798400878906, acc: 0.4844, auc: 0.5334, precision: 1.0, recall: 0.0149\n",
      "2018-12-28T11:13:39.851014, step: 290, loss: 0.6904909610748291, acc: 0.4219, auc: 0.5183, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:40.430473, step: 291, loss: 0.6843304634094238, acc: 0.5078, auc: 0.6183, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:41.046945, step: 292, loss: 0.685965895652771, acc: 0.4375, auc: 0.4936, precision: 1.0, recall: 0.0137\n",
      "2018-12-28T11:13:41.661547, step: 293, loss: 0.6777648329734802, acc: 0.4531, auc: 0.5887, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:42.248864, step: 294, loss: 0.6953266859054565, acc: 0.4609, auc: 0.4438, precision: 1.0, recall: 0.0282\n",
      "2018-12-28T11:13:42.815344, step: 295, loss: 0.7166947722434998, acc: 0.6328, auc: 0.513, precision: 1.0, recall: 0.0408\n",
      "2018-12-28T11:13:43.382694, step: 296, loss: 0.7018347978591919, acc: 0.5156, auc: 0.5517, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:43.961834, step: 297, loss: 0.6876848340034485, acc: 0.4688, auc: 0.5245, precision: 0.5, recall: 0.0147\n",
      "2018-12-28T11:13:44.554644, step: 298, loss: 0.7018476128578186, acc: 0.5312, auc: 0.4733, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:13:45.114132, step: 299, loss: 0.6872754096984863, acc: 0.4531, auc: 0.5046, precision: 1.0, recall: 0.0141\n",
      "2018-12-28T11:13:45.693491, step: 300, loss: 0.6981291174888611, acc: 0.4844, auc: 0.4906, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:14:08.198801, step: 300, loss: 0.6905054653945722, acc: 0.5002078947368421, auc: 0.5423315789473685, precision: 0.5811421052631579, recall: 0.0162921052631579\n",
      "2018-12-28T11:14:08.741909, step: 301, loss: 0.6900315284729004, acc: 0.5156, auc: 0.5435, precision: 1.0, recall: 0.0312\n",
      "2018-12-28T11:14:09.343268, step: 302, loss: 0.6929932832717896, acc: 0.5781, auc: 0.5723, precision: 0.5, recall: 0.0185\n",
      "2018-12-28T11:14:09.925967, step: 303, loss: 0.6942955851554871, acc: 0.4844, auc: 0.4913, precision: 1.0, recall: 0.0149\n",
      "2018-12-28T11:14:10.550615, step: 304, loss: 0.6931138038635254, acc: 0.5234, auc: 0.495, precision: 1.0, recall: 0.0317\n",
      "2018-12-28T11:14:11.114891, step: 305, loss: 0.6906867623329163, acc: 0.5312, auc: 0.4944, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:11.723734, step: 306, loss: 0.6874883770942688, acc: 0.5625, auc: 0.5466, precision: 1.0, recall: 0.0175\n",
      "2018-12-28T11:14:12.333664, step: 307, loss: 0.6887320280075073, acc: 0.5, auc: 0.5667, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:12.899814, step: 308, loss: 0.6924867630004883, acc: 0.4922, auc: 0.4907, precision: 1.0, recall: 0.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:14:13.462116, step: 309, loss: 0.6746433973312378, acc: 0.5391, auc: 0.6206, precision: 1.0, recall: 0.0167\n",
      "2018-12-28T11:14:14.101718, step: 310, loss: 0.6776739358901978, acc: 0.5547, auc: 0.575, precision: 0.0, recall: 0.0\n",
      "start training model\n",
      "2018-12-28T11:14:14.705305, step: 311, loss: 0.6746993064880371, acc: 0.4688, auc: 0.6486, precision: 1.0, recall: 0.0423\n",
      "2018-12-28T11:14:15.273005, step: 312, loss: 0.6779046654701233, acc: 0.4375, auc: 0.6438, precision: 1.0, recall: 0.0137\n",
      "2018-12-28T11:14:15.844783, step: 313, loss: 0.6913071870803833, acc: 0.5, auc: 0.4988, precision: 0.75, recall: 0.0455\n",
      "2018-12-28T11:14:16.457044, step: 314, loss: 0.6882620453834534, acc: 0.4453, auc: 0.5667, precision: 1.0, recall: 0.0139\n",
      "2018-12-28T11:14:17.040078, step: 315, loss: 0.6875081062316895, acc: 0.4609, auc: 0.5674, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:17.615908, step: 316, loss: 0.6796517372131348, acc: 0.5469, auc: 0.5645, precision: 1.0, recall: 0.0333\n",
      "2018-12-28T11:14:18.205143, step: 317, loss: 0.6814523935317993, acc: 0.5391, auc: 0.5703, precision: 0.6667, recall: 0.0333\n",
      "2018-12-28T11:14:18.783941, step: 318, loss: 0.6963354349136353, acc: 0.5859, auc: 0.4583, precision: 1.0, recall: 0.0536\n",
      "2018-12-28T11:14:19.328877, step: 319, loss: 0.671830415725708, acc: 0.6172, auc: 0.6053, precision: 0.75, recall: 0.0588\n",
      "2018-12-28T11:14:19.911571, step: 320, loss: 0.700278103351593, acc: 0.4688, auc: 0.4594, precision: 1.0, recall: 0.0286\n",
      "2018-12-28T11:14:20.474234, step: 321, loss: 0.6886349320411682, acc: 0.5312, auc: 0.5336, precision: 1.0, recall: 0.0476\n",
      "2018-12-28T11:14:21.048376, step: 322, loss: 0.679887592792511, acc: 0.5703, auc: 0.5258, precision: 1.0, recall: 0.0351\n",
      "2018-12-28T11:14:21.642184, step: 323, loss: 0.6808872818946838, acc: 0.5703, auc: 0.5896, precision: 0.75, recall: 0.0526\n",
      "2018-12-28T11:14:22.212583, step: 324, loss: 0.67021644115448, acc: 0.5156, auc: 0.6312, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:22.786285, step: 325, loss: 0.6759623289108276, acc: 0.5, auc: 0.577, precision: 1.0, recall: 0.0448\n",
      "2018-12-28T11:14:23.414756, step: 326, loss: 0.6868939399719238, acc: 0.4609, auc: 0.5914, precision: 0.6667, recall: 0.0286\n",
      "2018-12-28T11:14:24.047510, step: 327, loss: 0.6917368769645691, acc: 0.5, auc: 0.4659, precision: 1.0, recall: 0.0154\n",
      "2018-12-28T11:14:24.658641, step: 328, loss: 0.6787221431732178, acc: 0.5781, auc: 0.5474, precision: 1.0, recall: 0.0182\n",
      "2018-12-28T11:14:25.232116, step: 329, loss: 0.6759397387504578, acc: 0.5312, auc: 0.6291, precision: 0.8, recall: 0.0635\n",
      "2018-12-28T11:14:25.825383, step: 330, loss: 0.6851627230644226, acc: 0.4844, auc: 0.574, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:26.471024, step: 331, loss: 0.6679573059082031, acc: 0.5859, auc: 0.5402, precision: 1.0, recall: 0.1017\n",
      "2018-12-28T11:14:27.137404, step: 332, loss: 0.7026529312133789, acc: 0.4844, auc: 0.4857, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:14:27.738991, step: 333, loss: 0.6755764484405518, acc: 0.5391, auc: 0.5832, precision: 1.0, recall: 0.0635\n",
      "2018-12-28T11:14:28.306084, step: 334, loss: 0.6704568266868591, acc: 0.5312, auc: 0.5924, precision: 1.0, recall: 0.0476\n",
      "2018-12-28T11:14:28.905792, step: 335, loss: 0.6732116341590881, acc: 0.4688, auc: 0.6551, precision: 1.0, recall: 0.0423\n",
      "2018-12-28T11:14:29.490280, step: 336, loss: 0.6763656139373779, acc: 0.5234, auc: 0.5547, precision: 0.5, recall: 0.0164\n",
      "2018-12-28T11:14:30.092174, step: 337, loss: 0.6730165481567383, acc: 0.5078, auc: 0.6112, precision: 1.0, recall: 0.0597\n",
      "2018-12-28T11:14:30.659330, step: 338, loss: 0.6823807954788208, acc: 0.4844, auc: 0.5721, precision: 1.0, recall: 0.0149\n",
      "2018-12-28T11:14:31.245875, step: 339, loss: 0.6834907531738281, acc: 0.5391, auc: 0.5758, precision: 1.0, recall: 0.0484\n",
      "2018-12-28T11:14:31.830389, step: 340, loss: 0.6798239946365356, acc: 0.5234, auc: 0.5629, precision: 0.75, recall: 0.0923\n",
      "2018-12-28T11:14:32.480698, step: 341, loss: 0.6747324466705322, acc: 0.5547, auc: 0.6112, precision: 1.0, recall: 0.0806\n",
      "2018-12-28T11:14:33.103792, step: 342, loss: 0.6615133285522461, acc: 0.5, auc: 0.6133, precision: 0.9, recall: 0.125\n",
      "2018-12-28T11:14:33.691164, step: 343, loss: 0.6866484880447388, acc: 0.5469, auc: 0.5272, precision: 0.75, recall: 0.05\n",
      "2018-12-28T11:14:34.243085, step: 344, loss: 0.666487991809845, acc: 0.5, auc: 0.6082, precision: 1.0, recall: 0.0725\n",
      "2018-12-28T11:14:34.878393, step: 345, loss: 0.6741549968719482, acc: 0.5547, auc: 0.6282, precision: 0.8, recall: 0.0667\n",
      "2018-12-28T11:14:35.459613, step: 346, loss: 0.6745615601539612, acc: 0.5469, auc: 0.6004, precision: 0.8, recall: 0.0656\n",
      "2018-12-28T11:14:36.029470, step: 347, loss: 0.6767422556877136, acc: 0.5391, auc: 0.538, precision: 0.6667, recall: 0.0656\n",
      "2018-12-28T11:14:36.615172, step: 348, loss: 0.6669028997421265, acc: 0.5859, auc: 0.5849, precision: 1.0, recall: 0.0702\n",
      "2018-12-28T11:14:37.191732, step: 349, loss: 0.664574146270752, acc: 0.4531, auc: 0.5803, precision: 1.0, recall: 0.1026\n",
      "2018-12-28T11:14:37.789754, step: 350, loss: 0.6562530994415283, acc: 0.5625, auc: 0.6117, precision: 0.7778, recall: 0.1148\n",
      "2018-12-28T11:14:38.452546, step: 351, loss: 0.6843563914299011, acc: 0.5625, auc: 0.5138, precision: 1.0, recall: 0.082\n",
      "2018-12-28T11:14:39.104169, step: 352, loss: 0.6618760824203491, acc: 0.5625, auc: 0.6165, precision: 0.9, recall: 0.1406\n",
      "2018-12-28T11:14:39.716747, step: 353, loss: 0.6837636232376099, acc: 0.5234, auc: 0.523, precision: 0.8889, recall: 0.1176\n",
      "2018-12-28T11:14:40.288799, step: 354, loss: 0.6856301426887512, acc: 0.5078, auc: 0.5663, precision: 0.6667, recall: 0.0615\n",
      "2018-12-28T11:14:40.871173, step: 355, loss: 0.6661511659622192, acc: 0.5312, auc: 0.6406, precision: 0.75, recall: 0.0938\n",
      "2018-12-28T11:14:41.541426, step: 356, loss: 0.7048383951187134, acc: 0.5391, auc: 0.4897, precision: 0.4, recall: 0.0702\n",
      "2018-12-28T11:14:42.162630, step: 357, loss: 0.6669257879257202, acc: 0.5156, auc: 0.6326, precision: 0.8462, recall: 0.1549\n",
      "2018-12-28T11:14:42.796886, step: 358, loss: 0.664974570274353, acc: 0.5234, auc: 0.6189, precision: 0.7333, recall: 0.1618\n",
      "2018-12-28T11:14:43.401822, step: 359, loss: 0.6815906763076782, acc: 0.4844, auc: 0.6202, precision: 0.6667, recall: 0.1143\n",
      "2018-12-28T11:14:44.038830, step: 360, loss: 0.6688103675842285, acc: 0.5391, auc: 0.6398, precision: 0.8333, recall: 0.0794\n",
      "2018-12-28T11:14:44.585271, step: 361, loss: 0.6695144772529602, acc: 0.5234, auc: 0.5936, precision: 1.0, recall: 0.0758\n",
      "2018-12-28T11:14:45.162250, step: 362, loss: 0.6711405515670776, acc: 0.5391, auc: 0.6288, precision: 0.8, recall: 0.1231\n",
      "2018-12-28T11:14:45.768914, step: 363, loss: 0.6470195651054382, acc: 0.5391, auc: 0.6753, precision: 0.9167, recall: 0.1594\n",
      "2018-12-28T11:14:46.343109, step: 364, loss: 0.6898425817489624, acc: 0.5156, auc: 0.548, precision: 0.6667, recall: 0.0317\n",
      "2018-12-28T11:14:46.928029, step: 365, loss: 0.6560286283493042, acc: 0.5391, auc: 0.6151, precision: 1.0, recall: 0.1449\n",
      "2018-12-28T11:14:47.574157, step: 366, loss: 0.6785491704940796, acc: 0.5, auc: 0.5885, precision: 0.75, recall: 0.0455\n",
      "2018-12-28T11:14:48.211575, step: 367, loss: 0.662168025970459, acc: 0.6016, auc: 0.5467, precision: 0.9231, recall: 0.1935\n",
      "2018-12-28T11:14:48.810017, step: 368, loss: 0.713131308555603, acc: 0.5234, auc: 0.4882, precision: 0.4545, recall: 0.0833\n",
      "2018-12-28T11:14:49.388310, step: 369, loss: 0.6659464240074158, acc: 0.5703, auc: 0.598, precision: 0.75, recall: 0.1475\n",
      "2018-12-28T11:14:50.018901, step: 370, loss: 0.651688814163208, acc: 0.5703, auc: 0.596, precision: 0.8667, recall: 0.197\n",
      "2018-12-28T11:14:50.567899, step: 371, loss: 0.6953528523445129, acc: 0.5078, auc: 0.5139, precision: 0.6667, recall: 0.0909\n",
      "2018-12-28T11:14:51.207743, step: 372, loss: 0.6975785493850708, acc: 0.4922, auc: 0.4828, precision: 0.5833, recall: 0.1045\n",
      "2018-12-28T11:14:51.844583, step: 373, loss: 0.6678703427314758, acc: 0.6016, auc: 0.5698, precision: 0.6154, recall: 0.1481\n",
      "2018-12-28T11:14:52.417906, step: 374, loss: 0.6893929839134216, acc: 0.5312, auc: 0.5954, precision: 0.7143, recall: 0.0794\n",
      "2018-12-28T11:14:53.067156, step: 375, loss: 0.6550008654594421, acc: 0.6328, auc: 0.6359, precision: 0.7143, recall: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:14:53.707056, step: 376, loss: 0.6565098762512207, acc: 0.6016, auc: 0.577, precision: 0.5, recall: 0.0392\n",
      "2018-12-28T11:14:54.368259, step: 377, loss: 0.6880519390106201, acc: 0.4922, auc: 0.5156, precision: 1.0, recall: 0.058\n",
      "2018-12-28T11:14:55.036173, step: 378, loss: 0.6380945444107056, acc: 0.5703, auc: 0.6735, precision: 1.0, recall: 0.1129\n",
      "2018-12-28T11:14:55.704459, step: 379, loss: 0.6519590616226196, acc: 0.6562, auc: 0.5373, precision: 1.0, recall: 0.102\n",
      "2018-12-28T11:14:56.309667, step: 380, loss: 0.6988681554794312, acc: 0.4688, auc: 0.5958, precision: 0.875, recall: 0.0946\n",
      "2018-12-28T11:14:56.875332, step: 381, loss: 0.6803703308105469, acc: 0.5, auc: 0.6099, precision: 1.0, recall: 0.0725\n",
      "2018-12-28T11:14:57.460731, step: 382, loss: 0.6944196820259094, acc: 0.4688, auc: 0.5254, precision: 0.625, recall: 0.0714\n",
      "2018-12-28T11:14:58.068479, step: 383, loss: 0.6565747857093811, acc: 0.5391, auc: 0.6005, precision: 0.8667, recall: 0.1857\n",
      "2018-12-28T11:14:58.688101, step: 384, loss: 0.6701275110244751, acc: 0.5938, auc: 0.5618, precision: 0.875, recall: 0.1207\n",
      "2018-12-28T11:14:59.367001, step: 385, loss: 0.6481938362121582, acc: 0.5469, auc: 0.7004, precision: 0.6667, recall: 0.129\n",
      "2018-12-28T11:14:59.968322, step: 386, loss: 0.6678802371025085, acc: 0.5625, auc: 0.5988, precision: 0.8182, recall: 0.1429\n",
      "2018-12-28T11:15:00.580910, step: 387, loss: 0.6543282270431519, acc: 0.5469, auc: 0.6004, precision: 0.9333, recall: 0.1972\n",
      "2018-12-28T11:15:01.207135, step: 388, loss: 0.6311933994293213, acc: 0.6094, auc: 0.6877, precision: 1.0, recall: 0.1935\n",
      "2018-12-28T11:15:01.777532, step: 389, loss: 0.676308274269104, acc: 0.5391, auc: 0.5565, precision: 0.7143, recall: 0.0806\n",
      "2018-12-28T11:15:02.334146, step: 390, loss: 0.651669979095459, acc: 0.5781, auc: 0.612, precision: 0.875, recall: 0.1167\n",
      "2018-12-28T11:15:02.916092, step: 391, loss: 0.6245278716087341, acc: 0.5938, auc: 0.7204, precision: 1.0, recall: 0.2121\n",
      "2018-12-28T11:15:03.477010, step: 392, loss: 0.6739464998245239, acc: 0.6172, auc: 0.6617, precision: 0.6923, recall: 0.1667\n",
      "2018-12-28T11:15:04.076605, step: 393, loss: 0.6603252291679382, acc: 0.6094, auc: 0.6411, precision: 0.9091, recall: 0.1695\n",
      "2018-12-28T11:15:04.652040, step: 394, loss: 0.6722134351730347, acc: 0.5391, auc: 0.5437, precision: 0.5833, recall: 0.1148\n",
      "2018-12-28T11:15:05.220490, step: 395, loss: 0.6412715911865234, acc: 0.5859, auc: 0.649, precision: 0.8182, recall: 0.15\n",
      "2018-12-28T11:15:05.806839, step: 396, loss: 0.6453514099121094, acc: 0.5156, auc: 0.6944, precision: 1.0, recall: 0.0746\n",
      "2018-12-28T11:15:06.403357, step: 397, loss: 0.6396757364273071, acc: 0.6094, auc: 0.653, precision: 0.8667, recall: 0.2131\n",
      "2018-12-28T11:15:07.040907, step: 398, loss: 0.6444597244262695, acc: 0.5391, auc: 0.64, precision: 0.7333, recall: 0.1667\n",
      "2018-12-28T11:15:07.634613, step: 399, loss: 0.6518570780754089, acc: 0.5312, auc: 0.6887, precision: 0.9167, recall: 0.1571\n",
      "2018-12-28T11:15:08.229533, step: 400, loss: 0.6522248387336731, acc: 0.5859, auc: 0.6278, precision: 0.7083, recall: 0.2698\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:15:31.006019, step: 400, loss: 0.6834094100876859, acc: 0.5435921052631579, auc: 0.5880421052631578, precision: 0.7082552631578947, recall: 0.1726131578947368\n",
      "2018-12-28T11:15:31.592112, step: 401, loss: 0.6587140560150146, acc: 0.5391, auc: 0.575, precision: 0.8333, recall: 0.2113\n",
      "2018-12-28T11:15:32.169840, step: 402, loss: 0.6833196878433228, acc: 0.5781, auc: 0.5828, precision: 0.875, recall: 0.1167\n",
      "2018-12-28T11:15:32.798316, step: 403, loss: 0.6508294939994812, acc: 0.5625, auc: 0.6684, precision: 0.7333, recall: 0.1746\n",
      "2018-12-28T11:15:33.478461, step: 404, loss: 0.6461537480354309, acc: 0.5703, auc: 0.6882, precision: 0.7333, recall: 0.1774\n",
      "2018-12-28T11:15:34.127276, step: 405, loss: 0.6670617461204529, acc: 0.4844, auc: 0.6215, precision: 0.7143, recall: 0.0725\n",
      "2018-12-28T11:15:34.736135, step: 406, loss: 0.6159713268280029, acc: 0.6016, auc: 0.7243, precision: 0.8667, recall: 0.2097\n",
      "2018-12-28T11:15:35.319368, step: 407, loss: 0.624406635761261, acc: 0.6172, auc: 0.6872, precision: 0.8636, recall: 0.2923\n",
      "2018-12-28T11:15:35.914166, step: 408, loss: 0.6311658620834351, acc: 0.6484, auc: 0.6534, precision: 0.7419, recall: 0.3833\n",
      "2018-12-28T11:15:36.506283, step: 409, loss: 0.7244045734405518, acc: 0.5391, auc: 0.5242, precision: 0.5, recall: 0.0169\n",
      "2018-12-28T11:15:37.122383, step: 410, loss: 0.7057427763938904, acc: 0.5156, auc: 0.5894, precision: 0.75, recall: 0.0469\n",
      "2018-12-28T11:15:37.676296, step: 411, loss: 0.7404161691665649, acc: 0.5, auc: 0.5637, precision: 0.0, recall: 0.0\n",
      "2018-12-28T11:15:38.222987, step: 412, loss: 0.7080904245376587, acc: 0.5938, auc: 0.5631, precision: 0.75, recall: 0.0556\n",
      "2018-12-28T11:15:38.803695, step: 413, loss: 0.7039223313331604, acc: 0.4531, auc: 0.473, precision: 1.0, recall: 0.0667\n",
      "2018-12-28T11:15:39.403171, step: 414, loss: 0.6996957063674927, acc: 0.4609, auc: 0.5839, precision: 0.6667, recall: 0.0563\n",
      "2018-12-28T11:15:39.953386, step: 415, loss: 0.6745012998580933, acc: 0.5703, auc: 0.6053, precision: 0.7692, recall: 0.1613\n",
      "2018-12-28T11:15:40.571408, step: 416, loss: 0.6871061325073242, acc: 0.5156, auc: 0.5958, precision: 0.5676, recall: 0.3134\n",
      "2018-12-28T11:15:41.192247, step: 417, loss: 0.6792348027229309, acc: 0.5234, auc: 0.6016, precision: 0.5429, recall: 0.2969\n",
      "2018-12-28T11:15:41.786848, step: 418, loss: 0.7365258932113647, acc: 0.6328, auc: 0.5732, precision: 0.5143, recall: 0.375\n",
      "2018-12-28T11:15:42.375455, step: 419, loss: 0.649930477142334, acc: 0.5625, auc: 0.6347, precision: 0.7692, recall: 0.2857\n",
      "2018-12-28T11:15:42.938376, step: 420, loss: 0.6742790341377258, acc: 0.4922, auc: 0.585, precision: 0.56, recall: 0.2059\n",
      "2018-12-28T11:15:43.505265, step: 421, loss: 0.7097021341323853, acc: 0.5312, auc: 0.5205, precision: 0.6667, recall: 0.125\n",
      "2018-12-28T11:15:44.098284, step: 422, loss: 0.6733660697937012, acc: 0.5781, auc: 0.5272, precision: 0.7143, recall: 0.1667\n",
      "2018-12-28T11:15:44.667152, step: 423, loss: 0.6681586503982544, acc: 0.6094, auc: 0.519, precision: 0.7, recall: 0.1296\n",
      "2018-12-28T11:15:45.220193, step: 424, loss: 0.6532796621322632, acc: 0.5547, auc: 0.6298, precision: 0.8333, recall: 0.082\n",
      "2018-12-28T11:15:45.786773, step: 425, loss: 0.6775021553039551, acc: 0.5781, auc: 0.5782, precision: 0.7143, recall: 0.0877\n",
      "2018-12-28T11:15:46.376544, step: 426, loss: 0.7162756323814392, acc: 0.4609, auc: 0.523, precision: 0.5, recall: 0.029\n",
      "2018-12-28T11:15:46.935164, step: 427, loss: 0.6792970895767212, acc: 0.5547, auc: 0.5009, precision: 0.75, recall: 0.0508\n",
      "2018-12-28T11:15:47.525212, step: 428, loss: 0.6810075640678406, acc: 0.5234, auc: 0.5802, precision: 0.6667, recall: 0.0635\n",
      "2018-12-28T11:15:48.092793, step: 429, loss: 0.7106381058692932, acc: 0.5234, auc: 0.4901, precision: 0.75, recall: 0.0923\n",
      "2018-12-28T11:15:48.726974, step: 430, loss: 0.6974972486495972, acc: 0.5156, auc: 0.535, precision: 0.8, recall: 0.1176\n",
      "2018-12-28T11:15:49.323767, step: 431, loss: 0.6712884902954102, acc: 0.5312, auc: 0.5604, precision: 0.875, recall: 0.1061\n",
      "2018-12-28T11:15:49.902298, step: 432, loss: 0.7090739011764526, acc: 0.5547, auc: 0.5239, precision: 0.75, recall: 0.0984\n",
      "2018-12-28T11:15:50.459030, step: 433, loss: 0.6649305820465088, acc: 0.5391, auc: 0.5659, precision: 1.0, recall: 0.1194\n",
      "2018-12-28T11:15:51.034876, step: 434, loss: 0.700194239616394, acc: 0.5, auc: 0.5819, precision: 0.75, recall: 0.0455\n",
      "2018-12-28T11:15:51.627663, step: 435, loss: 0.645139753818512, acc: 0.5312, auc: 0.6391, precision: 1.0, recall: 0.1045\n",
      "2018-12-28T11:15:52.288239, step: 436, loss: 0.6773545742034912, acc: 0.5391, auc: 0.6049, precision: 0.875, recall: 0.1077\n",
      "2018-12-28T11:15:52.928159, step: 437, loss: 0.6355648040771484, acc: 0.6562, auc: 0.6521, precision: 0.9, recall: 0.1731\n",
      "2018-12-28T11:15:53.524817, step: 438, loss: 0.6600492000579834, acc: 0.6172, auc: 0.6367, precision: 0.9286, recall: 0.2131\n",
      "2018-12-28T11:15:54.144900, step: 439, loss: 0.6598881483078003, acc: 0.5312, auc: 0.637, precision: 0.7, recall: 0.1094\n",
      "2018-12-28T11:15:54.700499, step: 440, loss: 0.6851217746734619, acc: 0.4375, auc: 0.5688, precision: 0.8, recall: 0.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:15:55.270114, step: 441, loss: 0.6654698848724365, acc: 0.4688, auc: 0.6065, precision: 1.0, recall: 0.0685\n",
      "2018-12-28T11:15:55.878721, step: 442, loss: 0.6724981069564819, acc: 0.5547, auc: 0.5045, precision: 1.0, recall: 0.0339\n",
      "2018-12-28T11:15:56.426125, step: 443, loss: 0.6710162162780762, acc: 0.4688, auc: 0.5821, precision: 1.0, recall: 0.0685\n",
      "2018-12-28T11:15:56.998927, step: 444, loss: 0.6938177347183228, acc: 0.5469, auc: 0.5306, precision: 0.6, recall: 0.1\n",
      "2018-12-28T11:15:57.562583, step: 445, loss: 0.6857922673225403, acc: 0.5234, auc: 0.5865, precision: 0.5, recall: 0.082\n",
      "2018-12-28T11:15:58.142697, step: 446, loss: 0.645808219909668, acc: 0.5078, auc: 0.6324, precision: 1.0, recall: 0.1486\n",
      "2018-12-28T11:15:58.725950, step: 447, loss: 0.6900925636291504, acc: 0.5234, auc: 0.5648, precision: 0.75, recall: 0.0923\n",
      "2018-12-28T11:15:59.305834, step: 448, loss: 0.6840195655822754, acc: 0.5703, auc: 0.5078, precision: 1.0, recall: 0.1129\n",
      "2018-12-28T11:16:00.052755, step: 449, loss: 0.6555428504943848, acc: 0.5703, auc: 0.6681, precision: 0.6429, recall: 0.1525\n",
      "2018-12-28T11:16:00.658706, step: 450, loss: 0.6647861003875732, acc: 0.5391, auc: 0.5598, precision: 1.0, recall: 0.1324\n",
      "2018-12-28T11:16:01.312165, step: 451, loss: 0.6587791442871094, acc: 0.5078, auc: 0.5587, precision: 1.0, recall: 0.137\n",
      "2018-12-28T11:16:01.882082, step: 452, loss: 0.7128491401672363, acc: 0.5703, auc: 0.5238, precision: 0.7778, recall: 0.1167\n",
      "2018-12-28T11:16:02.487497, step: 453, loss: 0.6868498921394348, acc: 0.5234, auc: 0.5138, precision: 0.8, recall: 0.1194\n",
      "2018-12-28T11:16:03.130105, step: 454, loss: 0.6826766729354858, acc: 0.5156, auc: 0.4916, precision: 0.8571, recall: 0.0896\n",
      "2018-12-28T11:16:03.707986, step: 455, loss: 0.6912211179733276, acc: 0.5234, auc: 0.5485, precision: 0.4444, recall: 0.0667\n",
      "2018-12-28T11:16:04.250591, step: 456, loss: 0.6798666715621948, acc: 0.5312, auc: 0.516, precision: 0.8182, recall: 0.1343\n",
      "2018-12-28T11:16:04.871416, step: 457, loss: 0.6839828491210938, acc: 0.5391, auc: 0.6408, precision: 0.7143, recall: 0.0806\n",
      "2018-12-28T11:16:05.442776, step: 458, loss: 0.6870642900466919, acc: 0.5703, auc: 0.5114, precision: 0.875, recall: 0.1148\n",
      "2018-12-28T11:16:06.021596, step: 459, loss: 0.6842026710510254, acc: 0.5547, auc: 0.5458, precision: 0.75, recall: 0.1429\n",
      "2018-12-28T11:16:06.565637, step: 460, loss: 0.6808261871337891, acc: 0.5703, auc: 0.5706, precision: 0.75, recall: 0.1017\n",
      "2018-12-28T11:16:07.225882, step: 461, loss: 0.6500082612037659, acc: 0.5547, auc: 0.6, precision: 1.0, recall: 0.1231\n",
      "2018-12-28T11:16:07.797585, step: 462, loss: 0.6533056497573853, acc: 0.5859, auc: 0.6264, precision: 0.8571, recall: 0.1905\n",
      "2018-12-28T11:16:08.370180, step: 463, loss: 0.6570830345153809, acc: 0.5312, auc: 0.6465, precision: 0.7, recall: 0.1094\n",
      "2018-12-28T11:16:08.995087, step: 464, loss: 0.6632503271102905, acc: 0.5391, auc: 0.5376, precision: 0.7143, recall: 0.0806\n",
      "2018-12-28T11:16:09.574308, step: 465, loss: 0.6882755160331726, acc: 0.5078, auc: 0.5429, precision: 0.7273, recall: 0.1176\n",
      "start training model\n",
      "2018-12-28T11:16:10.193970, step: 466, loss: 0.6592265963554382, acc: 0.5469, auc: 0.6447, precision: 0.6667, recall: 0.129\n",
      "2018-12-28T11:16:10.778671, step: 467, loss: 0.6853384971618652, acc: 0.5156, auc: 0.545, precision: 0.8333, recall: 0.0758\n",
      "2018-12-28T11:16:11.443139, step: 468, loss: 0.6469109058380127, acc: 0.4844, auc: 0.6458, precision: 1.0, recall: 0.1429\n",
      "2018-12-28T11:16:12.043351, step: 469, loss: 0.6578356623649597, acc: 0.5547, auc: 0.6395, precision: 0.8571, recall: 0.0968\n",
      "2018-12-28T11:16:12.629922, step: 470, loss: 0.6395783424377441, acc: 0.625, auc: 0.6128, precision: 0.7778, recall: 0.1321\n",
      "2018-12-28T11:16:13.300925, step: 471, loss: 0.632646381855011, acc: 0.5781, auc: 0.6347, precision: 1.0, recall: 0.1818\n",
      "2018-12-28T11:16:13.888056, step: 472, loss: 0.6460866332054138, acc: 0.5469, auc: 0.613, precision: 0.8571, recall: 0.1765\n",
      "2018-12-28T11:16:14.546447, step: 473, loss: 0.6422809362411499, acc: 0.5391, auc: 0.6501, precision: 1.0, recall: 0.1194\n",
      "2018-12-28T11:16:15.171098, step: 474, loss: 0.652613639831543, acc: 0.5781, auc: 0.6751, precision: 0.6154, recall: 0.1404\n",
      "2018-12-28T11:16:15.768306, step: 475, loss: 0.6681300401687622, acc: 0.5469, auc: 0.5462, precision: 1.0, recall: 0.0645\n",
      "2018-12-28T11:16:16.357028, step: 476, loss: 0.645293116569519, acc: 0.5703, auc: 0.6288, precision: 0.9286, recall: 0.194\n",
      "2018-12-28T11:16:17.029262, step: 477, loss: 0.6432793140411377, acc: 0.5312, auc: 0.5714, precision: 0.9091, recall: 0.1449\n",
      "2018-12-28T11:16:17.641677, step: 478, loss: 0.6304940581321716, acc: 0.5156, auc: 0.683, precision: 1.0, recall: 0.1268\n",
      "2018-12-28T11:16:18.223253, step: 479, loss: 0.6713191270828247, acc: 0.5469, auc: 0.5974, precision: 0.6, recall: 0.0508\n",
      "2018-12-28T11:16:18.810961, step: 480, loss: 0.6535632610321045, acc: 0.5859, auc: 0.551, precision: 0.9231, recall: 0.1875\n",
      "2018-12-28T11:16:19.439243, step: 481, loss: 0.6730479001998901, acc: 0.5625, auc: 0.5983, precision: 0.7, recall: 0.1167\n",
      "2018-12-28T11:16:20.049127, step: 482, loss: 0.680971086025238, acc: 0.5312, auc: 0.5292, precision: 0.7778, recall: 0.1077\n",
      "2018-12-28T11:16:20.673834, step: 483, loss: 0.6656341552734375, acc: 0.5234, auc: 0.5987, precision: 0.8571, recall: 0.0909\n",
      "2018-12-28T11:16:21.276504, step: 484, loss: 0.6385606527328491, acc: 0.5547, auc: 0.5709, precision: 0.8824, recall: 0.2143\n",
      "2018-12-28T11:16:21.875082, step: 485, loss: 0.6319241523742676, acc: 0.5625, auc: 0.664, precision: 0.9333, recall: 0.2029\n",
      "2018-12-28T11:16:22.487864, step: 486, loss: 0.6602514982223511, acc: 0.5859, auc: 0.5873, precision: 0.8182, recall: 0.15\n",
      "2018-12-28T11:16:23.139822, step: 487, loss: 0.6522244811058044, acc: 0.5625, auc: 0.622, precision: 1.0, recall: 0.1385\n",
      "2018-12-28T11:16:23.726252, step: 488, loss: 0.6455832123756409, acc: 0.5312, auc: 0.6188, precision: 0.7778, recall: 0.1077\n",
      "2018-12-28T11:16:24.296285, step: 489, loss: 0.6328520774841309, acc: 0.5703, auc: 0.6007, precision: 0.9167, recall: 0.1692\n",
      "2018-12-28T11:16:24.867971, step: 490, loss: 0.6491528749465942, acc: 0.5859, auc: 0.6032, precision: 1.0, recall: 0.1167\n",
      "2018-12-28T11:16:25.449907, step: 491, loss: 0.6500843167304993, acc: 0.5703, auc: 0.623, precision: 1.0, recall: 0.1406\n",
      "2018-12-28T11:16:26.065118, step: 492, loss: 0.6214126348495483, acc: 0.5156, auc: 0.7443, precision: 0.9, recall: 0.1286\n",
      "2018-12-28T11:16:26.734070, step: 493, loss: 0.6102977395057678, acc: 0.5625, auc: 0.6832, precision: 1.0, recall: 0.2113\n",
      "2018-12-28T11:16:27.355196, step: 494, loss: 0.6394743919372559, acc: 0.5469, auc: 0.5958, precision: 0.9286, recall: 0.1857\n",
      "2018-12-28T11:16:27.928911, step: 495, loss: 0.6259629726409912, acc: 0.5859, auc: 0.6482, precision: 0.9, recall: 0.1475\n",
      "2018-12-28T11:16:28.480724, step: 496, loss: 0.6265237331390381, acc: 0.5312, auc: 0.6797, precision: 0.9, recall: 0.1324\n",
      "2018-12-28T11:16:29.026339, step: 497, loss: 0.6674349904060364, acc: 0.5625, auc: 0.5202, precision: 1.0, recall: 0.082\n",
      "2018-12-28T11:16:29.689641, step: 498, loss: 0.5981897115707397, acc: 0.6406, auc: 0.6558, precision: 0.8125, recall: 0.2321\n",
      "2018-12-28T11:16:30.270995, step: 499, loss: 0.6700438261032104, acc: 0.5, auc: 0.5725, precision: 0.75, recall: 0.0882\n",
      "2018-12-28T11:16:30.818401, step: 500, loss: 0.6617507934570312, acc: 0.625, auc: 0.5788, precision: 0.875, recall: 0.1296\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:16:52.826387, step: 500, loss: 0.7162096171002639, acc: 0.5117131578947367, auc: 0.5385342105263158, precision: 0.6079157894736842, recall: 0.10215263157894737\n",
      "2018-12-28T11:16:53.487806, step: 501, loss: 0.5937795639038086, acc: 0.5938, auc: 0.6406, precision: 1.0, recall: 0.2239\n",
      "2018-12-28T11:16:54.104188, step: 502, loss: 0.6332354545593262, acc: 0.5859, auc: 0.6117, precision: 0.8, recall: 0.1935\n",
      "2018-12-28T11:16:54.750980, step: 503, loss: 0.6648842692375183, acc: 0.5547, auc: 0.5502, precision: 0.9, recall: 0.1385\n",
      "2018-12-28T11:16:55.377601, step: 504, loss: 0.6304987668991089, acc: 0.5625, auc: 0.6083, precision: 0.9167, recall: 0.1667\n",
      "2018-12-28T11:16:55.967442, step: 505, loss: 0.6570509672164917, acc: 0.5938, auc: 0.6188, precision: 0.7333, recall: 0.1864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:16:56.626990, step: 506, loss: 0.6781946420669556, acc: 0.5391, auc: 0.4861, precision: 0.6364, recall: 0.1129\n",
      "2018-12-28T11:16:57.200840, step: 507, loss: 0.6058547496795654, acc: 0.6328, auc: 0.6954, precision: 0.9091, recall: 0.1786\n",
      "2018-12-28T11:16:57.803052, step: 508, loss: 0.6594014167785645, acc: 0.5781, auc: 0.6157, precision: 0.7143, recall: 0.1667\n",
      "2018-12-28T11:16:58.396582, step: 509, loss: 0.658683717250824, acc: 0.6016, auc: 0.5983, precision: 1.0, recall: 0.0727\n",
      "2018-12-28T11:16:58.955666, step: 510, loss: 0.6987203359603882, acc: 0.5, auc: 0.548, precision: 0.7273, recall: 0.1159\n",
      "2018-12-28T11:16:59.521817, step: 511, loss: 0.646671712398529, acc: 0.5469, auc: 0.6375, precision: 1.0, recall: 0.0938\n",
      "2018-12-28T11:17:00.089378, step: 512, loss: 0.6622017621994019, acc: 0.5703, auc: 0.5659, precision: 0.7778, recall: 0.1167\n",
      "2018-12-28T11:17:00.716653, step: 513, loss: 0.6633504629135132, acc: 0.5938, auc: 0.6141, precision: 1.0, recall: 0.1475\n",
      "2018-12-28T11:17:01.277001, step: 514, loss: 0.6880654096603394, acc: 0.6172, auc: 0.5233, precision: 0.2857, recall: 0.0435\n",
      "2018-12-28T11:17:01.835344, step: 515, loss: 0.6724791526794434, acc: 0.5469, auc: 0.5699, precision: 0.5714, recall: 0.0678\n",
      "2018-12-28T11:17:02.432535, step: 516, loss: 0.6301101446151733, acc: 0.5547, auc: 0.5917, precision: 1.0, recall: 0.1618\n",
      "2018-12-28T11:17:03.043427, step: 517, loss: 0.6698408722877502, acc: 0.5156, auc: 0.6205, precision: 0.9167, recall: 0.1528\n",
      "2018-12-28T11:17:03.598183, step: 518, loss: 0.6303551197052002, acc: 0.6484, auc: 0.6086, precision: 0.9, recall: 0.1698\n",
      "2018-12-28T11:17:04.163267, step: 519, loss: 0.6432431936264038, acc: 0.6016, auc: 0.5258, precision: 1.0, recall: 0.1053\n",
      "2018-12-28T11:17:04.731585, step: 520, loss: 0.6872038841247559, acc: 0.5156, auc: 0.5925, precision: 0.9091, recall: 0.1408\n",
      "2018-12-28T11:17:05.293957, step: 521, loss: 0.6667277812957764, acc: 0.5703, auc: 0.5657, precision: 0.7778, recall: 0.1167\n",
      "2018-12-28T11:17:05.872793, step: 522, loss: 0.654325544834137, acc: 0.5547, auc: 0.6158, precision: 0.5556, recall: 0.0862\n",
      "2018-12-28T11:17:06.440131, step: 523, loss: 0.6589131951332092, acc: 0.5469, auc: 0.5488, precision: 1.0, recall: 0.0938\n",
      "2018-12-28T11:17:07.002413, step: 524, loss: 0.6379581689834595, acc: 0.5391, auc: 0.6681, precision: 0.9091, recall: 0.1471\n",
      "2018-12-28T11:17:07.590658, step: 525, loss: 0.6437779068946838, acc: 0.5547, auc: 0.6218, precision: 1.0, recall: 0.1094\n",
      "2018-12-28T11:17:08.210970, step: 526, loss: 0.6564190983772278, acc: 0.5156, auc: 0.61, precision: 0.8571, recall: 0.0896\n",
      "2018-12-28T11:17:08.809566, step: 527, loss: 0.6672348976135254, acc: 0.5391, auc: 0.5618, precision: 0.6, recall: 0.0984\n",
      "2018-12-28T11:17:09.432508, step: 528, loss: 0.636124849319458, acc: 0.5547, auc: 0.5963, precision: 1.0, recall: 0.0952\n",
      "2018-12-28T11:17:10.004205, step: 529, loss: 0.6343255043029785, acc: 0.5703, auc: 0.625, precision: 0.8571, recall: 0.1\n",
      "2018-12-28T11:17:10.611798, step: 530, loss: 0.6727875471115112, acc: 0.4844, auc: 0.6488, precision: 0.9167, recall: 0.1447\n",
      "2018-12-28T11:17:11.213450, step: 531, loss: 0.6433062553405762, acc: 0.5391, auc: 0.6723, precision: 0.7143, recall: 0.0806\n",
      "2018-12-28T11:17:11.763743, step: 532, loss: 0.6530109643936157, acc: 0.6172, auc: 0.6183, precision: 0.7692, recall: 0.1786\n",
      "2018-12-28T11:17:12.423192, step: 533, loss: 0.6539595127105713, acc: 0.5859, auc: 0.5931, precision: 1.0, recall: 0.1452\n",
      "2018-12-28T11:17:12.993235, step: 534, loss: 0.6281869411468506, acc: 0.5391, auc: 0.6492, precision: 0.9167, recall: 0.1594\n",
      "2018-12-28T11:17:13.576542, step: 535, loss: 0.6473484039306641, acc: 0.5625, auc: 0.6618, precision: 0.9333, recall: 0.2029\n",
      "2018-12-28T11:17:14.192765, step: 536, loss: 0.6326144933700562, acc: 0.5469, auc: 0.611, precision: 0.7692, recall: 0.1538\n",
      "2018-12-28T11:17:14.803077, step: 537, loss: 0.6504194140434265, acc: 0.5156, auc: 0.633, precision: 0.7143, recall: 0.0769\n",
      "2018-12-28T11:17:15.371544, step: 538, loss: 0.6476215124130249, acc: 0.5312, auc: 0.5927, precision: 0.8462, recall: 0.1594\n",
      "2018-12-28T11:17:15.950379, step: 539, loss: 0.6821450591087341, acc: 0.5234, auc: 0.5575, precision: 0.75, recall: 0.0923\n",
      "2018-12-28T11:17:16.587293, step: 540, loss: 0.6581889390945435, acc: 0.5547, auc: 0.6398, precision: 0.9, recall: 0.1385\n",
      "2018-12-28T11:17:17.151856, step: 541, loss: 0.6399356722831726, acc: 0.4844, auc: 0.5657, precision: 1.0, recall: 0.0833\n",
      "2018-12-28T11:17:17.766094, step: 542, loss: 0.6174089908599854, acc: 0.5703, auc: 0.6542, precision: 0.9231, recall: 0.1818\n",
      "2018-12-28T11:17:18.350354, step: 543, loss: 0.6194165945053101, acc: 0.5469, auc: 0.6574, precision: 0.875, recall: 0.2\n",
      "2018-12-28T11:17:18.933477, step: 544, loss: 0.6573796272277832, acc: 0.5312, auc: 0.6296, precision: 0.7647, recall: 0.1884\n",
      "2018-12-28T11:17:19.533713, step: 545, loss: 0.6691522598266602, acc: 0.5234, auc: 0.5885, precision: 0.7059, recall: 0.1765\n",
      "2018-12-28T11:17:20.202786, step: 546, loss: 0.6481790542602539, acc: 0.5703, auc: 0.6384, precision: 0.8462, recall: 0.1719\n",
      "2018-12-28T11:17:20.832551, step: 547, loss: 0.6783710718154907, acc: 0.5859, auc: 0.5603, precision: 0.6667, recall: 0.1071\n",
      "2018-12-28T11:17:21.403903, step: 548, loss: 0.6514987945556641, acc: 0.5938, auc: 0.6049, precision: 0.75, recall: 0.1071\n",
      "2018-12-28T11:17:21.990911, step: 549, loss: 0.6539174318313599, acc: 0.5625, auc: 0.6463, precision: 0.7, recall: 0.1167\n",
      "2018-12-28T11:17:22.700686, step: 550, loss: 0.6448186635971069, acc: 0.5859, auc: 0.6042, precision: 1.0, recall: 0.1719\n",
      "2018-12-28T11:17:23.303552, step: 551, loss: 0.6336590051651001, acc: 0.5312, auc: 0.638, precision: 0.9, recall: 0.1324\n",
      "2018-12-28T11:17:23.901021, step: 552, loss: 0.6543947458267212, acc: 0.5469, auc: 0.5573, precision: 0.7778, recall: 0.1111\n",
      "2018-12-28T11:17:24.481493, step: 553, loss: 0.624326765537262, acc: 0.6172, auc: 0.6244, precision: 1.0, recall: 0.1695\n",
      "2018-12-28T11:17:25.078296, step: 554, loss: 0.6482064723968506, acc: 0.5312, auc: 0.6554, precision: 0.8, recall: 0.1739\n",
      "2018-12-28T11:17:25.649833, step: 555, loss: 0.6358452439308167, acc: 0.5312, auc: 0.6613, precision: 0.875, recall: 0.1061\n",
      "2018-12-28T11:17:26.269196, step: 556, loss: 0.620221734046936, acc: 0.625, auc: 0.6271, precision: 1.0, recall: 0.1864\n",
      "2018-12-28T11:17:26.873318, step: 557, loss: 0.6284193396568298, acc: 0.5703, auc: 0.6073, precision: 0.8333, recall: 0.1587\n",
      "2018-12-28T11:17:27.443756, step: 558, loss: 0.6461039185523987, acc: 0.5391, auc: 0.5682, precision: 1.0, recall: 0.1571\n",
      "2018-12-28T11:17:28.004854, step: 559, loss: 0.6946497559547424, acc: 0.5312, auc: 0.5456, precision: 0.8667, recall: 0.1831\n",
      "2018-12-28T11:17:28.576247, step: 560, loss: 0.6194812059402466, acc: 0.6016, auc: 0.6991, precision: 0.9, recall: 0.1525\n",
      "2018-12-28T11:17:29.118815, step: 561, loss: 0.6344828009605408, acc: 0.5625, auc: 0.6801, precision: 0.8, recall: 0.129\n",
      "2018-12-28T11:17:29.678366, step: 562, loss: 0.6655474305152893, acc: 0.5156, auc: 0.6064, precision: 0.8, recall: 0.1176\n",
      "2018-12-28T11:17:30.224227, step: 563, loss: 0.6631367206573486, acc: 0.5781, auc: 0.6107, precision: 0.8182, recall: 0.1475\n",
      "2018-12-28T11:17:30.825486, step: 564, loss: 0.6411402225494385, acc: 0.5859, auc: 0.6301, precision: 0.7692, recall: 0.1667\n",
      "2018-12-28T11:17:31.426667, step: 565, loss: 0.671952486038208, acc: 0.5312, auc: 0.5607, precision: 0.7778, recall: 0.1077\n",
      "2018-12-28T11:17:32.033430, step: 566, loss: 0.6651576161384583, acc: 0.5312, auc: 0.5635, precision: 0.75, recall: 0.0938\n",
      "2018-12-28T11:17:32.625664, step: 567, loss: 0.6556235551834106, acc: 0.5078, auc: 0.6017, precision: 1.0, recall: 0.0455\n",
      "2018-12-28T11:17:33.239906, step: 568, loss: 0.6451632976531982, acc: 0.5703, auc: 0.6394, precision: 0.8333, recall: 0.0847\n",
      "2018-12-28T11:17:33.819994, step: 569, loss: 0.6490911841392517, acc: 0.5938, auc: 0.6801, precision: 0.8571, recall: 0.1935\n",
      "2018-12-28T11:17:34.397337, step: 570, loss: 0.6719379425048828, acc: 0.5391, auc: 0.5858, precision: 0.75, recall: 0.0952\n",
      "2018-12-28T11:17:34.971677, step: 571, loss: 0.6804391741752625, acc: 0.5312, auc: 0.544, precision: 0.7143, recall: 0.1515\n",
      "2018-12-28T11:17:35.579181, step: 572, loss: 0.6661761999130249, acc: 0.5703, auc: 0.6318, precision: 0.8, recall: 0.1311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:17:36.204846, step: 573, loss: 0.652892529964447, acc: 0.5391, auc: 0.6393, precision: 1.0, recall: 0.0923\n",
      "2018-12-28T11:17:36.891182, step: 574, loss: 0.6446340084075928, acc: 0.625, auc: 0.6685, precision: 0.7692, recall: 0.1818\n",
      "2018-12-28T11:17:37.477871, step: 575, loss: 0.6316225528717041, acc: 0.6016, auc: 0.6711, precision: 0.8, recall: 0.1404\n",
      "2018-12-28T11:17:38.035874, step: 576, loss: 0.6284018754959106, acc: 0.5859, auc: 0.6981, precision: 0.9, recall: 0.1475\n",
      "2018-12-28T11:17:38.636230, step: 577, loss: 0.6561973690986633, acc: 0.5781, auc: 0.5701, precision: 0.8571, recall: 0.1017\n",
      "2018-12-28T11:17:39.230133, step: 578, loss: 0.6373506188392639, acc: 0.5703, auc: 0.6075, precision: 0.8889, recall: 0.129\n",
      "2018-12-28T11:17:39.854585, step: 579, loss: 0.640174925327301, acc: 0.5547, auc: 0.6576, precision: 0.75, recall: 0.1846\n",
      "2018-12-28T11:17:40.447802, step: 580, loss: 0.6409775614738464, acc: 0.5234, auc: 0.6432, precision: 0.8333, recall: 0.0769\n",
      "2018-12-28T11:17:41.125960, step: 581, loss: 0.6716697812080383, acc: 0.5156, auc: 0.5399, precision: 1.0, recall: 0.1143\n",
      "2018-12-28T11:17:41.707254, step: 582, loss: 0.6600618362426758, acc: 0.5547, auc: 0.6208, precision: 1.0, recall: 0.0952\n",
      "2018-12-28T11:17:42.309509, step: 583, loss: 0.6378328800201416, acc: 0.5859, auc: 0.6818, precision: 0.8571, recall: 0.1905\n",
      "2018-12-28T11:17:42.875278, step: 584, loss: 0.625403642654419, acc: 0.5781, auc: 0.6654, precision: 0.875, recall: 0.2121\n",
      "2018-12-28T11:17:43.451504, step: 585, loss: 0.6418906450271606, acc: 0.5312, auc: 0.6719, precision: 0.7692, recall: 0.1493\n",
      "2018-12-28T11:17:44.054710, step: 586, loss: 0.5948095321655273, acc: 0.6094, auc: 0.7268, precision: 0.875, recall: 0.2258\n",
      "2018-12-28T11:17:44.617181, step: 587, loss: 0.6326028108596802, acc: 0.5391, auc: 0.6891, precision: 0.8, recall: 0.1231\n",
      "2018-12-28T11:17:45.193348, step: 588, loss: 0.6165573596954346, acc: 0.5703, auc: 0.6848, precision: 0.85, recall: 0.2464\n",
      "2018-12-28T11:17:45.749937, step: 589, loss: 0.604293704032898, acc: 0.6797, auc: 0.7512, precision: 0.8889, recall: 0.3871\n",
      "2018-12-28T11:17:46.289419, step: 590, loss: 0.5882272124290466, acc: 0.6094, auc: 0.7147, precision: 1.0, recall: 0.2537\n",
      "2018-12-28T11:17:46.905891, step: 591, loss: 0.6165873408317566, acc: 0.6406, auc: 0.7151, precision: 0.7895, recall: 0.2632\n",
      "2018-12-28T11:17:47.496392, step: 592, loss: 0.6450978517532349, acc: 0.5938, auc: 0.6238, precision: 0.7143, recall: 0.3125\n",
      "2018-12-28T11:17:48.202539, step: 593, loss: 0.6744368076324463, acc: 0.5156, auc: 0.6244, precision: 0.9091, recall: 0.1408\n",
      "2018-12-28T11:17:48.822557, step: 594, loss: 0.6593859195709229, acc: 0.4688, auc: 0.676, precision: 0.8182, recall: 0.12\n",
      "2018-12-28T11:17:49.461809, step: 595, loss: 0.5970993041992188, acc: 0.6641, auc: 0.7033, precision: 0.9167, recall: 0.3492\n",
      "2018-12-28T11:17:50.064489, step: 596, loss: 0.6630270481109619, acc: 0.5234, auc: 0.6067, precision: 0.6286, recall: 0.3143\n",
      "2018-12-28T11:17:50.686520, step: 597, loss: 0.6010850667953491, acc: 0.5938, auc: 0.6801, precision: 0.7778, recall: 0.3889\n",
      "2018-12-28T11:17:51.287303, step: 598, loss: 0.5990004539489746, acc: 0.6797, auc: 0.71, precision: 0.7826, recall: 0.3333\n",
      "2018-12-28T11:17:51.905093, step: 599, loss: 0.6237403750419617, acc: 0.6016, auc: 0.6477, precision: 1.0, recall: 0.2388\n",
      "2018-12-28T11:17:52.478330, step: 600, loss: 0.5969966650009155, acc: 0.6016, auc: 0.6921, precision: 1.0, recall: 0.1905\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:18:15.402870, step: 600, loss: 0.6559064529444042, acc: 0.5908710526315788, auc: 0.6993052631578947, precision: 0.7122421052631577, recall: 0.3170552631578947\n",
      "2018-12-28T11:18:16.018143, step: 601, loss: 0.5968433022499084, acc: 0.5781, auc: 0.665, precision: 0.7407, recall: 0.2985\n",
      "2018-12-28T11:18:16.594407, step: 602, loss: 0.635283350944519, acc: 0.5547, auc: 0.6365, precision: 0.7308, recall: 0.2754\n",
      "2018-12-28T11:18:17.226019, step: 603, loss: 0.5279735326766968, acc: 0.6797, auc: 0.7851, precision: 0.9583, recall: 0.3651\n",
      "2018-12-28T11:18:17.771228, step: 604, loss: 0.652036190032959, acc: 0.6094, auc: 0.6755, precision: 0.7273, recall: 0.2667\n",
      "2018-12-28T11:18:18.347111, step: 605, loss: 0.6250673532485962, acc: 0.6328, auc: 0.7091, precision: 1.0, recall: 0.2295\n",
      "2018-12-28T11:18:18.900572, step: 606, loss: 0.6505327224731445, acc: 0.6641, auc: 0.7213, precision: 0.8125, recall: 0.2453\n",
      "2018-12-28T11:18:19.489702, step: 607, loss: 0.588569164276123, acc: 0.6719, auc: 0.7532, precision: 1.0, recall: 0.2759\n",
      "2018-12-28T11:18:20.168293, step: 608, loss: 0.6256289482116699, acc: 0.5781, auc: 0.7124, precision: 0.7778, recall: 0.2188\n",
      "2018-12-28T11:18:20.806825, step: 609, loss: 0.5925769805908203, acc: 0.6641, auc: 0.7126, precision: 0.7308, recall: 0.3455\n",
      "2018-12-28T11:18:21.409347, step: 610, loss: 0.619793176651001, acc: 0.5703, auc: 0.7706, precision: 1.0, recall: 0.1912\n",
      "2018-12-28T11:18:22.014505, step: 611, loss: 0.6820892691612244, acc: 0.5547, auc: 0.646, precision: 1.0, recall: 0.0656\n",
      "2018-12-28T11:18:22.628634, step: 612, loss: 0.6431468725204468, acc: 0.5781, auc: 0.6767, precision: 1.0, recall: 0.1429\n",
      "2018-12-28T11:18:23.221112, step: 613, loss: 0.7075906991958618, acc: 0.5391, auc: 0.5752, precision: 0.6, recall: 0.05\n",
      "2018-12-28T11:18:23.864888, step: 614, loss: 0.743860125541687, acc: 0.5078, auc: 0.5696, precision: 0.8571, recall: 0.0882\n",
      "2018-12-28T11:18:24.556025, step: 615, loss: 0.6767132878303528, acc: 0.5312, auc: 0.5628, precision: 1.0, recall: 0.1045\n",
      "2018-12-28T11:18:25.188096, step: 616, loss: 0.6839860081672668, acc: 0.5078, auc: 0.5782, precision: 0.8333, recall: 0.0746\n",
      "2018-12-28T11:18:25.740658, step: 617, loss: 0.6648498177528381, acc: 0.5234, auc: 0.6076, precision: 0.8333, recall: 0.0769\n",
      "2018-12-28T11:18:26.323611, step: 618, loss: 0.6426244378089905, acc: 0.5078, auc: 0.6797, precision: 0.8, recall: 0.1159\n",
      "2018-12-28T11:18:26.893775, step: 619, loss: 0.6758947372436523, acc: 0.4922, auc: 0.5699, precision: 0.8182, recall: 0.125\n",
      "2018-12-28T11:18:27.473680, step: 620, loss: 0.622442364692688, acc: 0.6641, auc: 0.752, precision: 0.9412, recall: 0.2759\n",
      "start training model\n",
      "2018-12-28T11:18:28.044257, step: 621, loss: 0.6296394467353821, acc: 0.5781, auc: 0.6716, precision: 0.7368, recall: 0.2222\n",
      "2018-12-28T11:18:28.601497, step: 622, loss: 0.627592921257019, acc: 0.6328, auc: 0.6877, precision: 0.8387, recall: 0.3824\n",
      "2018-12-28T11:18:29.159853, step: 623, loss: 0.6124266386032104, acc: 0.6719, auc: 0.7681, precision: 0.8125, recall: 0.4194\n",
      "2018-12-28T11:18:29.740372, step: 624, loss: 0.5906306505203247, acc: 0.7109, auc: 0.7734, precision: 0.8846, recall: 0.4035\n",
      "2018-12-28T11:18:30.323688, step: 625, loss: 0.6000624895095825, acc: 0.6562, auc: 0.7634, precision: 0.9545, recall: 0.3281\n",
      "2018-12-28T11:18:30.893726, step: 626, loss: 0.6430739164352417, acc: 0.6094, auc: 0.6986, precision: 0.6286, recall: 0.3729\n",
      "2018-12-28T11:18:31.488624, step: 627, loss: 0.6049138903617859, acc: 0.6328, auc: 0.7318, precision: 0.8333, recall: 0.3731\n",
      "2018-12-28T11:18:32.089389, step: 628, loss: 0.5958026051521301, acc: 0.6875, auc: 0.7946, precision: 0.8276, recall: 0.4068\n",
      "2018-12-28T11:18:32.649627, step: 629, loss: 0.5843297243118286, acc: 0.7031, auc: 0.8285, precision: 0.8889, recall: 0.4068\n",
      "2018-12-28T11:18:33.258146, step: 630, loss: 0.5979094505310059, acc: 0.5859, auc: 0.7208, precision: 0.9375, recall: 0.2239\n",
      "2018-12-28T11:18:33.969204, step: 631, loss: 0.5815062522888184, acc: 0.5938, auc: 0.8111, precision: 0.875, recall: 0.3\n",
      "2018-12-28T11:18:34.630232, step: 632, loss: 0.5898635387420654, acc: 0.6562, auc: 0.7781, precision: 0.8846, recall: 0.3594\n",
      "2018-12-28T11:18:35.274585, step: 633, loss: 0.5722647905349731, acc: 0.625, auc: 0.7664, precision: 0.7941, recall: 0.3971\n",
      "2018-12-28T11:18:35.856701, step: 634, loss: 0.6155062913894653, acc: 0.6094, auc: 0.7595, precision: 0.6552, recall: 0.322\n",
      "2018-12-28T11:18:36.467201, step: 635, loss: 0.5703315734863281, acc: 0.6562, auc: 0.7744, precision: 0.8, recall: 0.3871\n",
      "2018-12-28T11:18:37.083131, step: 636, loss: 0.5434528589248657, acc: 0.6172, auc: 0.872, precision: 0.9444, recall: 0.2615\n",
      "2018-12-28T11:18:37.737413, step: 637, loss: 0.5606997013092041, acc: 0.6328, auc: 0.7893, precision: 0.913, recall: 0.3182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:18:38.329953, step: 638, loss: 0.5151090025901794, acc: 0.6094, auc: 0.8921, precision: 1.0, recall: 0.2188\n",
      "2018-12-28T11:18:38.890851, step: 639, loss: 0.6282573938369751, acc: 0.5391, auc: 0.7367, precision: 0.8333, recall: 0.1493\n",
      "2018-12-28T11:18:39.468213, step: 640, loss: 0.5360091924667358, acc: 0.6484, auc: 0.836, precision: 1.0, recall: 0.3182\n",
      "2018-12-28T11:18:40.110178, step: 641, loss: 0.5488023161888123, acc: 0.7656, auc: 0.8332, precision: 0.775, recall: 0.5962\n",
      "2018-12-28T11:18:40.690442, step: 642, loss: 0.632789134979248, acc: 0.5547, auc: 0.6761, precision: 0.6102, recall: 0.5143\n",
      "2018-12-28T11:18:41.337429, step: 643, loss: 0.700786828994751, acc: 0.6172, auc: 0.6816, precision: 0.6027, recall: 0.6875\n",
      "2018-12-28T11:18:41.977021, step: 644, loss: 0.7193906307220459, acc: 0.6484, auc: 0.6743, precision: 0.641, recall: 0.7463\n",
      "2018-12-28T11:18:42.548198, step: 645, loss: 0.7353569269180298, acc: 0.5547, auc: 0.599, precision: 0.5312, recall: 0.5574\n",
      "2018-12-28T11:18:43.130870, step: 646, loss: 0.6273415088653564, acc: 0.6016, auc: 0.6746, precision: 0.5962, recall: 0.5082\n",
      "2018-12-28T11:18:43.709445, step: 647, loss: 0.6255322694778442, acc: 0.6328, auc: 0.701, precision: 0.6486, recall: 0.4138\n",
      "2018-12-28T11:18:44.267730, step: 648, loss: 0.6381513476371765, acc: 0.5625, auc: 0.6866, precision: 0.6471, recall: 0.1803\n",
      "2018-12-28T11:18:44.829507, step: 649, loss: 0.59309983253479, acc: 0.5859, auc: 0.7219, precision: 0.7308, recall: 0.2923\n",
      "2018-12-28T11:18:45.414870, step: 650, loss: 0.5611947774887085, acc: 0.5312, auc: 0.8072, precision: 0.9333, recall: 0.1918\n",
      "2018-12-28T11:18:46.025747, step: 651, loss: 0.537155032157898, acc: 0.6562, auc: 0.8254, precision: 1.0, recall: 0.2\n",
      "2018-12-28T11:18:46.622056, step: 652, loss: 0.5521734356880188, acc: 0.6328, auc: 0.8194, precision: 0.8333, recall: 0.1818\n",
      "2018-12-28T11:18:47.195375, step: 653, loss: 0.5704262256622314, acc: 0.6094, auc: 0.8292, precision: 1.0, recall: 0.1667\n",
      "2018-12-28T11:18:47.791118, step: 654, loss: 0.6300697326660156, acc: 0.6016, auc: 0.7534, precision: 1.0, recall: 0.1053\n",
      "2018-12-28T11:18:48.365041, step: 655, loss: 0.6818121075630188, acc: 0.5547, auc: 0.7676, precision: 0.8333, recall: 0.082\n",
      "2018-12-28T11:18:48.929597, step: 656, loss: 0.7673231363296509, acc: 0.4375, auc: 0.7682, precision: 1.0, recall: 0.0769\n",
      "2018-12-28T11:18:49.538859, step: 657, loss: 0.6052428483963013, acc: 0.5391, auc: 0.8366, precision: 0.875, recall: 0.1077\n",
      "2018-12-28T11:18:50.104400, step: 658, loss: 0.5733548998832703, acc: 0.6016, auc: 0.8016, precision: 1.0, recall: 0.2273\n",
      "2018-12-28T11:18:50.665278, step: 659, loss: 0.5077651739120483, acc: 0.5938, auc: 0.9059, precision: 0.9048, recall: 0.2754\n",
      "2018-12-28T11:18:51.250146, step: 660, loss: 0.6015762090682983, acc: 0.6016, auc: 0.7918, precision: 0.8571, recall: 0.2727\n",
      "2018-12-28T11:18:51.826532, step: 661, loss: 0.5971547961235046, acc: 0.5547, auc: 0.7708, precision: 0.75, recall: 0.36\n",
      "2018-12-28T11:18:52.389728, step: 662, loss: 0.5779087543487549, acc: 0.6719, auc: 0.791, precision: 0.7692, recall: 0.4762\n",
      "2018-12-28T11:18:52.983862, step: 663, loss: 0.6000692248344421, acc: 0.6016, auc: 0.7593, precision: 0.7778, recall: 0.3944\n",
      "2018-12-28T11:18:53.582449, step: 664, loss: 0.5233602523803711, acc: 0.6797, auc: 0.8556, precision: 0.8649, recall: 0.4706\n",
      "2018-12-28T11:18:54.216018, step: 665, loss: 0.5448846220970154, acc: 0.7188, auc: 0.8105, precision: 0.8293, recall: 0.5397\n",
      "2018-12-28T11:18:54.788550, step: 666, loss: 0.5609334111213684, acc: 0.6875, auc: 0.8403, precision: 0.6944, recall: 0.463\n",
      "2018-12-28T11:18:55.432508, step: 667, loss: 0.5368357300758362, acc: 0.6562, auc: 0.8327, precision: 0.8519, recall: 0.3651\n",
      "2018-12-28T11:18:56.034858, step: 668, loss: 0.5273741483688354, acc: 0.6953, auc: 0.8507, precision: 0.9167, recall: 0.3729\n",
      "2018-12-28T11:18:56.626696, step: 669, loss: 0.5215211510658264, acc: 0.7344, auc: 0.8414, precision: 0.8824, recall: 0.5\n",
      "2018-12-28T11:18:57.207065, step: 670, loss: 0.4944736361503601, acc: 0.7422, auc: 0.8806, precision: 0.9, recall: 0.5538\n",
      "2018-12-28T11:18:57.800306, step: 671, loss: 0.4698556959629059, acc: 0.7969, auc: 0.8752, precision: 0.8889, recall: 0.7059\n",
      "2018-12-28T11:18:58.388681, step: 672, loss: 0.5007119178771973, acc: 0.8359, auc: 0.8687, precision: 0.8033, recall: 0.8448\n",
      "2018-12-28T11:18:59.042201, step: 673, loss: 0.4389002323150635, acc: 0.8672, auc: 0.9137, precision: 0.9057, recall: 0.8\n",
      "2018-12-28T11:18:59.706646, step: 674, loss: 0.4567674398422241, acc: 0.7812, auc: 0.885, precision: 0.8846, recall: 0.6765\n",
      "2018-12-28T11:19:00.357853, step: 675, loss: 0.5048927664756775, acc: 0.7812, auc: 0.8237, precision: 0.78, recall: 0.6964\n",
      "2018-12-28T11:19:00.979582, step: 676, loss: 0.5116225481033325, acc: 0.7188, auc: 0.823, precision: 0.8182, recall: 0.5625\n",
      "2018-12-28T11:19:01.568383, step: 677, loss: 0.4736354947090149, acc: 0.7812, auc: 0.8725, precision: 0.9111, recall: 0.6308\n",
      "2018-12-28T11:19:02.154384, step: 678, loss: 0.4511260986328125, acc: 0.7969, auc: 0.8734, precision: 0.7907, recall: 0.6667\n",
      "2018-12-28T11:19:02.757594, step: 679, loss: 0.43580013513565063, acc: 0.8047, auc: 0.8799, precision: 0.8667, recall: 0.7536\n",
      "2018-12-28T11:19:03.335182, step: 680, loss: 0.4601426124572754, acc: 0.8203, auc: 0.8665, precision: 0.8788, recall: 0.7945\n",
      "2018-12-28T11:19:03.908150, step: 681, loss: 0.3722243309020996, acc: 0.8906, auc: 0.9253, precision: 0.9028, recall: 0.9028\n",
      "2018-12-28T11:19:04.489696, step: 682, loss: 0.38612663745880127, acc: 0.8438, auc: 0.9221, precision: 0.8571, recall: 0.8308\n",
      "2018-12-28T11:19:05.086471, step: 683, loss: 0.3918447196483612, acc: 0.8516, auc: 0.8999, precision: 0.94, recall: 0.746\n",
      "2018-12-28T11:19:05.643334, step: 684, loss: 0.48789530992507935, acc: 0.7734, auc: 0.8376, precision: 0.8333, recall: 0.6923\n",
      "2018-12-28T11:19:06.278778, step: 685, loss: 0.42115217447280884, acc: 0.8125, auc: 0.8957, precision: 0.92, recall: 0.697\n",
      "2018-12-28T11:19:06.868169, step: 686, loss: 0.41010141372680664, acc: 0.8359, auc: 0.8915, precision: 0.8571, recall: 0.8182\n",
      "2018-12-28T11:19:07.430198, step: 687, loss: 0.3900696337223053, acc: 0.8203, auc: 0.9126, precision: 0.9123, recall: 0.7429\n",
      "2018-12-28T11:19:08.029432, step: 688, loss: 0.3398090600967407, acc: 0.875, auc: 0.9396, precision: 0.8485, recall: 0.9032\n",
      "2018-12-28T11:19:08.600555, step: 689, loss: 0.461788147687912, acc: 0.8125, auc: 0.8725, precision: 0.9286, recall: 0.7222\n",
      "2018-12-28T11:19:09.210781, step: 690, loss: 0.41943812370300293, acc: 0.7891, auc: 0.8931, precision: 0.8889, recall: 0.6957\n",
      "2018-12-28T11:19:09.824062, step: 691, loss: 0.3600720465183258, acc: 0.8438, auc: 0.9206, precision: 0.8889, recall: 0.7742\n",
      "2018-12-28T11:19:10.511200, step: 692, loss: 0.4691920280456543, acc: 0.75, auc: 0.8737, precision: 0.907, recall: 0.5821\n",
      "2018-12-28T11:19:11.093272, step: 693, loss: 0.39466050267219543, acc: 0.8203, auc: 0.8842, precision: 0.9444, recall: 0.6182\n",
      "2018-12-28T11:19:11.645778, step: 694, loss: 0.4121984839439392, acc: 0.8438, auc: 0.9054, precision: 0.8889, recall: 0.7742\n",
      "2018-12-28T11:19:12.244296, step: 695, loss: 0.43516963720321655, acc: 0.8281, auc: 0.8894, precision: 0.8462, recall: 0.7586\n",
      "2018-12-28T11:19:12.806357, step: 696, loss: 0.4132649004459381, acc: 0.8516, auc: 0.9027, precision: 0.8413, recall: 0.8548\n",
      "2018-12-28T11:19:13.380196, step: 697, loss: 0.4948217272758484, acc: 0.7969, auc: 0.8417, precision: 0.8182, recall: 0.7941\n",
      "2018-12-28T11:19:13.971364, step: 698, loss: 0.35604411363601685, acc: 0.8906, auc: 0.9282, precision: 0.875, recall: 0.9032\n",
      "2018-12-28T11:19:14.606157, step: 699, loss: 0.464325487613678, acc: 0.7734, auc: 0.8703, precision: 0.82, recall: 0.6721\n",
      "2018-12-28T11:19:15.251966, step: 700, loss: 0.4681259095668793, acc: 0.7734, auc: 0.8735, precision: 0.8846, recall: 0.6667\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:19:37.788213, step: 700, loss: 0.46913681140071467, acc: 0.7942000000000001, auc: 0.8680894736842105, precision: 0.849621052631579, recall: 0.7181473684210528\n",
      "2018-12-28T11:19:38.388847, step: 701, loss: 0.32378655672073364, acc: 0.8672, auc: 0.932, precision: 0.898, recall: 0.7857\n",
      "2018-12-28T11:19:38.960391, step: 702, loss: 0.3896297216415405, acc: 0.8594, auc: 0.9036, precision: 0.8833, recall: 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:19:39.531201, step: 703, loss: 0.3860780894756317, acc: 0.8438, auc: 0.9061, precision: 0.9348, recall: 0.7167\n",
      "2018-12-28T11:19:40.172279, step: 704, loss: 0.4370121955871582, acc: 0.8281, auc: 0.8702, precision: 0.8857, recall: 0.8158\n",
      "2018-12-28T11:19:40.811673, step: 705, loss: 0.43071234226226807, acc: 0.8359, auc: 0.8873, precision: 0.8, recall: 0.8667\n",
      "2018-12-28T11:19:41.412291, step: 706, loss: 0.4423547387123108, acc: 0.8281, auc: 0.8928, precision: 0.7692, recall: 0.9375\n",
      "2018-12-28T11:19:41.979704, step: 707, loss: 0.4584547281265259, acc: 0.8125, auc: 0.8894, precision: 0.7424, recall: 0.875\n",
      "2018-12-28T11:19:42.538911, step: 708, loss: 0.37457039952278137, acc: 0.8516, auc: 0.9138, precision: 0.8485, recall: 0.8615\n",
      "2018-12-28T11:19:43.118087, step: 709, loss: 0.39698976278305054, acc: 0.8203, auc: 0.9102, precision: 0.8868, recall: 0.7344\n",
      "2018-12-28T11:19:43.701479, step: 710, loss: 0.4002242088317871, acc: 0.8203, auc: 0.9001, precision: 0.8846, recall: 0.7302\n",
      "2018-12-28T11:19:44.286452, step: 711, loss: 0.3254430294036865, acc: 0.8594, auc: 0.9272, precision: 0.8462, recall: 0.8148\n",
      "2018-12-28T11:19:44.904508, step: 712, loss: 0.3422175347805023, acc: 0.8672, auc: 0.9259, precision: 0.9091, recall: 0.8451\n",
      "2018-12-28T11:19:45.494301, step: 713, loss: 0.404920756816864, acc: 0.8203, auc: 0.9038, precision: 0.8056, recall: 0.8657\n",
      "2018-12-28T11:19:46.090256, step: 714, loss: 0.5287861824035645, acc: 0.8125, auc: 0.8492, precision: 0.7317, recall: 0.9677\n",
      "2018-12-28T11:19:46.658452, step: 715, loss: 0.5429901480674744, acc: 0.7734, auc: 0.8712, precision: 0.6897, recall: 0.9677\n",
      "2018-12-28T11:19:47.228863, step: 716, loss: 0.42593979835510254, acc: 0.7969, auc: 0.8716, precision: 0.7671, recall: 0.8615\n",
      "2018-12-28T11:19:47.829895, step: 717, loss: 0.4959256947040558, acc: 0.7422, auc: 0.8745, precision: 0.8966, recall: 0.4643\n",
      "2018-12-28T11:19:48.416450, step: 718, loss: 0.3720436096191406, acc: 0.8438, auc: 0.917, precision: 0.9512, recall: 0.6842\n",
      "2018-12-28T11:19:49.004153, step: 719, loss: 0.4921666979789734, acc: 0.7578, auc: 0.87, precision: 0.9487, recall: 0.5606\n",
      "2018-12-28T11:19:49.578532, step: 720, loss: 0.42049020528793335, acc: 0.7812, auc: 0.9124, precision: 0.9333, recall: 0.6269\n",
      "2018-12-28T11:19:50.167600, step: 721, loss: 0.43505388498306274, acc: 0.8203, auc: 0.8787, precision: 0.9024, recall: 0.6607\n",
      "2018-12-28T11:19:50.720832, step: 722, loss: 0.4262685179710388, acc: 0.8359, auc: 0.8982, precision: 0.8333, recall: 0.7895\n",
      "2018-12-28T11:19:51.313447, step: 723, loss: 0.41175684332847595, acc: 0.8438, auc: 0.897, precision: 0.8361, recall: 0.8361\n",
      "2018-12-28T11:19:51.910366, step: 724, loss: 0.47817713022232056, acc: 0.8047, auc: 0.8723, precision: 0.7647, recall: 0.8525\n",
      "2018-12-28T11:19:52.514133, step: 725, loss: 0.38546618819236755, acc: 0.8906, auc: 0.9255, precision: 0.9067, recall: 0.9067\n",
      "2018-12-28T11:19:53.053976, step: 726, loss: 0.4259217083454132, acc: 0.8203, auc: 0.8939, precision: 0.8209, recall: 0.8333\n",
      "2018-12-28T11:19:53.624136, step: 727, loss: 0.379536896944046, acc: 0.875, auc: 0.9279, precision: 0.9074, recall: 0.8167\n",
      "2018-12-28T11:19:54.196309, step: 728, loss: 0.3953099548816681, acc: 0.8281, auc: 0.9055, precision: 0.9362, recall: 0.6984\n",
      "2018-12-28T11:19:54.745811, step: 729, loss: 0.3734208345413208, acc: 0.8203, auc: 0.9347, precision: 0.9583, recall: 0.6866\n",
      "2018-12-28T11:19:55.347567, step: 730, loss: 0.33332136273384094, acc: 0.8906, auc: 0.9477, precision: 0.9444, recall: 0.8226\n",
      "2018-12-28T11:19:55.929227, step: 731, loss: 0.3700677752494812, acc: 0.8594, auc: 0.9209, precision: 0.8784, recall: 0.8784\n",
      "2018-12-28T11:19:56.576430, step: 732, loss: 0.4385119378566742, acc: 0.8281, auc: 0.9022, precision: 0.7791, recall: 0.9571\n",
      "2018-12-28T11:19:57.132468, step: 733, loss: 0.5869430303573608, acc: 0.7422, auc: 0.8459, precision: 0.6632, recall: 0.9844\n",
      "2018-12-28T11:19:57.711349, step: 734, loss: 0.744384765625, acc: 0.6797, auc: 0.7494, precision: 0.5978, recall: 0.9322\n",
      "2018-12-28T11:19:58.315298, step: 735, loss: 0.7079635858535767, acc: 0.6797, auc: 0.7456, precision: 0.5859, recall: 1.0\n",
      "2018-12-28T11:19:58.901711, step: 736, loss: 0.6558749079704285, acc: 0.7031, auc: 0.7214, precision: 0.6404, recall: 0.9048\n",
      "2018-12-28T11:19:59.476864, step: 737, loss: 0.41881099343299866, acc: 0.8359, auc: 0.8884, precision: 0.7949, recall: 0.9254\n",
      "2018-12-28T11:20:00.087958, step: 738, loss: 0.36677640676498413, acc: 0.8203, auc: 0.9099, precision: 0.7414, recall: 0.8431\n",
      "2018-12-28T11:20:00.674769, step: 739, loss: 0.3333495557308197, acc: 0.8672, auc: 0.9347, precision: 0.9077, recall: 0.8429\n",
      "2018-12-28T11:20:01.287524, step: 740, loss: 0.3932502567768097, acc: 0.7969, auc: 0.9148, precision: 0.8919, recall: 0.6\n",
      "2018-12-28T11:20:01.879136, step: 741, loss: 0.5045194029808044, acc: 0.6953, auc: 0.9199, precision: 0.9706, recall: 0.4648\n",
      "2018-12-28T11:20:02.489940, step: 742, loss: 0.5540517568588257, acc: 0.7109, auc: 0.9091, precision: 0.9394, recall: 0.4697\n",
      "2018-12-28T11:20:03.065990, step: 743, loss: 0.3423230051994324, acc: 0.8359, auc: 0.9313, precision: 1.0, recall: 0.625\n",
      "2018-12-28T11:20:03.739094, step: 744, loss: 0.3978767693042755, acc: 0.8672, auc: 0.9007, precision: 0.8909, recall: 0.8167\n",
      "2018-12-28T11:20:04.355551, step: 745, loss: 0.36232197284698486, acc: 0.8828, auc: 0.914, precision: 0.8852, recall: 0.871\n",
      "2018-12-28T11:20:04.991555, step: 746, loss: 0.5293676257133484, acc: 0.7734, auc: 0.8495, precision: 0.6571, recall: 0.902\n",
      "2018-12-28T11:20:05.572262, step: 747, loss: 0.4617895483970642, acc: 0.7891, auc: 0.8381, precision: 0.7692, recall: 0.8696\n",
      "2018-12-28T11:20:06.128459, step: 748, loss: 0.42493417859077454, acc: 0.8359, auc: 0.874, precision: 0.8056, recall: 0.8923\n",
      "2018-12-28T11:20:06.669769, step: 749, loss: 0.3876177668571472, acc: 0.875, auc: 0.8969, precision: 0.8472, recall: 0.9242\n",
      "2018-12-28T11:20:07.257754, step: 750, loss: 0.5009948015213013, acc: 0.7812, auc: 0.7958, precision: 0.75, recall: 0.8824\n",
      "2018-12-28T11:20:07.984249, step: 751, loss: 0.3578506410121918, acc: 0.8828, auc: 0.9293, precision: 0.9028, recall: 0.8904\n",
      "2018-12-28T11:20:08.586227, step: 752, loss: 0.41031110286712646, acc: 0.8438, auc: 0.8992, precision: 0.9123, recall: 0.7761\n",
      "2018-12-28T11:20:09.184166, step: 753, loss: 0.4116775691509247, acc: 0.8203, auc: 0.9192, precision: 0.9259, recall: 0.7246\n",
      "2018-12-28T11:20:09.805662, step: 754, loss: 0.38044971227645874, acc: 0.8594, auc: 0.9411, precision: 1.0, recall: 0.76\n",
      "2018-12-28T11:20:10.356734, step: 755, loss: 0.4390537142753601, acc: 0.8203, auc: 0.8932, precision: 0.9388, recall: 0.697\n",
      "2018-12-28T11:20:10.931250, step: 756, loss: 0.4062161445617676, acc: 0.8438, auc: 0.9133, precision: 0.931, recall: 0.7714\n",
      "2018-12-28T11:20:11.496023, step: 757, loss: 0.2498260885477066, acc: 0.8984, auc: 0.9819, precision: 0.9464, recall: 0.8413\n",
      "2018-12-28T11:20:12.074635, step: 758, loss: 0.3479510247707367, acc: 0.8828, auc: 0.9331, precision: 0.8889, recall: 0.875\n",
      "2018-12-28T11:20:12.651485, step: 759, loss: 0.3908708095550537, acc: 0.8672, auc: 0.9063, precision: 0.8438, recall: 0.8852\n",
      "2018-12-28T11:20:13.219398, step: 760, loss: 0.38043633103370667, acc: 0.8281, auc: 0.9077, precision: 0.8361, recall: 0.8095\n",
      "2018-12-28T11:20:13.804090, step: 761, loss: 0.36643946170806885, acc: 0.8359, auc: 0.9091, precision: 0.8406, recall: 0.8529\n",
      "2018-12-28T11:20:14.438001, step: 762, loss: 0.27992844581604004, acc: 0.8906, auc: 0.9649, precision: 0.9231, recall: 0.8696\n",
      "2018-12-28T11:20:15.002106, step: 763, loss: 0.3640531301498413, acc: 0.8359, auc: 0.9199, precision: 0.88, recall: 0.7458\n",
      "2018-12-28T11:20:15.601766, step: 764, loss: 0.3820337653160095, acc: 0.8672, auc: 0.9005, precision: 0.9149, recall: 0.7679\n",
      "2018-12-28T11:20:16.189471, step: 765, loss: 0.3689587116241455, acc: 0.8125, auc: 0.9236, precision: 0.898, recall: 0.6984\n",
      "2018-12-28T11:20:16.820444, step: 766, loss: 0.5578607320785522, acc: 0.7656, auc: 0.8551, precision: 0.9524, recall: 0.5882\n",
      "2018-12-28T11:20:17.397417, step: 767, loss: 0.38504111766815186, acc: 0.8203, auc: 0.9014, precision: 0.881, recall: 0.6727\n",
      "2018-12-28T11:20:18.038885, step: 768, loss: 0.4915896952152252, acc: 0.7812, auc: 0.8725, precision: 0.8889, recall: 0.6349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:20:18.641933, step: 769, loss: 0.33007490634918213, acc: 0.8359, auc: 0.9527, precision: 0.9643, recall: 0.7397\n",
      "2018-12-28T11:20:19.269997, step: 770, loss: 0.2771227955818176, acc: 0.8984, auc: 0.9504, precision: 0.9818, recall: 0.8182\n",
      "2018-12-28T11:20:19.886326, step: 771, loss: 0.2493421733379364, acc: 0.9141, auc: 0.9577, precision: 0.9394, recall: 0.8986\n",
      "2018-12-28T11:20:20.506079, step: 772, loss: 0.3210184872150421, acc: 0.875, auc: 0.9404, precision: 0.875, recall: 0.8448\n",
      "2018-12-28T11:20:21.140232, step: 773, loss: 0.316522479057312, acc: 0.875, auc: 0.9398, precision: 0.9206, recall: 0.8406\n",
      "2018-12-28T11:20:21.707557, step: 774, loss: 0.3606179654598236, acc: 0.8906, auc: 0.9184, precision: 0.871, recall: 0.9\n",
      "2018-12-28T11:20:22.259923, step: 775, loss: 0.2316812425851822, acc: 0.9297, auc: 0.9765, precision: 0.9194, recall: 0.9344\n",
      "start training model\n",
      "2018-12-28T11:20:22.866418, step: 776, loss: 0.21203571557998657, acc: 0.9297, auc: 0.9721, precision: 0.9531, recall: 0.9104\n",
      "2018-12-28T11:20:23.475577, step: 777, loss: 0.2279694825410843, acc: 0.9062, auc: 0.9718, precision: 0.9434, recall: 0.8475\n",
      "2018-12-28T11:20:24.070474, step: 778, loss: 0.26543742418289185, acc: 0.8984, auc: 0.9554, precision: 0.9574, recall: 0.8036\n",
      "2018-12-28T11:20:24.636515, step: 779, loss: 0.2766616940498352, acc: 0.9062, auc: 0.9446, precision: 0.9815, recall: 0.8281\n",
      "2018-12-28T11:20:25.294989, step: 780, loss: 0.35751017928123474, acc: 0.8438, auc: 0.9318, precision: 0.8627, recall: 0.7719\n",
      "2018-12-28T11:20:25.932085, step: 781, loss: 0.26980966329574585, acc: 0.8906, auc: 0.9518, precision: 0.8909, recall: 0.8596\n",
      "2018-12-28T11:20:26.537828, step: 782, loss: 0.31601762771606445, acc: 0.8594, auc: 0.942, precision: 0.9655, recall: 0.7778\n",
      "2018-12-28T11:20:27.127068, step: 783, loss: 0.23101423680782318, acc: 0.9297, auc: 0.9544, precision: 0.9692, recall: 0.9\n",
      "2018-12-28T11:20:27.689550, step: 784, loss: 0.3442903161048889, acc: 0.8906, auc: 0.9385, precision: 0.8387, recall: 0.9286\n",
      "2018-12-28T11:20:28.262052, step: 785, loss: 0.2672373354434967, acc: 0.9062, auc: 0.9645, precision: 0.8875, recall: 0.9595\n",
      "2018-12-28T11:20:28.854658, step: 786, loss: 0.38831135630607605, acc: 0.8594, auc: 0.926, precision: 0.8406, recall: 0.8923\n",
      "2018-12-28T11:20:29.479118, step: 787, loss: 0.4393742084503174, acc: 0.8516, auc: 0.8997, precision: 0.8243, recall: 0.9104\n",
      "2018-12-28T11:20:30.056022, step: 788, loss: 0.43968114256858826, acc: 0.8359, auc: 0.9164, precision: 0.806, recall: 0.871\n",
      "2018-12-28T11:20:30.606231, step: 789, loss: 0.29407933354377747, acc: 0.8672, auc: 0.9516, precision: 0.8551, recall: 0.8939\n",
      "2018-12-28T11:20:31.182658, step: 790, loss: 0.28780460357666016, acc: 0.8594, auc: 0.9545, precision: 0.9242, recall: 0.8243\n",
      "2018-12-28T11:20:31.768891, step: 791, loss: 0.27822378277778625, acc: 0.8906, auc: 0.9563, precision: 0.98, recall: 0.7903\n",
      "2018-12-28T11:20:32.367652, step: 792, loss: 0.2222125381231308, acc: 0.9062, auc: 0.9716, precision: 0.9623, recall: 0.8361\n",
      "2018-12-28T11:20:32.954762, step: 793, loss: 0.2547284662723541, acc: 0.9062, auc: 0.9673, precision: 1.0, recall: 0.8065\n",
      "2018-12-28T11:20:33.591982, step: 794, loss: 0.19017988443374634, acc: 0.9141, auc: 0.9902, precision: 1.0, recall: 0.8226\n",
      "2018-12-28T11:20:34.179928, step: 795, loss: 0.21954894065856934, acc: 0.9219, auc: 0.9702, precision: 0.9492, recall: 0.8889\n",
      "2018-12-28T11:20:34.785459, step: 796, loss: 0.2833004593849182, acc: 0.9141, auc: 0.9512, precision: 0.9344, recall: 0.8906\n",
      "2018-12-28T11:20:35.387169, step: 797, loss: 0.22152449190616608, acc: 0.9297, auc: 0.9775, precision: 0.9206, recall: 0.9355\n",
      "2018-12-28T11:20:35.963972, step: 798, loss: 0.24958735704421997, acc: 0.9375, auc: 0.9634, precision: 0.9118, recall: 0.9688\n",
      "2018-12-28T11:20:36.561848, step: 799, loss: 0.312954843044281, acc: 0.8828, auc: 0.9445, precision: 0.8553, recall: 0.942\n",
      "2018-12-28T11:20:37.158909, step: 800, loss: 0.27094146609306335, acc: 0.8984, auc: 0.9528, precision: 0.9206, recall: 0.8788\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:21:00.052746, step: 800, loss: 0.40252548926755, acc: 0.8386078947368422, auc: 0.9083631578947367, precision: 0.8486157894736838, recall: 0.8310157894736843\n",
      "2018-12-28T11:21:00.659002, step: 801, loss: 0.268330454826355, acc: 0.9062, auc: 0.9475, precision: 0.8788, recall: 0.9355\n",
      "2018-12-28T11:21:01.260459, step: 802, loss: 0.20651954412460327, acc: 0.9297, auc: 0.969, precision: 0.9649, recall: 0.8871\n",
      "2018-12-28T11:21:01.837152, step: 803, loss: 0.26932641863822937, acc: 0.8828, auc: 0.9574, precision: 0.9245, recall: 0.8167\n",
      "2018-12-28T11:21:02.435336, step: 804, loss: 0.2625679075717926, acc: 0.9062, auc: 0.9555, precision: 0.9298, recall: 0.8689\n",
      "2018-12-28T11:21:03.092171, step: 805, loss: 0.3236504793167114, acc: 0.8828, auc: 0.9218, precision: 0.9265, recall: 0.863\n",
      "2018-12-28T11:21:03.739077, step: 806, loss: 0.3377850353717804, acc: 0.8906, auc: 0.9306, precision: 0.8644, recall: 0.8947\n",
      "2018-12-28T11:21:04.372795, step: 807, loss: 0.3078485131263733, acc: 0.8984, auc: 0.9328, precision: 0.9016, recall: 0.8871\n",
      "2018-12-28T11:21:05.004606, step: 808, loss: 0.18429626524448395, acc: 0.9297, auc: 0.9826, precision: 0.9818, recall: 0.871\n",
      "2018-12-28T11:21:05.597199, step: 809, loss: 0.1892969012260437, acc: 0.9375, auc: 0.973, precision: 0.9259, recall: 0.9259\n",
      "2018-12-28T11:21:06.236193, step: 810, loss: 0.31170254945755005, acc: 0.8906, auc: 0.9323, precision: 0.8909, recall: 0.8596\n",
      "2018-12-28T11:21:06.887880, step: 811, loss: 0.21218451857566833, acc: 0.8984, auc: 0.9769, precision: 0.9423, recall: 0.8305\n",
      "2018-12-28T11:21:07.434170, step: 812, loss: 0.3075164556503296, acc: 0.8672, auc: 0.9367, precision: 0.9091, recall: 0.8065\n",
      "2018-12-28T11:21:08.009996, step: 813, loss: 0.25080040097236633, acc: 0.8984, auc: 0.9631, precision: 0.9091, recall: 0.8955\n",
      "2018-12-28T11:21:08.620918, step: 814, loss: 0.36569249629974365, acc: 0.8594, auc: 0.9217, precision: 0.8356, recall: 0.9104\n",
      "2018-12-28T11:21:09.269300, step: 815, loss: 0.31542038917541504, acc: 0.8906, auc: 0.9664, precision: 0.8462, recall: 0.9706\n",
      "2018-12-28T11:21:09.815216, step: 816, loss: 0.3303762674331665, acc: 0.875, auc: 0.9525, precision: 0.8228, recall: 0.9701\n",
      "2018-12-28T11:21:10.366114, step: 817, loss: 0.2680436074733734, acc: 0.9062, auc: 0.9536, precision: 0.9355, recall: 0.8788\n",
      "2018-12-28T11:21:10.935399, step: 818, loss: 0.2238243818283081, acc: 0.9297, auc: 0.9626, precision: 0.913, recall: 0.9545\n",
      "2018-12-28T11:21:11.503857, step: 819, loss: 0.2751212418079376, acc: 0.8828, auc: 0.9502, precision: 0.9032, recall: 0.8615\n",
      "2018-12-28T11:21:12.126553, step: 820, loss: 0.23566529154777527, acc: 0.8984, auc: 0.9653, precision: 0.8814, recall: 0.8966\n",
      "2018-12-28T11:21:12.712245, step: 821, loss: 0.19812341034412384, acc: 0.9375, auc: 0.9775, precision: 1.0, recall: 0.8769\n",
      "2018-12-28T11:21:13.303283, step: 822, loss: 0.28008171916007996, acc: 0.8828, auc: 0.9503, precision: 0.9565, recall: 0.7719\n",
      "2018-12-28T11:21:13.897767, step: 823, loss: 0.22716124355793, acc: 0.9297, auc: 0.9695, precision: 0.9273, recall: 0.9107\n",
      "2018-12-28T11:21:14.492890, step: 824, loss: 0.15797147154808044, acc: 0.9453, auc: 0.9867, precision: 0.9811, recall: 0.8966\n",
      "2018-12-28T11:21:15.136316, step: 825, loss: 0.30650776624679565, acc: 0.8984, auc: 0.9402, precision: 0.9365, recall: 0.8676\n",
      "2018-12-28T11:21:15.732083, step: 826, loss: 0.1819894164800644, acc: 0.9141, auc: 0.9851, precision: 0.9636, recall: 0.8548\n",
      "2018-12-28T11:21:16.278824, step: 827, loss: 0.21257442235946655, acc: 0.9219, auc: 0.9625, precision: 0.9394, recall: 0.9118\n",
      "2018-12-28T11:21:16.888437, step: 828, loss: 0.23602108657360077, acc: 0.9141, auc: 0.9575, precision: 0.9194, recall: 0.9048\n",
      "2018-12-28T11:21:17.509639, step: 829, loss: 0.16373594105243683, acc: 0.9375, auc: 0.9814, precision: 0.9344, recall: 0.9344\n",
      "2018-12-28T11:21:18.203422, step: 830, loss: 0.296395480632782, acc: 0.9219, auc: 0.9382, precision: 0.9091, recall: 0.9375\n",
      "2018-12-28T11:21:18.799245, step: 831, loss: 0.36876368522644043, acc: 0.8828, auc: 0.9246, precision: 0.8714, recall: 0.9104\n",
      "2018-12-28T11:21:19.367000, step: 832, loss: 0.29148006439208984, acc: 0.8984, auc: 0.9443, precision: 0.88, recall: 0.9429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:21:19.937120, step: 833, loss: 0.1385454535484314, acc: 0.9531, auc: 0.9904, precision: 0.95, recall: 0.95\n",
      "2018-12-28T11:21:20.625783, step: 834, loss: 0.3510746955871582, acc: 0.875, auc: 0.9296, precision: 0.8906, recall: 0.8636\n",
      "2018-12-28T11:21:21.213545, step: 835, loss: 0.23071864247322083, acc: 0.9062, auc: 0.9658, precision: 0.9333, recall: 0.875\n",
      "2018-12-28T11:21:21.796576, step: 836, loss: 0.27148932218551636, acc: 0.8828, auc: 0.9544, precision: 0.9508, recall: 0.8286\n",
      "2018-12-28T11:21:22.414050, step: 837, loss: 0.2384152114391327, acc: 0.875, auc: 0.9695, precision: 0.9, recall: 0.8438\n",
      "2018-12-28T11:21:23.007132, step: 838, loss: 0.3183767795562744, acc: 0.875, auc: 0.9301, precision: 0.8833, recall: 0.8548\n",
      "2018-12-28T11:21:23.586066, step: 839, loss: 0.2821207046508789, acc: 0.8828, auc: 0.9555, precision: 0.9538, recall: 0.8378\n",
      "2018-12-28T11:21:24.242521, step: 840, loss: 0.26925238966941833, acc: 0.8672, auc: 0.956, precision: 0.875, recall: 0.7925\n",
      "2018-12-28T11:21:24.857862, step: 841, loss: 0.2844960689544678, acc: 0.8672, auc: 0.9591, precision: 0.9649, recall: 0.7857\n",
      "2018-12-28T11:21:25.434333, step: 842, loss: 0.22351109981536865, acc: 0.8984, auc: 0.9777, precision: 0.9655, recall: 0.8358\n",
      "2018-12-28T11:21:26.027747, step: 843, loss: 0.23889271914958954, acc: 0.8828, auc: 0.9695, precision: 0.9574, recall: 0.7759\n",
      "2018-12-28T11:21:26.597197, step: 844, loss: 0.23059287667274475, acc: 0.9297, auc: 0.9641, precision: 0.95, recall: 0.9048\n",
      "2018-12-28T11:21:27.175820, step: 845, loss: 0.23985739052295685, acc: 0.9062, auc: 0.968, precision: 0.9206, recall: 0.8923\n",
      "2018-12-28T11:21:27.765889, step: 846, loss: 0.16256177425384521, acc: 0.9453, auc: 0.9885, precision: 0.9375, recall: 0.9524\n",
      "2018-12-28T11:21:28.341727, step: 847, loss: 0.2142605483531952, acc: 0.9297, auc: 0.9652, precision: 0.9844, recall: 0.8873\n",
      "2018-12-28T11:21:28.934895, step: 848, loss: 0.3016817569732666, acc: 0.8984, auc: 0.9387, precision: 0.9273, recall: 0.85\n",
      "2018-12-28T11:21:29.515008, step: 849, loss: 0.19120118021965027, acc: 0.9375, auc: 0.9802, precision: 0.9706, recall: 0.9167\n",
      "2018-12-28T11:21:30.080964, step: 850, loss: 0.31729796528816223, acc: 0.8828, auc: 0.9343, precision: 0.931, recall: 0.8308\n",
      "2018-12-28T11:21:30.649524, step: 851, loss: 0.17679591476917267, acc: 0.9453, auc: 0.9702, precision: 0.9828, recall: 0.9048\n",
      "2018-12-28T11:21:31.227064, step: 852, loss: 0.16482695937156677, acc: 0.9375, auc: 0.9792, precision: 0.9661, recall: 0.9048\n",
      "2018-12-28T11:21:31.815628, step: 853, loss: 0.2147062122821808, acc: 0.9141, auc: 0.9712, precision: 0.9649, recall: 0.8594\n",
      "2018-12-28T11:21:32.375482, step: 854, loss: 0.15579062700271606, acc: 0.9453, auc: 0.9885, precision: 0.9833, recall: 0.9077\n",
      "2018-12-28T11:21:32.988513, step: 855, loss: 0.20310574769973755, acc: 0.9219, auc: 0.9688, precision: 0.9574, recall: 0.8491\n",
      "2018-12-28T11:21:33.601897, step: 856, loss: 0.2519342601299286, acc: 0.8984, auc: 0.9638, precision: 0.88, recall: 0.8627\n",
      "2018-12-28T11:21:34.233960, step: 857, loss: 0.20584024488925934, acc: 0.9141, auc: 0.978, precision: 0.9322, recall: 0.8871\n",
      "2018-12-28T11:21:34.871190, step: 858, loss: 0.18124979734420776, acc: 0.9141, auc: 0.9868, precision: 0.9492, recall: 0.875\n",
      "2018-12-28T11:21:35.470299, step: 859, loss: 0.18253156542778015, acc: 0.9219, auc: 0.9831, precision: 0.9683, recall: 0.8841\n",
      "2018-12-28T11:21:36.050236, step: 860, loss: 0.24850742518901825, acc: 0.8828, auc: 0.9636, precision: 0.918, recall: 0.8485\n",
      "2018-12-28T11:21:36.654156, step: 861, loss: 0.23528265953063965, acc: 0.9062, auc: 0.9654, precision: 0.9692, recall: 0.863\n",
      "2018-12-28T11:21:37.228749, step: 862, loss: 0.25006622076034546, acc: 0.8594, auc: 0.9624, precision: 0.9434, recall: 0.7692\n",
      "2018-12-28T11:21:37.807757, step: 863, loss: 0.24951310455799103, acc: 0.9062, auc: 0.9612, precision: 0.92, recall: 0.8519\n",
      "2018-12-28T11:21:38.402844, step: 864, loss: 0.20315580070018768, acc: 0.9453, auc: 0.9703, precision: 0.9552, recall: 0.9412\n",
      "2018-12-28T11:21:38.956800, step: 865, loss: 0.27153849601745605, acc: 0.9141, auc: 0.9628, precision: 0.8906, recall: 0.9344\n",
      "2018-12-28T11:21:39.527900, step: 866, loss: 0.16025668382644653, acc: 0.9453, auc: 0.9872, precision: 0.9636, recall: 0.9138\n",
      "2018-12-28T11:21:40.119308, step: 867, loss: 0.16016319394111633, acc: 0.9531, auc: 0.9817, precision: 0.9577, recall: 0.9577\n",
      "2018-12-28T11:21:40.675483, step: 868, loss: 0.2821659445762634, acc: 0.8828, auc: 0.9542, precision: 0.8824, recall: 0.8955\n",
      "2018-12-28T11:21:41.235363, step: 869, loss: 0.1807309091091156, acc: 0.9375, auc: 0.9775, precision: 0.9815, recall: 0.8833\n",
      "2018-12-28T11:21:41.819426, step: 870, loss: 0.34602969884872437, acc: 0.8594, auc: 0.9355, precision: 0.8696, recall: 0.7692\n",
      "2018-12-28T11:21:42.478771, step: 871, loss: 0.25421032309532166, acc: 0.9062, auc: 0.9619, precision: 0.918, recall: 0.8889\n",
      "2018-12-28T11:21:43.080662, step: 872, loss: 0.26366090774536133, acc: 0.9062, auc: 0.9609, precision: 0.9649, recall: 0.8462\n",
      "2018-12-28T11:21:43.662497, step: 873, loss: 0.3526989817619324, acc: 0.8672, auc: 0.9343, precision: 0.9608, recall: 0.7656\n",
      "2018-12-28T11:21:44.217342, step: 874, loss: 0.2722240090370178, acc: 0.875, auc: 0.9613, precision: 0.9636, recall: 0.791\n",
      "2018-12-28T11:21:44.834519, step: 875, loss: 0.27878114581108093, acc: 0.8828, auc: 0.9533, precision: 0.8889, recall: 0.8421\n",
      "2018-12-28T11:21:45.416499, step: 876, loss: 0.2532171607017517, acc: 0.8984, auc: 0.9617, precision: 0.8594, recall: 0.9322\n",
      "2018-12-28T11:21:45.998351, step: 877, loss: 0.28538820147514343, acc: 0.9062, auc: 0.9588, precision: 0.8636, recall: 0.95\n",
      "2018-12-28T11:21:46.551208, step: 878, loss: 0.15730154514312744, acc: 0.9531, auc: 0.9867, precision: 0.9846, recall: 0.9275\n",
      "2018-12-28T11:21:47.130932, step: 879, loss: 0.27135345339775085, acc: 0.8984, auc: 0.9514, precision: 0.9315, recall: 0.8947\n",
      "2018-12-28T11:21:47.722247, step: 880, loss: 0.2697887420654297, acc: 0.9219, auc: 0.9512, precision: 0.9265, recall: 0.9265\n",
      "2018-12-28T11:21:48.295948, step: 881, loss: 0.26084089279174805, acc: 0.8906, auc: 0.9683, precision: 0.8769, recall: 0.9048\n",
      "2018-12-28T11:21:48.873829, step: 882, loss: 0.2701387405395508, acc: 0.8906, auc: 0.958, precision: 0.8571, recall: 0.8889\n",
      "2018-12-28T11:21:49.426986, step: 883, loss: 0.28483080863952637, acc: 0.875, auc: 0.9488, precision: 0.92, recall: 0.7931\n",
      "2018-12-28T11:21:50.004192, step: 884, loss: 0.29827451705932617, acc: 0.8828, auc: 0.9494, precision: 0.9355, recall: 0.8406\n",
      "2018-12-28T11:21:50.613664, step: 885, loss: 0.36925727128982544, acc: 0.8594, auc: 0.9169, precision: 0.8621, recall: 0.8333\n",
      "2018-12-28T11:21:51.204299, step: 886, loss: 0.26651236414909363, acc: 0.8984, auc: 0.9636, precision: 0.9636, recall: 0.8281\n",
      "2018-12-28T11:21:51.754455, step: 887, loss: 0.25270771980285645, acc: 0.9141, auc: 0.9611, precision: 0.9273, recall: 0.8793\n",
      "2018-12-28T11:21:52.332599, step: 888, loss: 0.23103663325309753, acc: 0.8828, auc: 0.9765, precision: 0.9434, recall: 0.8065\n",
      "2018-12-28T11:21:52.912571, step: 889, loss: 0.2140776515007019, acc: 0.9141, auc: 0.9805, precision: 1.0, recall: 0.8308\n",
      "2018-12-28T11:21:53.479426, step: 890, loss: 0.20915015041828156, acc: 0.8984, auc: 0.9782, precision: 0.9778, recall: 0.7857\n",
      "2018-12-28T11:21:54.073832, step: 891, loss: 0.30950671434402466, acc: 0.8516, auc: 0.9473, precision: 0.9423, recall: 0.7538\n",
      "2018-12-28T11:21:54.685009, step: 892, loss: 0.2926444113254547, acc: 0.8828, auc: 0.9462, precision: 0.9322, recall: 0.8333\n",
      "2018-12-28T11:21:55.291038, step: 893, loss: 0.23038333654403687, acc: 0.9141, auc: 0.9806, precision: 0.88, recall: 0.9706\n",
      "2018-12-28T11:21:55.886906, step: 894, loss: 0.30237749218940735, acc: 0.9062, auc: 0.9416, precision: 0.8955, recall: 0.9231\n",
      "2018-12-28T11:21:56.484803, step: 895, loss: 0.244557186961174, acc: 0.8984, auc: 0.9663, precision: 0.8806, recall: 0.9219\n",
      "2018-12-28T11:21:57.123598, step: 896, loss: 0.2888838052749634, acc: 0.8672, auc: 0.9506, precision: 0.8857, recall: 0.8732\n",
      "2018-12-28T11:21:57.776903, step: 897, loss: 0.22249945998191833, acc: 0.9219, auc: 0.9684, precision: 0.9254, recall: 0.9254\n",
      "2018-12-28T11:21:58.385277, step: 898, loss: 0.19203566014766693, acc: 0.9297, auc: 0.9785, precision: 0.9385, recall: 0.9242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T11:21:58.932226, step: 899, loss: 0.2235698252916336, acc: 0.9219, auc: 0.9611, precision: 0.9333, recall: 0.9032\n",
      "2018-12-28T11:21:59.483407, step: 900, loss: 0.19065946340560913, acc: 0.9297, auc: 0.9826, precision: 0.9483, recall: 0.9016\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T11:22:21.905910, step: 900, loss: 0.36856716086989955, acc: 0.8449947368421055, auc: 0.9262999999999999, precision: 0.8937289473684209, recall: 0.7873894736842105\n",
      "2018-12-28T11:22:22.548822, step: 901, loss: 0.23797687888145447, acc: 0.9141, auc: 0.9643, precision: 0.9636, recall: 0.8548\n",
      "2018-12-28T11:22:23.149891, step: 902, loss: 0.2272333800792694, acc: 0.9141, auc: 0.9662, precision: 0.9701, recall: 0.8784\n",
      "2018-12-28T11:22:23.740817, step: 903, loss: 0.23763276636600494, acc: 0.9141, auc: 0.9651, precision: 0.918, recall: 0.9032\n",
      "2018-12-28T11:22:24.307419, step: 904, loss: 0.33594149351119995, acc: 0.8516, auc: 0.9299, precision: 0.8929, recall: 0.7937\n",
      "2018-12-28T11:22:24.879804, step: 905, loss: 0.24715924263000488, acc: 0.8984, auc: 0.9699, precision: 0.875, recall: 0.918\n",
      "2018-12-28T11:22:25.471278, step: 906, loss: 0.23041662573814392, acc: 0.9219, auc: 0.9636, precision: 0.9194, recall: 0.9194\n",
      "2018-12-28T11:22:26.072003, step: 907, loss: 0.26181560754776, acc: 0.8906, auc: 0.9497, precision: 0.9194, recall: 0.8636\n",
      "2018-12-28T11:22:26.636920, step: 908, loss: 0.22173169255256653, acc: 0.9375, auc: 0.9574, precision: 0.9403, recall: 0.9403\n",
      "2018-12-28T11:22:27.198879, step: 909, loss: 0.13356024026870728, acc: 0.9609, auc: 0.9847, precision: 0.9577, recall: 0.9714\n",
      "2018-12-28T11:22:27.778881, step: 910, loss: 0.1711585521697998, acc: 0.9219, auc: 0.9862, precision: 0.971, recall: 0.8933\n",
      "2018-12-28T11:22:28.332910, step: 911, loss: 0.18519774079322815, acc: 0.9375, auc: 0.9786, precision: 0.9552, recall: 0.9275\n",
      "2018-12-28T11:22:28.921658, step: 912, loss: 0.24319443106651306, acc: 0.9219, auc: 0.966, precision: 0.9048, recall: 0.9344\n",
      "2018-12-28T11:22:29.539691, step: 913, loss: 0.2667255997657776, acc: 0.9297, auc: 0.9551, precision: 0.92, recall: 0.9583\n",
      "2018-12-28T11:22:30.123848, step: 914, loss: 0.26703664660453796, acc: 0.9141, auc: 0.9688, precision: 0.8429, recall: 1.0\n",
      "2018-12-28T11:22:30.698832, step: 915, loss: 0.24702675640583038, acc: 0.9141, auc: 0.9564, precision: 0.9118, recall: 0.9254\n",
      "2018-12-28T11:22:31.242578, step: 916, loss: 0.21127581596374512, acc: 0.9297, auc: 0.9632, precision: 0.9403, recall: 0.9265\n",
      "2018-12-28T11:22:31.833949, step: 917, loss: 0.2540671229362488, acc: 0.9219, auc: 0.9545, precision: 0.9123, recall: 0.9123\n",
      "2018-12-28T11:22:32.426747, step: 918, loss: 0.32359087467193604, acc: 0.8906, auc: 0.9406, precision: 0.9333, recall: 0.8485\n",
      "2018-12-28T11:22:33.012931, step: 919, loss: 0.17929986119270325, acc: 0.9141, auc: 0.9804, precision: 0.9828, recall: 0.8507\n",
      "2018-12-28T11:22:33.579373, step: 920, loss: 0.3655310869216919, acc: 0.8359, auc: 0.9567, precision: 0.9672, recall: 0.7564\n",
      "2018-12-28T11:22:34.164020, step: 921, loss: 0.35871607065200806, acc: 0.8438, auc: 0.9804, precision: 1.0, recall: 0.6774\n",
      "2018-12-28T11:22:34.717806, step: 922, loss: 0.3006053566932678, acc: 0.8516, auc: 0.965, precision: 0.9756, recall: 0.6897\n",
      "2018-12-28T11:22:35.374949, step: 923, loss: 0.2837521433830261, acc: 0.8438, auc: 0.9618, precision: 0.9318, recall: 0.7069\n",
      "2018-12-28T11:22:36.016059, step: 924, loss: 0.24036559462547302, acc: 0.8984, auc: 0.9733, precision: 0.9661, recall: 0.8382\n",
      "2018-12-28T11:22:36.622538, step: 925, loss: 0.2491443157196045, acc: 0.9062, auc: 0.9645, precision: 0.9286, recall: 0.8667\n",
      "2018-12-28T11:22:37.198872, step: 926, loss: 0.3300195336341858, acc: 0.8672, auc: 0.9383, precision: 0.8824, recall: 0.8696\n",
      "2018-12-28T11:22:37.756771, step: 927, loss: 0.3653794229030609, acc: 0.8672, auc: 0.9382, precision: 0.8485, recall: 0.8889\n",
      "2018-12-28T11:22:38.365581, step: 928, loss: 0.2602892816066742, acc: 0.9375, auc: 0.9878, precision: 0.9, recall: 0.9844\n",
      "2018-12-28T11:22:39.038022, step: 929, loss: 0.266063392162323, acc: 0.9219, auc: 0.9775, precision: 0.9167, recall: 0.9167\n",
      "2018-12-28T11:22:39.621413, step: 930, loss: 0.3135196268558502, acc: 0.8828, auc: 0.9616, precision: 0.8806, recall: 0.8939\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTM(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
