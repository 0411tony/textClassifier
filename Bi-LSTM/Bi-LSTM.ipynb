{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 6\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # 单层LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTM(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "        concatedOutput = tf.concat(outputs, 2)\n",
    "        \n",
    "        # 去除最后时间步的输出作为全连接的输入\n",
    "        finalOutput = concatedOutput[:, -1, :]\n",
    "        \n",
    "        outputSize = config.model.hiddenSizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(finalOutput, [-1, outputSize])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-02T16:59:18.832380, step: 1, loss: 0.6918196678161621, acc: 0.4922, auc: 0.5429, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:19.225578, step: 2, loss: 0.7008865475654602, acc: 0.5547, auc: 0.4769, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:19.634123, step: 3, loss: 0.6865664124488831, acc: 0.4453, auc: 0.6281, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:20.018214, step: 4, loss: 0.6992751359939575, acc: 0.5703, auc: 0.4665, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:20.381714, step: 5, loss: 0.6875778436660767, acc: 0.5078, auc: 0.6327, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:20.762952, step: 6, loss: 0.6850719451904297, acc: 0.4766, auc: 0.6012, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:21.183105, step: 7, loss: 0.6895829439163208, acc: 0.4219, auc: 0.5305, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:21.585431, step: 8, loss: 0.689278781414032, acc: 0.5078, auc: 0.5941, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:21.974914, step: 9, loss: 0.6903138160705566, acc: 0.5625, auc: 0.5546, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:22.383269, step: 10, loss: 0.6888094544410706, acc: 0.4766, auc: 0.5283, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:22.846547, step: 11, loss: 0.6938868761062622, acc: 0.5781, auc: 0.4705, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:23.258499, step: 12, loss: 0.6861361265182495, acc: 0.4922, auc: 0.5767, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:23.663312, step: 13, loss: 0.6740270256996155, acc: 0.4531, auc: 0.697, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:24.071660, step: 14, loss: 0.6872490644454956, acc: 0.5312, auc: 0.6098, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:24.462707, step: 15, loss: 0.6866103410720825, acc: 0.4219, auc: 0.6013, precision: 1.0, recall: 0.0133\n",
      "2019-01-02T16:59:24.895004, step: 16, loss: 0.6966150999069214, acc: 0.5547, auc: 0.4935, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:59:25.344042, step: 17, loss: 0.6797671318054199, acc: 0.5469, auc: 0.6003, precision: 1.0, recall: 0.0169\n",
      "2019-01-02T16:59:25.741562, step: 18, loss: 0.6672235727310181, acc: 0.5234, auc: 0.6676, precision: 1.0, recall: 0.0317\n",
      "2019-01-02T16:59:26.133470, step: 19, loss: 0.6347751021385193, acc: 0.5312, auc: 0.7214, precision: 0.8571, recall: 0.0923\n",
      "2019-01-02T16:59:26.515096, step: 20, loss: 0.6344015002250671, acc: 0.5859, auc: 0.7468, precision: 1.0, recall: 0.209\n",
      "2019-01-02T16:59:26.898256, step: 21, loss: 0.6524057388305664, acc: 0.5781, auc: 0.7158, precision: 0.6316, recall: 0.375\n",
      "2019-01-02T16:59:27.310048, step: 22, loss: 0.6772264838218689, acc: 0.5, auc: 0.5834, precision: 0.5814, recall: 0.3521\n",
      "2019-01-02T16:59:27.703493, step: 23, loss: 0.6526060104370117, acc: 0.5547, auc: 0.7074, precision: 0.6667, recall: 0.3478\n",
      "2019-01-02T16:59:28.140298, step: 24, loss: 0.664706826210022, acc: 0.5312, auc: 0.6367, precision: 0.6, recall: 0.3134\n",
      "2019-01-02T16:59:28.554380, step: 25, loss: 0.6391100883483887, acc: 0.5, auc: 0.6998, precision: 0.8947, recall: 0.2152\n",
      "2019-01-02T16:59:28.950503, step: 26, loss: 0.523654580116272, acc: 0.7031, auc: 0.8493, precision: 0.9474, recall: 0.3273\n",
      "2019-01-02T16:59:29.340059, step: 27, loss: 0.5724747776985168, acc: 0.6328, auc: 0.7991, precision: 0.8333, recall: 0.2542\n",
      "2019-01-02T16:59:29.743442, step: 28, loss: 0.5014218688011169, acc: 0.7188, auc: 0.8545, precision: 0.8462, recall: 0.5238\n",
      "2019-01-02T16:59:30.167516, step: 29, loss: 0.5432206392288208, acc: 0.7656, auc: 0.7947, precision: 0.7213, recall: 0.7719\n",
      "2019-01-02T16:59:30.575425, step: 30, loss: 0.6078780889511108, acc: 0.7188, auc: 0.7309, precision: 0.7193, recall: 0.6721\n",
      "2019-01-02T16:59:30.982414, step: 31, loss: 0.5655314922332764, acc: 0.7422, auc: 0.8121, precision: 0.8491, recall: 0.6429\n",
      "2019-01-02T16:59:31.354625, step: 32, loss: 0.5933058261871338, acc: 0.7344, auc: 0.7199, precision: 0.7222, recall: 0.7879\n",
      "2019-01-02T16:59:31.754413, step: 33, loss: 0.5006869435310364, acc: 0.8047, auc: 0.842, precision: 0.8519, recall: 0.7302\n",
      "2019-01-02T16:59:32.145898, step: 34, loss: 0.5401245951652527, acc: 0.7578, auc: 0.7971, precision: 0.8654, recall: 0.6522\n",
      "2019-01-02T16:59:32.518803, step: 35, loss: 0.6283459663391113, acc: 0.6719, auc: 0.6932, precision: 0.7742, recall: 0.4068\n",
      "2019-01-02T16:59:32.939500, step: 36, loss: 0.5183532238006592, acc: 0.7891, auc: 0.8299, precision: 0.7959, recall: 0.6964\n",
      "2019-01-02T16:59:33.338525, step: 37, loss: 0.5321081876754761, acc: 0.7656, auc: 0.8579, precision: 0.7778, recall: 0.7538\n",
      "2019-01-02T16:59:33.739383, step: 38, loss: 0.5348511934280396, acc: 0.7812, auc: 0.8227, precision: 0.85, recall: 0.6071\n",
      "2019-01-02T16:59:34.118964, step: 39, loss: 0.6125377416610718, acc: 0.6562, auc: 0.7409, precision: 0.8, recall: 0.4706\n",
      "2019-01-02T16:59:34.512542, step: 40, loss: 0.48897311091423035, acc: 0.7891, auc: 0.8261, precision: 0.8519, recall: 0.7077\n",
      "2019-01-02T16:59:34.924231, step: 41, loss: 0.5017338991165161, acc: 0.7891, auc: 0.8623, precision: 0.7808, recall: 0.8382\n",
      "2019-01-02T16:59:35.308442, step: 42, loss: 0.5841110944747925, acc: 0.7656, auc: 0.7768, precision: 0.7188, recall: 0.9583\n",
      "2019-01-02T16:59:35.734274, step: 43, loss: 0.5634380578994751, acc: 0.7734, auc: 0.8236, precision: 0.7126, recall: 0.9394\n",
      "2019-01-02T16:59:36.140441, step: 44, loss: 0.5007107257843018, acc: 0.8203, auc: 0.8415, precision: 0.7941, recall: 0.8571\n",
      "2019-01-02T16:59:36.555209, step: 45, loss: 0.4628232717514038, acc: 0.8281, auc: 0.8994, precision: 0.8333, recall: 0.7407\n",
      "2019-01-02T16:59:36.948945, step: 46, loss: 0.5531033277511597, acc: 0.6953, auc: 0.7988, precision: 0.9286, recall: 0.2549\n",
      "2019-01-02T16:59:37.363998, step: 47, loss: 0.6823548674583435, acc: 0.5859, auc: 0.7579, precision: 0.7778, recall: 0.1207\n",
      "2019-01-02T16:59:37.775696, step: 48, loss: 0.7074320912361145, acc: 0.5703, auc: 0.7556, precision: 1.0, recall: 0.1912\n",
      "2019-01-02T16:59:38.163829, step: 49, loss: 0.552437961101532, acc: 0.7344, auc: 0.8391, precision: 0.9189, recall: 0.5231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:59:38.574662, step: 50, loss: 0.44961315393447876, acc: 0.8359, auc: 0.8755, precision: 0.8704, recall: 0.7705\n",
      "2019-01-02T16:59:38.966104, step: 51, loss: 0.48873838782310486, acc: 0.8203, auc: 0.8762, precision: 0.72, recall: 0.9643\n",
      "2019-01-02T16:59:39.366052, step: 52, loss: 0.6200796365737915, acc: 0.6797, auc: 0.7782, precision: 0.5957, recall: 0.9492\n",
      "2019-01-02T16:59:39.827507, step: 53, loss: 0.506675124168396, acc: 0.7812, auc: 0.8385, precision: 0.7407, recall: 0.8955\n",
      "2019-01-02T16:59:40.220504, step: 54, loss: 0.48574212193489075, acc: 0.7969, auc: 0.8354, precision: 0.8, recall: 0.8\n",
      "2019-01-02T16:59:40.634257, step: 55, loss: 0.5988105535507202, acc: 0.7422, auc: 0.7713, precision: 0.8644, recall: 0.6711\n",
      "2019-01-02T16:59:41.060946, step: 56, loss: 0.6629310846328735, acc: 0.7031, auc: 0.7243, precision: 0.7414, recall: 0.6515\n",
      "2019-01-02T16:59:41.482556, step: 57, loss: 0.5766755938529968, acc: 0.75, auc: 0.8087, precision: 0.9048, recall: 0.5758\n",
      "2019-01-02T16:59:41.908403, step: 58, loss: 0.7362514734268188, acc: 0.6484, auc: 0.6869, precision: 0.9545, recall: 0.3231\n",
      "2019-01-02T16:59:42.338270, step: 59, loss: 0.8149441480636597, acc: 0.5859, auc: 0.598, precision: 0.913, recall: 0.2917\n",
      "2019-01-02T16:59:42.708567, step: 60, loss: 0.7453573942184448, acc: 0.625, auc: 0.6234, precision: 0.9474, recall: 0.2769\n",
      "2019-01-02T16:59:43.125075, step: 61, loss: 0.8060382604598999, acc: 0.5391, auc: 0.592, precision: 0.9375, recall: 0.2055\n",
      "2019-01-02T16:59:43.520976, step: 62, loss: 0.7744240760803223, acc: 0.5391, auc: 0.55, precision: 1.0, recall: 0.1194\n",
      "2019-01-02T16:59:43.927907, step: 63, loss: 0.7285723686218262, acc: 0.5312, auc: 0.5861, precision: 0.8462, recall: 0.1594\n",
      "2019-01-02T16:59:44.372036, step: 64, loss: 0.6995718479156494, acc: 0.5781, auc: 0.5473, precision: 0.9091, recall: 0.1587\n",
      "2019-01-02T16:59:44.775447, step: 65, loss: 0.6570472717285156, acc: 0.625, auc: 0.5495, precision: 1.0, recall: 0.0588\n",
      "2019-01-02T16:59:45.164099, step: 66, loss: 0.6804416179656982, acc: 0.5547, auc: 0.5406, precision: 1.0, recall: 0.0806\n",
      "2019-01-02T16:59:45.537512, step: 67, loss: 0.6953890323638916, acc: 0.4609, auc: 0.5416, precision: 1.0, recall: 0.08\n",
      "2019-01-02T16:59:45.938323, step: 68, loss: 0.6647751331329346, acc: 0.5312, auc: 0.6299, precision: 1.0, recall: 0.0625\n",
      "2019-01-02T16:59:46.338816, step: 69, loss: 0.6871404051780701, acc: 0.5312, auc: 0.5274, precision: 0.75, recall: 0.0484\n",
      "2019-01-02T16:59:46.732788, step: 70, loss: 0.7016734480857849, acc: 0.5312, auc: 0.5063, precision: 0.3333, recall: 0.0169\n",
      "2019-01-02T16:59:47.130670, step: 71, loss: 0.6682183146476746, acc: 0.4922, auc: 0.6088, precision: 1.0, recall: 0.0845\n",
      "2019-01-02T16:59:47.535284, step: 72, loss: 0.6990170478820801, acc: 0.5938, auc: 0.5445, precision: 0.6667, recall: 0.0741\n",
      "2019-01-02T16:59:47.935658, step: 73, loss: 0.7031916975975037, acc: 0.5234, auc: 0.4713, precision: 0.6429, recall: 0.1385\n",
      "2019-01-02T16:59:48.343703, step: 74, loss: 0.6878223419189453, acc: 0.5781, auc: 0.6339, precision: 0.7, recall: 0.2258\n",
      "2019-01-02T16:59:48.738135, step: 75, loss: 0.7052267789840698, acc: 0.5156, auc: 0.5302, precision: 0.5294, recall: 0.1429\n",
      "2019-01-02T16:59:49.128792, step: 76, loss: 0.6966133117675781, acc: 0.4844, auc: 0.5431, precision: 0.5417, recall: 0.1912\n",
      "2019-01-02T16:59:49.539343, step: 77, loss: 0.6888304948806763, acc: 0.4922, auc: 0.4875, precision: 0.6667, recall: 0.169\n",
      "2019-01-02T16:59:49.938616, step: 78, loss: 0.7037190198898315, acc: 0.6094, auc: 0.5511, precision: 0.625, recall: 0.2679\n",
      "2019-01-02T16:59:50.317399, step: 79, loss: 0.6925758719444275, acc: 0.5625, auc: 0.5841, precision: 0.7143, recall: 0.0847\n",
      "2019-01-02T16:59:50.708970, step: 80, loss: 0.6746736764907837, acc: 0.5703, auc: 0.615, precision: 0.7778, recall: 0.1167\n",
      "2019-01-02T16:59:51.165585, step: 81, loss: 0.6882858276367188, acc: 0.4922, auc: 0.5278, precision: 0.5833, recall: 0.1045\n",
      "2019-01-02T16:59:51.592642, step: 82, loss: 0.6811802387237549, acc: 0.5391, auc: 0.5488, precision: 0.8571, recall: 0.0938\n",
      "2019-01-02T16:59:52.012182, step: 83, loss: 0.6783921122550964, acc: 0.5703, auc: 0.5451, precision: 0.8333, recall: 0.1587\n",
      "2019-01-02T16:59:52.467945, step: 84, loss: 0.6457703709602356, acc: 0.5781, auc: 0.6151, precision: 1.0, recall: 0.194\n",
      "2019-01-02T16:59:52.893741, step: 85, loss: 0.6550465226173401, acc: 0.625, auc: 0.6262, precision: 0.9, recall: 0.1607\n",
      "2019-01-02T16:59:53.332979, step: 86, loss: 0.6751065254211426, acc: 0.5391, auc: 0.5454, precision: 0.8571, recall: 0.0938\n",
      "2019-01-02T16:59:53.753515, step: 87, loss: 0.6562069654464722, acc: 0.5781, auc: 0.6125, precision: 0.875, recall: 0.1167\n",
      "2019-01-02T16:59:54.169297, step: 88, loss: 0.6569462418556213, acc: 0.5859, auc: 0.5602, precision: 1.0, recall: 0.1587\n",
      "2019-01-02T16:59:54.550169, step: 89, loss: 0.6460810899734497, acc: 0.5547, auc: 0.6161, precision: 1.0, recall: 0.1364\n",
      "2019-01-02T16:59:54.953647, step: 90, loss: 0.6750322580337524, acc: 0.5781, auc: 0.5787, precision: 0.8182, recall: 0.1475\n",
      "2019-01-02T16:59:55.382194, step: 91, loss: 0.7071443796157837, acc: 0.5312, auc: 0.4362, precision: 1.0, recall: 0.0323\n",
      "2019-01-02T16:59:55.778855, step: 92, loss: 0.7029601335525513, acc: 0.5156, auc: 0.5071, precision: 0.8333, recall: 0.0758\n",
      "2019-01-02T16:59:56.169441, step: 93, loss: 0.6531010866165161, acc: 0.5703, auc: 0.62, precision: 0.8889, recall: 0.129\n",
      "2019-01-02T16:59:56.556186, step: 94, loss: 0.6551942825317383, acc: 0.5703, auc: 0.6056, precision: 0.875, recall: 0.209\n",
      "2019-01-02T16:59:56.958849, step: 95, loss: 0.6371289491653442, acc: 0.6016, auc: 0.6487, precision: 1.0, recall: 0.1356\n",
      "2019-01-02T16:59:57.372866, step: 96, loss: 0.6905620694160461, acc: 0.5312, auc: 0.5777, precision: 0.8182, recall: 0.1343\n",
      "2019-01-02T16:59:57.776490, step: 97, loss: 0.6196107268333435, acc: 0.6172, auc: 0.6807, precision: 1.0, recall: 0.1695\n",
      "2019-01-02T16:59:58.163608, step: 98, loss: 0.635976254940033, acc: 0.625, auc: 0.6207, precision: 0.9333, recall: 0.2295\n",
      "2019-01-02T16:59:58.601524, step: 99, loss: 0.631371259689331, acc: 0.5625, auc: 0.6271, precision: 1.0, recall: 0.2\n",
      "2019-01-02T16:59:59.025377, step: 100, loss: 0.6744961738586426, acc: 0.6016, auc: 0.5893, precision: 0.7857, recall: 0.1864\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:00:14.996941, step: 100, loss: 0.6698680581190647, acc: 0.5528897435897436, auc: 0.6604179487179486, precision: 0.8566948717948719, recall: 0.1383974358974359\n",
      "2019-01-02T17:00:15.389323, step: 101, loss: 0.6475458145141602, acc: 0.6016, auc: 0.5914, precision: 1.0, recall: 0.1207\n",
      "2019-01-02T17:00:15.801554, step: 102, loss: 0.7217215895652771, acc: 0.5312, auc: 0.5646, precision: 0.6364, recall: 0.1111\n",
      "2019-01-02T17:00:16.190226, step: 103, loss: 0.6430993676185608, acc: 0.5234, auc: 0.6716, precision: 1.0, recall: 0.1528\n",
      "2019-01-02T17:00:16.612592, step: 104, loss: 0.6959649920463562, acc: 0.5391, auc: 0.5751, precision: 0.75, recall: 0.1385\n",
      "2019-01-02T17:00:17.009237, step: 105, loss: 0.6593753695487976, acc: 0.5078, auc: 0.5767, precision: 1.0, recall: 0.1711\n",
      "2019-01-02T17:00:17.403956, step: 106, loss: 0.6745789051055908, acc: 0.5391, auc: 0.561, precision: 0.8333, recall: 0.1493\n",
      "2019-01-02T17:00:17.795712, step: 107, loss: 0.6770642995834351, acc: 0.5625, auc: 0.5711, precision: 0.875, recall: 0.1129\n",
      "2019-01-02T17:00:18.198301, step: 108, loss: 0.6843410134315491, acc: 0.5859, auc: 0.5621, precision: 0.7273, recall: 0.1379\n",
      "2019-01-02T17:00:18.611636, step: 109, loss: 0.6747342944145203, acc: 0.5625, auc: 0.5809, precision: 0.8333, recall: 0.2206\n",
      "2019-01-02T17:00:19.021627, step: 110, loss: 0.6510346531867981, acc: 0.5234, auc: 0.5905, precision: 1.0, recall: 0.1644\n",
      "2019-01-02T17:00:19.414849, step: 111, loss: 0.644855260848999, acc: 0.5625, auc: 0.5877, precision: 1.0, recall: 0.1642\n",
      "2019-01-02T17:00:19.858626, step: 112, loss: 0.6799625158309937, acc: 0.5312, auc: 0.5914, precision: 0.7692, recall: 0.1493\n",
      "2019-01-02T17:00:20.288951, step: 113, loss: 0.6772053241729736, acc: 0.5547, auc: 0.5771, precision: 0.8182, recall: 0.1406\n",
      "2019-01-02T17:00:20.735018, step: 114, loss: 0.6490408182144165, acc: 0.5703, auc: 0.6161, precision: 0.9167, recall: 0.1692\n",
      "2019-01-02T17:00:21.149915, step: 115, loss: 0.6797159910202026, acc: 0.5625, auc: 0.5346, precision: 0.8889, recall: 0.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:00:21.578079, step: 116, loss: 0.6729180216789246, acc: 0.5625, auc: 0.5586, precision: 0.8333, recall: 0.1562\n",
      "2019-01-02T17:00:21.986325, step: 117, loss: 0.6651807427406311, acc: 0.5703, auc: 0.5597, precision: 0.9, recall: 0.1429\n",
      "2019-01-02T17:00:22.360390, step: 118, loss: 0.6686782836914062, acc: 0.6172, auc: 0.5331, precision: 0.8667, recall: 0.2167\n",
      "2019-01-02T17:00:22.784244, step: 119, loss: 0.6412840485572815, acc: 0.5312, auc: 0.6424, precision: 1.0, recall: 0.1667\n",
      "2019-01-02T17:00:23.160363, step: 120, loss: 0.6676203608512878, acc: 0.5625, auc: 0.5994, precision: 0.9, recall: 0.1406\n",
      "2019-01-02T17:00:23.570971, step: 121, loss: 0.6119674444198608, acc: 0.5938, auc: 0.6863, precision: 1.0, recall: 0.2464\n",
      "2019-01-02T17:00:23.983651, step: 122, loss: 0.6208840608596802, acc: 0.625, auc: 0.6862, precision: 0.9474, recall: 0.2769\n",
      "2019-01-02T17:00:24.364589, step: 123, loss: 0.640465259552002, acc: 0.5391, auc: 0.621, precision: 1.0, recall: 0.169\n",
      "2019-01-02T17:00:24.781781, step: 124, loss: 0.6466420292854309, acc: 0.6484, auc: 0.5813, precision: 1.0, recall: 0.1818\n",
      "2019-01-02T17:00:25.202738, step: 125, loss: 0.631916880607605, acc: 0.5625, auc: 0.6809, precision: 1.0, recall: 0.1765\n",
      "2019-01-02T17:00:25.592653, step: 126, loss: 0.6888495683670044, acc: 0.5312, auc: 0.5239, precision: 0.8, recall: 0.1212\n",
      "2019-01-02T17:00:26.032599, step: 127, loss: 0.656472384929657, acc: 0.6172, auc: 0.502, precision: 1.0, recall: 0.1833\n",
      "2019-01-02T17:00:26.425991, step: 128, loss: 0.6523392200469971, acc: 0.625, auc: 0.6335, precision: 0.8235, recall: 0.2373\n",
      "2019-01-02T17:00:26.827353, step: 129, loss: 0.6683263778686523, acc: 0.5547, auc: 0.5355, precision: 1.0, recall: 0.0952\n",
      "2019-01-02T17:00:27.234837, step: 130, loss: 0.6506994962692261, acc: 0.5781, auc: 0.6049, precision: 1.0, recall: 0.1429\n",
      "2019-01-02T17:00:27.654686, step: 131, loss: 0.5962973833084106, acc: 0.6953, auc: 0.7049, precision: 1.0, recall: 0.2642\n",
      "2019-01-02T17:00:28.049106, step: 132, loss: 0.6511634588241577, acc: 0.5859, auc: 0.5944, precision: 0.875, recall: 0.2154\n",
      "2019-01-02T17:00:28.448252, step: 133, loss: 0.6226810812950134, acc: 0.6094, auc: 0.6252, precision: 1.0, recall: 0.2063\n",
      "2019-01-02T17:00:28.840523, step: 134, loss: 0.6778087615966797, acc: 0.6016, auc: 0.6614, precision: 0.6667, recall: 0.1111\n",
      "2019-01-02T17:00:29.209167, step: 135, loss: 0.677241861820221, acc: 0.5625, auc: 0.5493, precision: 0.8333, recall: 0.0833\n",
      "2019-01-02T17:00:29.610078, step: 136, loss: 0.6201925277709961, acc: 0.6094, auc: 0.6129, precision: 1.0, recall: 0.2424\n",
      "2019-01-02T17:00:29.987560, step: 137, loss: 0.6931952834129333, acc: 0.5234, auc: 0.5094, precision: 0.8, recall: 0.1194\n",
      "2019-01-02T17:00:30.392260, step: 138, loss: 0.6747497320175171, acc: 0.5703, auc: 0.5231, precision: 0.875, recall: 0.1148\n",
      "2019-01-02T17:00:30.811192, step: 139, loss: 0.5878483057022095, acc: 0.6797, auc: 0.746, precision: 0.9412, recall: 0.2857\n",
      "2019-01-02T17:00:31.243923, step: 140, loss: 0.6806431412696838, acc: 0.5859, auc: 0.4917, precision: 0.8462, recall: 0.1774\n",
      "2019-01-02T17:00:31.689633, step: 141, loss: 0.6914446353912354, acc: 0.5156, auc: 0.601, precision: 0.7895, recall: 0.2055\n",
      "2019-01-02T17:00:32.124436, step: 142, loss: 0.5756745338439941, acc: 0.6797, auc: 0.7189, precision: 0.9286, recall: 0.4\n",
      "2019-01-02T17:00:32.585486, step: 143, loss: 0.6136524081230164, acc: 0.75, auc: 0.7502, precision: 0.8103, recall: 0.6912\n",
      "2019-01-02T17:00:33.014032, step: 144, loss: 0.7148737907409668, acc: 0.7422, auc: 0.7454, precision: 0.7101, recall: 0.7903\n",
      "2019-01-02T17:00:33.459182, step: 145, loss: 0.7396759986877441, acc: 0.7109, auc: 0.7155, precision: 0.6923, recall: 0.875\n",
      "2019-01-02T17:00:33.883531, step: 146, loss: 0.8901939392089844, acc: 0.6172, auc: 0.6603, precision: 0.5673, recall: 0.9365\n",
      "2019-01-02T17:00:34.308343, step: 147, loss: 0.7180367708206177, acc: 0.6719, auc: 0.7769, precision: 0.6064, recall: 0.9194\n",
      "2019-01-02T17:00:34.715984, step: 148, loss: 0.6146520376205444, acc: 0.7422, auc: 0.7842, precision: 0.6824, recall: 0.9062\n",
      "2019-01-02T17:00:35.133081, step: 149, loss: 0.6568900346755981, acc: 0.6719, auc: 0.7135, precision: 0.6118, recall: 0.8525\n",
      "2019-01-02T17:00:35.571163, step: 150, loss: 0.6481579542160034, acc: 0.6406, auc: 0.6924, precision: 0.619, recall: 0.6393\n",
      "2019-01-02T17:00:35.965101, step: 151, loss: 0.6083664894104004, acc: 0.6953, auc: 0.7732, precision: 0.725, recall: 0.5088\n",
      "2019-01-02T17:00:36.338419, step: 152, loss: 0.5930743217468262, acc: 0.6094, auc: 0.8059, precision: 0.7917, recall: 0.2969\n",
      "2019-01-02T17:00:36.745650, step: 153, loss: 0.6079199910163879, acc: 0.5312, auc: 0.7815, precision: 0.875, recall: 0.1944\n",
      "2019-01-02T17:00:37.156721, step: 154, loss: 0.5889077186584473, acc: 0.5859, auc: 0.7929, precision: 0.8, recall: 0.1356\n",
      "2019-01-02T17:00:37.552888, step: 155, loss: 0.6161304712295532, acc: 0.5547, auc: 0.7441, precision: 0.75, recall: 0.0984\n",
      "2019-01-02T17:00:37.965350, step: 156, loss: 0.5494887232780457, acc: 0.5781, auc: 0.8586, precision: 0.8889, recall: 0.1311\n",
      "start training model\n",
      "2019-01-02T17:00:38.450451, step: 157, loss: 0.5900511741638184, acc: 0.6172, auc: 0.7755, precision: 1.0, recall: 0.1695\n",
      "2019-01-02T17:00:38.887006, step: 158, loss: 0.7364190816879272, acc: 0.4453, auc: 0.6414, precision: 1.0, recall: 0.0139\n",
      "2019-01-02T17:00:39.313197, step: 159, loss: 0.7213492393493652, acc: 0.5234, auc: 0.6298, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:39.765753, step: 160, loss: 0.7725287675857544, acc: 0.5, auc: 0.5249, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:40.163489, step: 161, loss: 0.7873488664627075, acc: 0.4688, auc: 0.5654, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:40.610218, step: 162, loss: 0.7343219518661499, acc: 0.5391, auc: 0.5144, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:40.999458, step: 163, loss: 0.7210253477096558, acc: 0.5234, auc: 0.586, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:41.435657, step: 164, loss: 0.7167387008666992, acc: 0.5234, auc: 0.577, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:41.841933, step: 165, loss: 0.6843327283859253, acc: 0.5781, auc: 0.5611, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:42.287996, step: 166, loss: 0.7293001413345337, acc: 0.4922, auc: 0.5206, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:42.733313, step: 167, loss: 0.7582172155380249, acc: 0.3984, auc: 0.5618, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:43.196515, step: 168, loss: 0.6817907094955444, acc: 0.5234, auc: 0.6234, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:43.650014, step: 169, loss: 0.6766038537025452, acc: 0.5156, auc: 0.642, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:44.086478, step: 170, loss: 0.6816354990005493, acc: 0.5156, auc: 0.5958, precision: 1.0, recall: 0.0159\n",
      "2019-01-02T17:00:44.536860, step: 171, loss: 0.7042196393013, acc: 0.3984, auc: 0.5554, precision: 1.0, recall: 0.0128\n",
      "2019-01-02T17:00:44.954245, step: 172, loss: 0.6932736039161682, acc: 0.4922, auc: 0.5216, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:45.374441, step: 173, loss: 0.6824653148651123, acc: 0.5391, auc: 0.5824, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:45.781849, step: 174, loss: 0.6733227968215942, acc: 0.3906, auc: 0.6446, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:46.163235, step: 175, loss: 0.6808847188949585, acc: 0.5312, auc: 0.5983, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:46.550251, step: 176, loss: 0.6724929809570312, acc: 0.4844, auc: 0.6284, precision: 1.0, recall: 0.0294\n",
      "2019-01-02T17:00:46.926733, step: 177, loss: 0.6909053325653076, acc: 0.4922, auc: 0.5548, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:47.293878, step: 178, loss: 0.6837426424026489, acc: 0.5234, auc: 0.6158, precision: 1.0, recall: 0.0161\n",
      "2019-01-02T17:00:47.661582, step: 179, loss: 0.6898479461669922, acc: 0.5391, auc: 0.5493, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:48.052760, step: 180, loss: 0.6835106015205383, acc: 0.5469, auc: 0.5894, precision: 0.5, recall: 0.0172\n",
      "2019-01-02T17:00:48.413971, step: 181, loss: 0.6909096240997314, acc: 0.5, auc: 0.5575, precision: 1.0, recall: 0.0154\n",
      "2019-01-02T17:00:48.854153, step: 182, loss: 0.6950492262840271, acc: 0.4766, auc: 0.4691, precision: 1.0, recall: 0.0147\n",
      "2019-01-02T17:00:49.294991, step: 183, loss: 0.6907312273979187, acc: 0.5234, auc: 0.4922, precision: 1.0, recall: 0.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:00:49.727665, step: 184, loss: 0.6820317506790161, acc: 0.5234, auc: 0.5832, precision: 1.0, recall: 0.0317\n",
      "2019-01-02T17:00:50.182810, step: 185, loss: 0.6797091960906982, acc: 0.5312, auc: 0.6172, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:50.614497, step: 186, loss: 0.6912664771080017, acc: 0.5625, auc: 0.5365, precision: 1.0, recall: 0.0345\n",
      "2019-01-02T17:00:51.026541, step: 187, loss: 0.6833398342132568, acc: 0.4531, auc: 0.5766, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:51.451925, step: 188, loss: 0.6832302212715149, acc: 0.4453, auc: 0.5473, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:51.860308, step: 189, loss: 0.679970920085907, acc: 0.5859, auc: 0.6212, precision: 1.0, recall: 0.0364\n",
      "2019-01-02T17:00:52.230247, step: 190, loss: 0.6605371236801147, acc: 0.4688, auc: 0.6751, precision: 1.0, recall: 0.0286\n",
      "2019-01-02T17:00:52.607274, step: 191, loss: 0.7064157724380493, acc: 0.5938, auc: 0.5104, precision: 0.5, recall: 0.0192\n",
      "2019-01-02T17:00:53.000515, step: 192, loss: 0.6669027209281921, acc: 0.5703, auc: 0.6896, precision: 1.0, recall: 0.0351\n",
      "2019-01-02T17:00:53.366474, step: 193, loss: 0.6844528913497925, acc: 0.5469, auc: 0.6127, precision: 1.0, recall: 0.0333\n",
      "2019-01-02T17:00:53.760101, step: 194, loss: 0.6809947490692139, acc: 0.5156, auc: 0.5817, precision: 1.0, recall: 0.0462\n",
      "2019-01-02T17:00:54.194323, step: 195, loss: 0.685250461101532, acc: 0.5547, auc: 0.548, precision: 1.0, recall: 0.0172\n",
      "2019-01-02T17:00:54.630368, step: 196, loss: 0.681837797164917, acc: 0.5156, auc: 0.5841, precision: 1.0, recall: 0.0159\n",
      "2019-01-02T17:00:55.027863, step: 197, loss: 0.6853235960006714, acc: 0.5391, auc: 0.5662, precision: 0.5, recall: 0.0169\n",
      "2019-01-02T17:00:55.444620, step: 198, loss: 0.675127387046814, acc: 0.4766, auc: 0.6078, precision: 1.0, recall: 0.0147\n",
      "2019-01-02T17:00:55.855174, step: 199, loss: 0.6819356679916382, acc: 0.5547, auc: 0.5683, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:00:56.239322, step: 200, loss: 0.6753763556480408, acc: 0.5234, auc: 0.5899, precision: 1.0, recall: 0.0161\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:01:12.283170, step: 200, loss: 0.6797498678549742, acc: 0.49900769230769243, auc: 0.6145435897435896, precision: 0.4017102564102564, recall: 0.00911025641025641\n",
      "2019-01-02T17:01:12.712562, step: 201, loss: 0.6777538061141968, acc: 0.5156, auc: 0.6281, precision: 1.0, recall: 0.0159\n",
      "2019-01-02T17:01:13.121607, step: 202, loss: 0.6697757840156555, acc: 0.5156, auc: 0.654, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:13.496721, step: 203, loss: 0.660309910774231, acc: 0.5391, auc: 0.6828, precision: 1.0, recall: 0.0484\n",
      "2019-01-02T17:01:13.898731, step: 204, loss: 0.6803216338157654, acc: 0.4844, auc: 0.642, precision: 0.5, recall: 0.0152\n",
      "2019-01-02T17:01:14.308658, step: 205, loss: 0.6657965779304504, acc: 0.5391, auc: 0.6956, precision: 1.0, recall: 0.0328\n",
      "2019-01-02T17:01:14.743415, step: 206, loss: 0.6708470582962036, acc: 0.5781, auc: 0.6344, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:15.173074, step: 207, loss: 0.6529449820518494, acc: 0.5391, auc: 0.7273, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:15.600521, step: 208, loss: 0.6653962731361389, acc: 0.4844, auc: 0.676, precision: 1.0, recall: 0.0149\n",
      "2019-01-02T17:01:16.045305, step: 209, loss: 0.6835081577301025, acc: 0.4844, auc: 0.6114, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:16.439047, step: 210, loss: 0.6647125482559204, acc: 0.4297, auc: 0.708, precision: 1.0, recall: 0.0135\n",
      "2019-01-02T17:01:16.874575, step: 211, loss: 0.68801349401474, acc: 0.4297, auc: 0.6102, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:17.333824, step: 212, loss: 0.6601858139038086, acc: 0.4688, auc: 0.736, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:17.781579, step: 213, loss: 0.6526327133178711, acc: 0.5312, auc: 0.717, precision: 1.0, recall: 0.0323\n",
      "2019-01-02T17:01:18.200506, step: 214, loss: 0.6699848771095276, acc: 0.5156, auc: 0.6242, precision: 0.3333, recall: 0.0164\n",
      "2019-01-02T17:01:18.636127, step: 215, loss: 0.6427778005599976, acc: 0.5469, auc: 0.7357, precision: 0.8, recall: 0.0656\n",
      "2019-01-02T17:01:19.038367, step: 216, loss: 0.6775677800178528, acc: 0.4766, auc: 0.6266, precision: 1.0, recall: 0.029\n",
      "2019-01-02T17:01:19.451643, step: 217, loss: 0.676363468170166, acc: 0.4219, auc: 0.7251, precision: 0.8333, recall: 0.0641\n",
      "2019-01-02T17:01:19.839984, step: 218, loss: 0.6746865510940552, acc: 0.5156, auc: 0.6706, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:20.229594, step: 219, loss: 0.7111241817474365, acc: 0.4609, auc: 0.6135, precision: 0.75, recall: 0.0423\n",
      "2019-01-02T17:01:20.613653, step: 220, loss: 0.6832869052886963, acc: 0.5, auc: 0.6354, precision: 1.0, recall: 0.0303\n",
      "2019-01-02T17:01:21.018449, step: 221, loss: 0.6923642754554749, acc: 0.5469, auc: 0.5296, precision: 1.0, recall: 0.0169\n",
      "2019-01-02T17:01:21.436624, step: 222, loss: 0.6636292934417725, acc: 0.5859, auc: 0.5836, precision: 1.0, recall: 0.0364\n",
      "2019-01-02T17:01:21.857311, step: 223, loss: 0.7048390507698059, acc: 0.4766, auc: 0.5977, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:22.275326, step: 224, loss: 0.696849524974823, acc: 0.5078, auc: 0.5978, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:22.715185, step: 225, loss: 0.6902897357940674, acc: 0.5234, auc: 0.5772, precision: 1.0, recall: 0.0161\n",
      "2019-01-02T17:01:23.111565, step: 226, loss: 0.68599933385849, acc: 0.5156, auc: 0.5848, precision: 1.0, recall: 0.0746\n",
      "2019-01-02T17:01:23.475978, step: 227, loss: 0.6904757022857666, acc: 0.5312, auc: 0.556, precision: 1.0, recall: 0.0769\n",
      "2019-01-02T17:01:23.889018, step: 228, loss: 0.6822378635406494, acc: 0.4844, auc: 0.6105, precision: 0.5, recall: 0.0303\n",
      "2019-01-02T17:01:24.325151, step: 229, loss: 0.6705942153930664, acc: 0.5469, auc: 0.6335, precision: 0.6, recall: 0.0508\n",
      "2019-01-02T17:01:24.749831, step: 230, loss: 0.6375212669372559, acc: 0.5703, auc: 0.7795, precision: 1.0, recall: 0.0984\n",
      "2019-01-02T17:01:25.120694, step: 231, loss: 0.6314871311187744, acc: 0.5547, auc: 0.8438, precision: 0.8571, recall: 0.0968\n",
      "2019-01-02T17:01:25.556703, step: 232, loss: 0.643690824508667, acc: 0.5234, auc: 0.7293, precision: 0.9091, recall: 0.1429\n",
      "2019-01-02T17:01:25.949833, step: 233, loss: 0.6422538757324219, acc: 0.5234, auc: 0.7438, precision: 0.8333, recall: 0.1449\n",
      "2019-01-02T17:01:26.321609, step: 234, loss: 0.6556152105331421, acc: 0.6328, auc: 0.7917, precision: 0.8, recall: 0.1509\n",
      "2019-01-02T17:01:26.728023, step: 235, loss: 0.6652417778968811, acc: 0.4922, auc: 0.6728, precision: 0.6, recall: 0.0455\n",
      "2019-01-02T17:01:27.141027, step: 236, loss: 0.6342664957046509, acc: 0.5625, auc: 0.7869, precision: 0.8, recall: 0.129\n",
      "2019-01-02T17:01:27.568408, step: 237, loss: 0.6378620862960815, acc: 0.5781, auc: 0.7669, precision: 0.7778, recall: 0.1186\n",
      "2019-01-02T17:01:27.986099, step: 238, loss: 0.6271675825119019, acc: 0.5312, auc: 0.7878, precision: 1.0, recall: 0.0769\n",
      "2019-01-02T17:01:28.395378, step: 239, loss: 0.6396732330322266, acc: 0.5156, auc: 0.7443, precision: 0.7143, recall: 0.0769\n",
      "2019-01-02T17:01:28.833604, step: 240, loss: 0.6078588366508484, acc: 0.5938, auc: 0.8285, precision: 0.8462, recall: 0.1803\n",
      "2019-01-02T17:01:29.259731, step: 241, loss: 0.6115875840187073, acc: 0.5547, auc: 0.8015, precision: 0.875, recall: 0.1111\n",
      "2019-01-02T17:01:29.667598, step: 242, loss: 0.6252334117889404, acc: 0.5469, auc: 0.7306, precision: 0.8571, recall: 0.0952\n",
      "2019-01-02T17:01:30.083934, step: 243, loss: 0.6085690259933472, acc: 0.4688, auc: 0.8132, precision: 1.0, recall: 0.0685\n",
      "2019-01-02T17:01:30.502786, step: 244, loss: 0.614496111869812, acc: 0.5469, auc: 0.7554, precision: 0.8333, recall: 0.0806\n",
      "2019-01-02T17:01:30.885945, step: 245, loss: 0.6015205383300781, acc: 0.5156, auc: 0.8142, precision: 0.875, recall: 0.1029\n",
      "2019-01-02T17:01:31.321995, step: 246, loss: 0.7194420099258423, acc: 0.5, auc: 0.6201, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:31.773673, step: 247, loss: 0.7322381138801575, acc: 0.5625, auc: 0.5402, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:32.183136, step: 248, loss: 0.8064789175987244, acc: 0.4844, auc: 0.5562, precision: 1.0, recall: 0.0149\n",
      "2019-01-02T17:01:32.591809, step: 249, loss: 0.82371985912323, acc: 0.4531, auc: 0.5357, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:01:33.006978, step: 250, loss: 0.8092740774154663, acc: 0.4766, auc: 0.5998, precision: 1.0, recall: 0.0147\n",
      "2019-01-02T17:01:33.428436, step: 251, loss: 0.8290076851844788, acc: 0.4297, auc: 0.5096, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:33.845774, step: 252, loss: 0.7447402477264404, acc: 0.5078, auc: 0.5614, precision: 1.0, recall: 0.0308\n",
      "2019-01-02T17:01:34.291230, step: 253, loss: 0.7368309497833252, acc: 0.5078, auc: 0.5213, precision: 0.0, recall: 0.0\n",
      "2019-01-02T17:01:34.710316, step: 254, loss: 0.7568743228912354, acc: 0.4766, auc: 0.4614, precision: 0.3333, recall: 0.0152\n",
      "2019-01-02T17:01:35.127427, step: 255, loss: 0.7142102122306824, acc: 0.5156, auc: 0.4923, precision: 0.6, recall: 0.0476\n",
      "2019-01-02T17:01:35.530699, step: 256, loss: 0.7115918397903442, acc: 0.5234, auc: 0.4969, precision: 0.5, recall: 0.0328\n",
      "2019-01-02T17:01:35.936360, step: 257, loss: 0.7096775770187378, acc: 0.4062, auc: 0.5195, precision: 0.6667, recall: 0.0759\n",
      "2019-01-02T17:01:36.348198, step: 258, loss: 0.6993852853775024, acc: 0.5469, auc: 0.5666, precision: 0.4737, recall: 0.1579\n",
      "2019-01-02T17:01:36.772165, step: 259, loss: 0.7020176649093628, acc: 0.5625, auc: 0.575, precision: 0.5143, recall: 0.3158\n",
      "2019-01-02T17:01:37.220746, step: 260, loss: 0.7213701605796814, acc: 0.5312, auc: 0.5004, precision: 0.5641, recall: 0.3385\n",
      "2019-01-02T17:01:37.644641, step: 261, loss: 0.7232590317726135, acc: 0.5547, auc: 0.5748, precision: 0.4737, recall: 0.3273\n",
      "2019-01-02T17:01:38.066649, step: 262, loss: 0.708571195602417, acc: 0.4766, auc: 0.5114, precision: 0.5238, recall: 0.3188\n",
      "2019-01-02T17:01:38.509598, step: 263, loss: 0.7224791646003723, acc: 0.4922, auc: 0.505, precision: 0.5, recall: 0.2615\n",
      "2019-01-02T17:01:38.930112, step: 264, loss: 0.6944296360015869, acc: 0.5625, auc: 0.5728, precision: 0.5641, recall: 0.3607\n",
      "2019-01-02T17:01:39.346158, step: 265, loss: 0.7219051122665405, acc: 0.5078, auc: 0.5076, precision: 0.5349, recall: 0.3485\n",
      "2019-01-02T17:01:39.846306, step: 266, loss: 0.7252238392829895, acc: 0.4531, auc: 0.448, precision: 0.4444, recall: 0.1791\n",
      "2019-01-02T17:01:40.302158, step: 267, loss: 0.7020741701126099, acc: 0.5078, auc: 0.5254, precision: 0.6071, recall: 0.2464\n",
      "2019-01-02T17:01:40.713451, step: 268, loss: 0.6639741659164429, acc: 0.5859, auc: 0.6479, precision: 0.7037, recall: 0.2969\n",
      "2019-01-02T17:01:41.110346, step: 269, loss: 0.6981021165847778, acc: 0.5703, auc: 0.5379, precision: 0.5625, recall: 0.1579\n",
      "2019-01-02T17:01:41.523799, step: 270, loss: 0.6879922151565552, acc: 0.4766, auc: 0.5406, precision: 0.5714, recall: 0.1714\n",
      "2019-01-02T17:01:41.947966, step: 271, loss: 0.6835112571716309, acc: 0.5, auc: 0.5855, precision: 0.5714, recall: 0.1212\n",
      "2019-01-02T17:01:42.357859, step: 272, loss: 0.7023711800575256, acc: 0.5078, auc: 0.4935, precision: 0.8, recall: 0.1159\n",
      "2019-01-02T17:01:42.794813, step: 273, loss: 0.6523856520652771, acc: 0.6016, auc: 0.6663, precision: 0.8889, recall: 0.1379\n",
      "2019-01-02T17:01:43.219639, step: 274, loss: 0.6892762184143066, acc: 0.4609, auc: 0.5844, precision: 0.5, recall: 0.029\n",
      "2019-01-02T17:01:43.627618, step: 275, loss: 0.7001068592071533, acc: 0.5625, auc: 0.5221, precision: 0.5, recall: 0.0714\n",
      "2019-01-02T17:01:44.042833, step: 276, loss: 0.7033090591430664, acc: 0.5469, auc: 0.5105, precision: 0.7, recall: 0.1129\n",
      "2019-01-02T17:01:44.465543, step: 277, loss: 0.7138857841491699, acc: 0.4609, auc: 0.5257, precision: 0.2, recall: 0.0152\n",
      "2019-01-02T17:01:44.867913, step: 278, loss: 0.7090234756469727, acc: 0.4453, auc: 0.538, precision: 0.25, recall: 0.0145\n",
      "2019-01-02T17:01:45.294977, step: 279, loss: 0.6905883550643921, acc: 0.5859, auc: 0.533, precision: 0.75, recall: 0.1053\n",
      "2019-01-02T17:01:45.708449, step: 280, loss: 0.6943256855010986, acc: 0.3984, auc: 0.5938, precision: 0.6667, recall: 0.075\n",
      "2019-01-02T17:01:46.111334, step: 281, loss: 0.6877886056900024, acc: 0.5391, auc: 0.5632, precision: 0.7273, recall: 0.125\n",
      "2019-01-02T17:01:46.516208, step: 282, loss: 0.7002750635147095, acc: 0.4531, auc: 0.5345, precision: 0.5714, recall: 0.0563\n",
      "2019-01-02T17:01:46.921856, step: 283, loss: 0.6725900173187256, acc: 0.4922, auc: 0.6151, precision: 0.6667, recall: 0.0597\n",
      "2019-01-02T17:01:47.312973, step: 284, loss: 0.6754459738731384, acc: 0.6016, auc: 0.6037, precision: 0.8889, recall: 0.1379\n",
      "2019-01-02T17:01:47.720130, step: 285, loss: 0.6829119920730591, acc: 0.5312, auc: 0.5772, precision: 0.8333, recall: 0.1471\n",
      "2019-01-02T17:01:48.106869, step: 286, loss: 0.7183748483657837, acc: 0.4922, auc: 0.5017, precision: 0.5333, recall: 0.1212\n",
      "2019-01-02T17:01:48.502986, step: 287, loss: 0.6978548765182495, acc: 0.5, auc: 0.5166, precision: 0.625, recall: 0.0758\n",
      "2019-01-02T17:01:48.880823, step: 288, loss: 0.68180251121521, acc: 0.4922, auc: 0.5635, precision: 0.8571, recall: 0.0857\n",
      "2019-01-02T17:01:49.271898, step: 289, loss: 0.7102065682411194, acc: 0.4766, auc: 0.4519, precision: 0.7778, recall: 0.0972\n",
      "2019-01-02T17:01:49.661785, step: 290, loss: 0.7029498815536499, acc: 0.5703, auc: 0.531, precision: 0.5263, recall: 0.1786\n",
      "2019-01-02T17:01:50.081774, step: 291, loss: 0.7013154029846191, acc: 0.4844, auc: 0.5002, precision: 0.5714, recall: 0.1176\n",
      "2019-01-02T17:01:50.510696, step: 292, loss: 0.6936430931091309, acc: 0.5547, auc: 0.5669, precision: 0.5625, recall: 0.1525\n",
      "2019-01-02T17:01:50.915100, step: 293, loss: 0.7064598798751831, acc: 0.4922, auc: 0.5114, precision: 0.45, recall: 0.1429\n",
      "2019-01-02T17:01:51.293611, step: 294, loss: 0.6974453926086426, acc: 0.5859, auc: 0.5494, precision: 0.5789, recall: 0.1964\n",
      "2019-01-02T17:01:51.687455, step: 295, loss: 0.6976817846298218, acc: 0.5156, auc: 0.5367, precision: 0.625, recall: 0.1515\n",
      "2019-01-02T17:01:52.082898, step: 296, loss: 0.7049223184585571, acc: 0.5547, auc: 0.5102, precision: 0.6667, recall: 0.1311\n",
      "2019-01-02T17:01:52.504200, step: 297, loss: 0.7016301155090332, acc: 0.5234, auc: 0.561, precision: 0.4545, recall: 0.0833\n",
      "2019-01-02T17:01:52.909501, step: 298, loss: 0.6848823428153992, acc: 0.4922, auc: 0.5909, precision: 0.3333, recall: 0.0656\n",
      "2019-01-02T17:01:53.317233, step: 299, loss: 0.7134242057800293, acc: 0.5078, auc: 0.4739, precision: 0.4444, recall: 0.0645\n",
      "2019-01-02T17:01:53.713781, step: 300, loss: 0.6621690988540649, acc: 0.5234, auc: 0.6451, precision: 0.9, recall: 0.1304\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:02:09.439581, step: 300, loss: 0.6874917928989117, acc: 0.5096102564102564, auc: 0.5430076923076923, precision: 0.7295461538461538, recall: 0.0459\n",
      "2019-01-02T17:02:09.821259, step: 301, loss: 0.696492075920105, acc: 0.5078, auc: 0.5302, precision: 0.6667, recall: 0.0615\n",
      "2019-01-02T17:02:10.216789, step: 302, loss: 0.6682402491569519, acc: 0.6094, auc: 0.6189, precision: 0.8333, recall: 0.0926\n",
      "2019-01-02T17:02:10.610611, step: 303, loss: 0.6800336837768555, acc: 0.5703, auc: 0.592, precision: 0.5, recall: 0.0545\n",
      "2019-01-02T17:02:10.991623, step: 304, loss: 0.6704099178314209, acc: 0.5547, auc: 0.6141, precision: 0.8333, recall: 0.082\n",
      "2019-01-02T17:02:11.365953, step: 305, loss: 0.6832009553909302, acc: 0.5625, auc: 0.5741, precision: 1.0, recall: 0.1111\n",
      "2019-01-02T17:02:11.766617, step: 306, loss: 0.6803966760635376, acc: 0.5547, auc: 0.5833, precision: 1.0, recall: 0.0806\n",
      "2019-01-02T17:02:12.139445, step: 307, loss: 0.6975350379943848, acc: 0.5391, auc: 0.5216, precision: 0.25, recall: 0.0175\n",
      "2019-01-02T17:02:12.532182, step: 308, loss: 0.6802854537963867, acc: 0.5781, auc: 0.5317, precision: 1.0, recall: 0.0357\n",
      "2019-01-02T17:02:12.930578, step: 309, loss: 0.7005676031112671, acc: 0.4688, auc: 0.5559, precision: 1.0, recall: 0.0286\n",
      "2019-01-02T17:02:13.358755, step: 310, loss: 0.6941199898719788, acc: 0.5234, auc: 0.5495, precision: 0.6667, recall: 0.0635\n",
      "2019-01-02T17:02:13.790287, step: 311, loss: 0.667845606803894, acc: 0.6016, auc: 0.5761, precision: 1.0, recall: 0.0556\n",
      "2019-01-02T17:02:14.220982, step: 312, loss: 0.6856245398521423, acc: 0.4531, auc: 0.6091, precision: 1.0, recall: 0.0278\n",
      "start training model\n",
      "2019-01-02T17:02:14.689103, step: 313, loss: 0.7103566527366638, acc: 0.4922, auc: 0.4829, precision: 0.6, recall: 0.0455\n",
      "2019-01-02T17:02:15.116331, step: 314, loss: 0.7264261245727539, acc: 0.4844, auc: 0.4877, precision: 0.75, recall: 0.0441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:02:15.537493, step: 315, loss: 0.6940463185310364, acc: 0.4766, auc: 0.5787, precision: 0.5714, recall: 0.0588\n",
      "2019-01-02T17:02:15.955166, step: 316, loss: 0.6368294358253479, acc: 0.5938, auc: 0.6879, precision: 1.0, recall: 0.1034\n",
      "2019-01-02T17:02:16.353082, step: 317, loss: 0.6676848530769348, acc: 0.5938, auc: 0.5934, precision: 0.9, recall: 0.15\n",
      "2019-01-02T17:02:16.776288, step: 318, loss: 0.6574845910072327, acc: 0.625, auc: 0.6169, precision: 1.0, recall: 0.0943\n",
      "2019-01-02T17:02:17.173054, step: 319, loss: 0.6939704418182373, acc: 0.5, auc: 0.5419, precision: 0.5714, recall: 0.0615\n",
      "2019-01-02T17:02:17.562989, step: 320, loss: 0.698289155960083, acc: 0.5625, auc: 0.5221, precision: 0.5455, recall: 0.1053\n",
      "2019-01-02T17:02:17.981847, step: 321, loss: 0.6929269433021545, acc: 0.5625, auc: 0.5584, precision: 0.5714, recall: 0.0702\n",
      "2019-01-02T17:02:18.391868, step: 322, loss: 0.6727933883666992, acc: 0.5469, auc: 0.5923, precision: 1.0, recall: 0.0938\n",
      "2019-01-02T17:02:18.803050, step: 323, loss: 0.6826711893081665, acc: 0.5, auc: 0.5642, precision: 0.7143, recall: 0.0746\n",
      "2019-01-02T17:02:19.230700, step: 324, loss: 0.6996882557868958, acc: 0.4922, auc: 0.5127, precision: 0.6667, recall: 0.0303\n",
      "2019-01-02T17:02:19.644045, step: 325, loss: 0.6941713690757751, acc: 0.5391, auc: 0.5177, precision: 0.7, recall: 0.1111\n",
      "2019-01-02T17:02:20.113197, step: 326, loss: 0.7283090353012085, acc: 0.4297, auc: 0.4457, precision: 0.5714, recall: 0.0541\n",
      "2019-01-02T17:02:20.521803, step: 327, loss: 0.6924017071723938, acc: 0.4922, auc: 0.5032, precision: 0.7143, recall: 0.0735\n",
      "2019-01-02T17:02:20.918458, step: 328, loss: 0.688696563243866, acc: 0.5703, auc: 0.5492, precision: 0.5, recall: 0.0727\n",
      "2019-01-02T17:02:21.304069, step: 329, loss: 0.689046323299408, acc: 0.5234, auc: 0.5401, precision: 0.7778, recall: 0.1061\n",
      "2019-01-02T17:02:21.688749, step: 330, loss: 0.6750380992889404, acc: 0.5938, auc: 0.5377, precision: 1.0, recall: 0.1333\n",
      "2019-01-02T17:02:22.076739, step: 331, loss: 0.6877098083496094, acc: 0.5078, auc: 0.5286, precision: 0.4615, recall: 0.0968\n",
      "2019-01-02T17:02:22.519045, step: 332, loss: 0.6780074834823608, acc: 0.6172, auc: 0.5694, precision: 0.8, recall: 0.1455\n",
      "2019-01-02T17:02:22.945598, step: 333, loss: 0.6862006783485413, acc: 0.4766, auc: 0.5676, precision: 0.8, recall: 0.1096\n",
      "2019-01-02T17:02:23.396052, step: 334, loss: 0.6737461090087891, acc: 0.5312, auc: 0.5819, precision: 0.7143, recall: 0.0794\n",
      "2019-01-02T17:02:23.802189, step: 335, loss: 0.6629692912101746, acc: 0.5625, auc: 0.5998, precision: 0.8889, recall: 0.127\n",
      "2019-01-02T17:02:24.187646, step: 336, loss: 0.7139304280281067, acc: 0.5156, auc: 0.4556, precision: 0.6667, recall: 0.0625\n",
      "2019-01-02T17:02:24.579211, step: 337, loss: 0.6849566698074341, acc: 0.5, auc: 0.5676, precision: 0.5, recall: 0.0781\n",
      "2019-01-02T17:02:24.989512, step: 338, loss: 0.6799861788749695, acc: 0.5625, auc: 0.5667, precision: 0.7778, recall: 0.1148\n",
      "2019-01-02T17:02:25.428904, step: 339, loss: 0.6915537118911743, acc: 0.5391, auc: 0.5257, precision: 0.7143, recall: 0.0806\n",
      "2019-01-02T17:02:25.857203, step: 340, loss: 0.6906639933586121, acc: 0.5234, auc: 0.5859, precision: 0.7143, recall: 0.0781\n",
      "2019-01-02T17:02:26.274035, step: 341, loss: 0.6741956472396851, acc: 0.6406, auc: 0.549, precision: 1.0, recall: 0.1481\n",
      "2019-01-02T17:02:26.677104, step: 342, loss: 0.6876190304756165, acc: 0.5703, auc: 0.56, precision: 0.6471, recall: 0.1833\n",
      "2019-01-02T17:02:27.124002, step: 343, loss: 0.6496060490608215, acc: 0.5781, auc: 0.6264, precision: 1.0, recall: 0.194\n",
      "2019-01-02T17:02:27.530437, step: 344, loss: 0.6725290417671204, acc: 0.5156, auc: 0.5592, precision: 0.8, recall: 0.0615\n",
      "2019-01-02T17:02:27.936401, step: 345, loss: 0.6741292476654053, acc: 0.5703, auc: 0.5686, precision: 0.875, recall: 0.1148\n",
      "2019-01-02T17:02:28.357773, step: 346, loss: 0.6630603075027466, acc: 0.5469, auc: 0.5846, precision: 0.9167, recall: 0.1618\n",
      "2019-01-02T17:02:28.779586, step: 347, loss: 0.6712584495544434, acc: 0.5156, auc: 0.593, precision: 0.625, recall: 0.0781\n",
      "2019-01-02T17:02:29.181580, step: 348, loss: 0.6776458024978638, acc: 0.5, auc: 0.5911, precision: 1.0, recall: 0.0448\n",
      "2019-01-02T17:02:29.567861, step: 349, loss: 0.683975100517273, acc: 0.4922, auc: 0.5725, precision: 1.0, recall: 0.0441\n",
      "2019-01-02T17:02:29.938978, step: 350, loss: 0.67097008228302, acc: 0.4922, auc: 0.595, precision: 0.9, recall: 0.1233\n",
      "2019-01-02T17:02:30.327562, step: 351, loss: 0.66130530834198, acc: 0.5312, auc: 0.5853, precision: 0.6364, recall: 0.1111\n",
      "2019-01-02T17:02:30.717988, step: 352, loss: 0.6323923468589783, acc: 0.5469, auc: 0.6611, precision: 0.8182, recall: 0.1385\n",
      "2019-01-02T17:02:31.148663, step: 353, loss: 0.6848818063735962, acc: 0.5, auc: 0.5432, precision: 0.6667, recall: 0.0896\n",
      "2019-01-02T17:02:31.550208, step: 354, loss: 0.690060019493103, acc: 0.5703, auc: 0.5599, precision: 0.6, recall: 0.1053\n",
      "2019-01-02T17:02:31.930529, step: 355, loss: 0.6714385151863098, acc: 0.5, auc: 0.599, precision: 0.6154, recall: 0.1194\n",
      "2019-01-02T17:02:32.324929, step: 356, loss: 0.6482653617858887, acc: 0.4609, auc: 0.6218, precision: 0.8333, recall: 0.1299\n",
      "2019-01-02T17:02:32.737037, step: 357, loss: 0.6821483373641968, acc: 0.5547, auc: 0.5405, precision: 0.7059, recall: 0.1875\n",
      "2019-01-02T17:02:33.168676, step: 358, loss: 0.6585358381271362, acc: 0.5156, auc: 0.6047, precision: 0.7895, recall: 0.2055\n",
      "2019-01-02T17:02:33.552560, step: 359, loss: 0.6834990382194519, acc: 0.5, auc: 0.5523, precision: 0.75, recall: 0.0455\n",
      "2019-01-02T17:02:33.904757, step: 360, loss: 0.686888575553894, acc: 0.4766, auc: 0.5238, precision: 0.3571, recall: 0.0794\n",
      "2019-01-02T17:02:34.263733, step: 361, loss: 0.6801260113716125, acc: 0.4922, auc: 0.5458, precision: 0.7, recall: 0.1014\n",
      "2019-01-02T17:02:34.653318, step: 362, loss: 0.6595165729522705, acc: 0.4766, auc: 0.6017, precision: 0.7778, recall: 0.1818\n",
      "2019-01-02T17:02:35.003172, step: 363, loss: 0.6664032936096191, acc: 0.5859, auc: 0.6446, precision: 0.6, recall: 0.1607\n",
      "2019-01-02T17:02:35.394593, step: 364, loss: 0.710405707359314, acc: 0.5312, auc: 0.5286, precision: 0.4375, recall: 0.1207\n",
      "2019-01-02T17:02:35.810662, step: 365, loss: 0.6796247959136963, acc: 0.6016, auc: 0.5721, precision: 0.6429, recall: 0.1636\n",
      "2019-01-02T17:02:36.227720, step: 366, loss: 0.6757112741470337, acc: 0.5312, auc: 0.5772, precision: 0.75, recall: 0.1364\n",
      "2019-01-02T17:02:36.615980, step: 367, loss: 0.6893453598022461, acc: 0.5, auc: 0.494, precision: 0.6667, recall: 0.0896\n",
      "2019-01-02T17:02:37.012645, step: 368, loss: 0.6589667201042175, acc: 0.4609, auc: 0.6251, precision: 0.8462, recall: 0.141\n",
      "2019-01-02T17:02:37.414580, step: 369, loss: 0.688897967338562, acc: 0.5469, auc: 0.5408, precision: 0.7143, recall: 0.1562\n",
      "2019-01-02T17:02:37.828987, step: 370, loss: 0.6661153435707092, acc: 0.6562, auc: 0.686, precision: 0.7692, recall: 0.1961\n",
      "2019-01-02T17:02:38.213063, step: 371, loss: 0.7068821787834167, acc: 0.4844, auc: 0.4726, precision: 0.5, recall: 0.0455\n",
      "2019-01-02T17:02:38.609178, step: 372, loss: 0.6586415767669678, acc: 0.5391, auc: 0.5911, precision: 0.7778, recall: 0.1094\n",
      "2019-01-02T17:02:39.008978, step: 373, loss: 0.6733953952789307, acc: 0.5469, auc: 0.5566, precision: 0.6, recall: 0.1\n",
      "2019-01-02T17:02:39.403649, step: 374, loss: 0.6516758799552917, acc: 0.5, auc: 0.6324, precision: 0.8333, recall: 0.1389\n",
      "2019-01-02T17:02:39.799561, step: 375, loss: 0.6911007165908813, acc: 0.4766, auc: 0.4943, precision: 0.7143, recall: 0.0714\n",
      "2019-01-02T17:02:40.192137, step: 376, loss: 0.6798495054244995, acc: 0.5625, auc: 0.5523, precision: 0.6, recall: 0.0526\n",
      "2019-01-02T17:02:40.611943, step: 377, loss: 0.663872480392456, acc: 0.5781, auc: 0.6413, precision: 0.9, recall: 0.1452\n",
      "2019-01-02T17:02:41.006106, step: 378, loss: 0.653896152973175, acc: 0.5469, auc: 0.5913, precision: 0.75, recall: 0.1406\n",
      "2019-01-02T17:02:41.379934, step: 379, loss: 0.7119921445846558, acc: 0.5312, auc: 0.4954, precision: 0.7, recall: 0.1094\n",
      "2019-01-02T17:02:41.815731, step: 380, loss: 0.6682471036911011, acc: 0.5625, auc: 0.5963, precision: 0.6875, recall: 0.1774\n",
      "2019-01-02T17:02:42.191368, step: 381, loss: 0.6487562656402588, acc: 0.6484, auc: 0.6651, precision: 0.7778, recall: 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:02:42.578669, step: 382, loss: 0.6697988510131836, acc: 0.5547, auc: 0.5924, precision: 0.8462, recall: 0.1667\n",
      "2019-01-02T17:02:42.959389, step: 383, loss: 0.6898916959762573, acc: 0.5156, auc: 0.5268, precision: 1.0, recall: 0.0746\n",
      "2019-01-02T17:02:43.341593, step: 384, loss: 0.6813129186630249, acc: 0.5625, auc: 0.5238, precision: 0.75, recall: 0.1\n",
      "2019-01-02T17:02:43.728311, step: 385, loss: 0.6623761057853699, acc: 0.5469, auc: 0.6074, precision: 1.0, recall: 0.0938\n",
      "2019-01-02T17:02:44.111627, step: 386, loss: 0.6553503274917603, acc: 0.5703, auc: 0.6244, precision: 0.875, recall: 0.1148\n",
      "2019-01-02T17:02:44.499519, step: 387, loss: 0.6671549677848816, acc: 0.5469, auc: 0.5708, precision: 1.0, recall: 0.0938\n",
      "2019-01-02T17:02:44.912148, step: 388, loss: 0.655626654624939, acc: 0.625, auc: 0.5745, precision: 0.8, recall: 0.0784\n",
      "2019-01-02T17:02:45.317024, step: 389, loss: 0.6839375495910645, acc: 0.5625, auc: 0.5214, precision: 0.6667, recall: 0.0351\n",
      "2019-01-02T17:02:45.712998, step: 390, loss: 0.6694637537002563, acc: 0.5234, auc: 0.5932, precision: 0.75, recall: 0.0923\n",
      "2019-01-02T17:02:46.110992, step: 391, loss: 0.6989285945892334, acc: 0.5156, auc: 0.585, precision: 0.7778, recall: 0.1045\n",
      "2019-01-02T17:02:46.504108, step: 392, loss: 0.6625983715057373, acc: 0.5781, auc: 0.6121, precision: 0.6667, recall: 0.1379\n",
      "2019-01-02T17:02:46.872812, step: 393, loss: 0.6849888563156128, acc: 0.5547, auc: 0.5496, precision: 0.7273, recall: 0.129\n",
      "2019-01-02T17:02:47.245575, step: 394, loss: 0.6655629873275757, acc: 0.5703, auc: 0.6032, precision: 0.8571, recall: 0.1\n",
      "2019-01-02T17:02:47.646596, step: 395, loss: 0.6736438870429993, acc: 0.5312, auc: 0.6459, precision: 0.8, recall: 0.1212\n",
      "2019-01-02T17:02:48.068452, step: 396, loss: 0.6780991554260254, acc: 0.5234, auc: 0.5567, precision: 0.6923, recall: 0.1364\n",
      "2019-01-02T17:02:48.461197, step: 397, loss: 0.6804416179656982, acc: 0.5469, auc: 0.5021, precision: 0.6667, recall: 0.0339\n",
      "2019-01-02T17:02:48.861206, step: 398, loss: 0.6929565668106079, acc: 0.5469, auc: 0.4984, precision: 1.0, recall: 0.0794\n",
      "2019-01-02T17:02:49.257488, step: 399, loss: 0.6765705943107605, acc: 0.5234, auc: 0.5549, precision: 0.8, recall: 0.1194\n",
      "2019-01-02T17:02:49.644542, step: 400, loss: 0.669739842414856, acc: 0.5781, auc: 0.5377, precision: 0.8571, recall: 0.1017\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:03:05.390211, step: 400, loss: 0.6932965379494888, acc: 0.5134230769230769, auc: 0.5631307692307693, precision: 0.6661256410256409, recall: 0.08223333333333335\n",
      "2019-01-02T17:03:05.828633, step: 401, loss: 0.6728906035423279, acc: 0.5312, auc: 0.6163, precision: 0.8182, recall: 0.1343\n",
      "2019-01-02T17:03:06.251688, step: 402, loss: 0.6538358926773071, acc: 0.5469, auc: 0.6445, precision: 0.875, recall: 0.1094\n",
      "2019-01-02T17:03:06.666930, step: 403, loss: 0.6538289785385132, acc: 0.5078, auc: 0.6007, precision: 0.7692, recall: 0.1429\n",
      "2019-01-02T17:03:07.056869, step: 404, loss: 0.6712839007377625, acc: 0.5391, auc: 0.6486, precision: 0.8182, recall: 0.1364\n",
      "2019-01-02T17:03:07.451300, step: 405, loss: 0.6813433170318604, acc: 0.5781, auc: 0.5699, precision: 0.7692, recall: 0.1639\n",
      "2019-01-02T17:03:07.847511, step: 406, loss: 0.665435791015625, acc: 0.5312, auc: 0.6264, precision: 0.7273, recall: 0.1231\n",
      "2019-01-02T17:03:08.257048, step: 407, loss: 0.6875410676002502, acc: 0.6016, auc: 0.52, precision: 0.8, recall: 0.0741\n",
      "2019-01-02T17:03:08.634599, step: 408, loss: 0.6674326658248901, acc: 0.5, auc: 0.578, precision: 0.8889, recall: 0.1127\n",
      "2019-01-02T17:03:09.055821, step: 409, loss: 0.6700211763381958, acc: 0.5469, auc: 0.6073, precision: 0.6667, recall: 0.0984\n",
      "2019-01-02T17:03:09.472801, step: 410, loss: 0.6717426180839539, acc: 0.4922, auc: 0.5441, precision: 0.7273, recall: 0.1143\n",
      "2019-01-02T17:03:09.895747, step: 411, loss: 0.6750341653823853, acc: 0.5391, auc: 0.5534, precision: 0.75, recall: 0.0952\n",
      "2019-01-02T17:03:10.297666, step: 412, loss: 0.6973085999488831, acc: 0.5547, auc: 0.5318, precision: 0.5833, recall: 0.1186\n",
      "2019-01-02T17:03:10.703941, step: 413, loss: 0.6729519963264465, acc: 0.4844, auc: 0.5569, precision: 0.625, recall: 0.0735\n",
      "2019-01-02T17:03:11.110222, step: 414, loss: 0.6470426321029663, acc: 0.5469, auc: 0.6357, precision: 0.75, recall: 0.1406\n",
      "2019-01-02T17:03:11.532701, step: 415, loss: 0.7024733424186707, acc: 0.5547, auc: 0.5377, precision: 0.5714, recall: 0.069\n",
      "2019-01-02T17:03:11.941249, step: 416, loss: 0.6531789302825928, acc: 0.5547, auc: 0.5911, precision: 0.8889, recall: 0.125\n",
      "2019-01-02T17:03:12.361480, step: 417, loss: 0.7165255546569824, acc: 0.4219, auc: 0.499, precision: 0.3125, recall: 0.0735\n",
      "2019-01-02T17:03:12.776107, step: 418, loss: 0.6438761949539185, acc: 0.5938, auc: 0.6458, precision: 0.8333, recall: 0.1667\n",
      "2019-01-02T17:03:13.163970, step: 419, loss: 0.699639081954956, acc: 0.5391, auc: 0.599, precision: 0.4286, recall: 0.0517\n",
      "2019-01-02T17:03:13.550864, step: 420, loss: 0.7014398574829102, acc: 0.5, auc: 0.5238, precision: 0.75, recall: 0.0882\n",
      "2019-01-02T17:03:13.966035, step: 421, loss: 0.6684476137161255, acc: 0.4375, auc: 0.58, precision: 0.8889, recall: 0.1013\n",
      "2019-01-02T17:03:14.366066, step: 422, loss: 0.6920663118362427, acc: 0.4375, auc: 0.5288, precision: 0.6667, recall: 0.0541\n",
      "2019-01-02T17:03:14.781704, step: 423, loss: 0.659935712814331, acc: 0.5234, auc: 0.5987, precision: 0.875, recall: 0.1045\n",
      "2019-01-02T17:03:15.209170, step: 424, loss: 0.6237494349479675, acc: 0.5781, auc: 0.7331, precision: 1.0, recall: 0.1148\n",
      "2019-01-02T17:03:15.612516, step: 425, loss: 0.6383764743804932, acc: 0.5547, auc: 0.6801, precision: 1.0, recall: 0.0952\n",
      "2019-01-02T17:03:15.995691, step: 426, loss: 0.6634572744369507, acc: 0.5234, auc: 0.5923, precision: 0.7143, recall: 0.0781\n",
      "2019-01-02T17:03:16.411305, step: 427, loss: 0.6560070514678955, acc: 0.5, auc: 0.565, precision: 0.7273, recall: 0.1159\n",
      "2019-01-02T17:03:16.828923, step: 428, loss: 0.6633373498916626, acc: 0.5078, auc: 0.5834, precision: 0.75, recall: 0.1304\n",
      "2019-01-02T17:03:17.257685, step: 429, loss: 0.6967064738273621, acc: 0.4688, auc: 0.5175, precision: 0.6, recall: 0.0857\n",
      "2019-01-02T17:03:17.678703, step: 430, loss: 0.6691849231719971, acc: 0.4688, auc: 0.6195, precision: 0.8, recall: 0.0563\n",
      "2019-01-02T17:03:18.144680, step: 431, loss: 0.671615719795227, acc: 0.5312, auc: 0.554, precision: 0.75, recall: 0.1364\n",
      "2019-01-02T17:03:18.561574, step: 432, loss: 0.625207245349884, acc: 0.6484, auc: 0.7335, precision: 0.9091, recall: 0.1852\n",
      "2019-01-02T17:03:18.951887, step: 433, loss: 0.6987124681472778, acc: 0.5078, auc: 0.4729, precision: 0.5385, recall: 0.1094\n",
      "2019-01-02T17:03:19.344373, step: 434, loss: 0.6804945468902588, acc: 0.5625, auc: 0.5468, precision: 0.6, recall: 0.1034\n",
      "2019-01-02T17:03:19.746367, step: 435, loss: 0.6830655336380005, acc: 0.5703, auc: 0.5951, precision: 0.6923, recall: 0.15\n",
      "2019-01-02T17:03:20.148962, step: 436, loss: 0.6575506925582886, acc: 0.5547, auc: 0.6117, precision: 0.9091, recall: 0.1515\n",
      "2019-01-02T17:03:20.553985, step: 437, loss: 0.667370080947876, acc: 0.5938, auc: 0.6061, precision: 0.9167, recall: 0.1774\n",
      "2019-01-02T17:03:20.967315, step: 438, loss: 0.638505756855011, acc: 0.5156, auc: 0.6564, precision: 0.875, recall: 0.1029\n",
      "2019-01-02T17:03:21.366389, step: 439, loss: 0.6725344657897949, acc: 0.6016, auc: 0.5543, precision: 0.7778, recall: 0.125\n",
      "2019-01-02T17:03:21.790792, step: 440, loss: 0.6698557138442993, acc: 0.4531, auc: 0.5799, precision: 1.0, recall: 0.0667\n",
      "2019-01-02T17:03:22.211502, step: 441, loss: 0.669484555721283, acc: 0.5938, auc: 0.5627, precision: 0.9, recall: 0.15\n",
      "2019-01-02T17:03:22.625814, step: 442, loss: 0.6831984519958496, acc: 0.5703, auc: 0.5273, precision: 1.0, recall: 0.0984\n",
      "2019-01-02T17:03:23.022366, step: 443, loss: 0.6736091375350952, acc: 0.5156, auc: 0.5899, precision: 0.7, recall: 0.1061\n",
      "2019-01-02T17:03:23.415426, step: 444, loss: 0.6816837787628174, acc: 0.5781, auc: 0.5259, precision: 0.8333, recall: 0.1613\n",
      "2019-01-02T17:03:23.819066, step: 445, loss: 0.6587717533111572, acc: 0.5703, auc: 0.5717, precision: 0.6, recall: 0.0536\n",
      "2019-01-02T17:03:24.236628, step: 446, loss: 0.6657676696777344, acc: 0.5938, auc: 0.6146, precision: 1.0, recall: 0.1186\n",
      "2019-01-02T17:03:24.645907, step: 447, loss: 0.691443145275116, acc: 0.5625, auc: 0.5202, precision: 0.8571, recall: 0.0984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:03:25.071058, step: 448, loss: 0.6355047225952148, acc: 0.5547, auc: 0.6703, precision: 0.8, recall: 0.1818\n",
      "2019-01-02T17:03:25.462673, step: 449, loss: 0.661806583404541, acc: 0.5781, auc: 0.5726, precision: 0.7273, recall: 0.1356\n",
      "2019-01-02T17:03:25.877162, step: 450, loss: 0.6549288630485535, acc: 0.5938, auc: 0.5855, precision: 0.9, recall: 0.15\n",
      "2019-01-02T17:03:26.317839, step: 451, loss: 0.6513785123825073, acc: 0.625, auc: 0.5886, precision: 0.875, recall: 0.1296\n",
      "2019-01-02T17:03:26.721265, step: 452, loss: 0.7000529766082764, acc: 0.4297, auc: 0.5931, precision: 0.7143, recall: 0.0658\n",
      "2019-01-02T17:03:27.102502, step: 453, loss: 0.6909462809562683, acc: 0.4375, auc: 0.588, precision: 0.6667, recall: 0.0274\n",
      "2019-01-02T17:03:27.482908, step: 454, loss: 0.6493810415267944, acc: 0.5625, auc: 0.6144, precision: 0.8889, recall: 0.127\n",
      "2019-01-02T17:03:27.907136, step: 455, loss: 0.6854274868965149, acc: 0.5469, auc: 0.5639, precision: 0.7273, recall: 0.127\n",
      "2019-01-02T17:03:28.301011, step: 456, loss: 0.6505333185195923, acc: 0.5547, auc: 0.6303, precision: 0.6667, recall: 0.0345\n",
      "2019-01-02T17:03:28.722886, step: 457, loss: 0.6845072507858276, acc: 0.5469, auc: 0.4977, precision: 0.8889, recall: 0.1231\n",
      "2019-01-02T17:03:29.123066, step: 458, loss: 0.6520847082138062, acc: 0.5703, auc: 0.6126, precision: 0.75, recall: 0.0526\n",
      "2019-01-02T17:03:29.547688, step: 459, loss: 0.6542548537254333, acc: 0.5312, auc: 0.5814, precision: 0.8889, recall: 0.1194\n",
      "2019-01-02T17:03:29.957157, step: 460, loss: 0.6514631509780884, acc: 0.6172, auc: 0.6232, precision: 1.0, recall: 0.1552\n",
      "2019-01-02T17:03:30.360648, step: 461, loss: 0.6913751363754272, acc: 0.5, auc: 0.5921, precision: 0.7143, recall: 0.0746\n",
      "2019-01-02T17:03:30.765631, step: 462, loss: 0.663705587387085, acc: 0.5312, auc: 0.6437, precision: 0.6667, recall: 0.0952\n",
      "2019-01-02T17:03:31.185256, step: 463, loss: 0.6501867771148682, acc: 0.6094, auc: 0.6009, precision: 1.0, recall: 0.1228\n",
      "2019-01-02T17:03:31.573358, step: 464, loss: 0.6711441278457642, acc: 0.5469, auc: 0.5614, precision: 0.8889, recall: 0.1231\n",
      "2019-01-02T17:03:31.982360, step: 465, loss: 0.6831947565078735, acc: 0.5156, auc: 0.5442, precision: 0.5, recall: 0.0645\n",
      "2019-01-02T17:03:32.378357, step: 466, loss: 0.6926543712615967, acc: 0.5234, auc: 0.5145, precision: 1.0, recall: 0.0615\n",
      "2019-01-02T17:03:32.776800, step: 467, loss: 0.692550778388977, acc: 0.4531, auc: 0.5591, precision: 0.8333, recall: 0.0676\n",
      "2019-01-02T17:03:33.186433, step: 468, loss: 0.6580891609191895, acc: 0.4766, auc: 0.6391, precision: 0.8571, recall: 0.0833\n",
      "start training model\n",
      "2019-01-02T17:03:33.647866, step: 469, loss: 0.6317768096923828, acc: 0.5938, auc: 0.622, precision: 0.9231, recall: 0.1905\n",
      "2019-01-02T17:03:34.035094, step: 470, loss: 0.6524775624275208, acc: 0.5391, auc: 0.5913, precision: 0.8571, recall: 0.0938\n",
      "2019-01-02T17:03:34.448194, step: 471, loss: 0.6219378113746643, acc: 0.5391, auc: 0.6543, precision: 0.7778, recall: 0.1094\n",
      "2019-01-02T17:03:34.858001, step: 472, loss: 0.6789018511772156, acc: 0.5781, auc: 0.5971, precision: 0.8462, recall: 0.1746\n",
      "2019-01-02T17:03:35.254726, step: 473, loss: 0.6496875286102295, acc: 0.6172, auc: 0.5987, precision: 0.8333, recall: 0.0943\n",
      "2019-01-02T17:03:35.670172, step: 474, loss: 0.6769018173217773, acc: 0.5078, auc: 0.5904, precision: 0.7, recall: 0.1045\n",
      "2019-01-02T17:03:36.136215, step: 475, loss: 0.6359073519706726, acc: 0.5625, auc: 0.6512, precision: 1.0, recall: 0.1765\n",
      "2019-01-02T17:03:36.545470, step: 476, loss: 0.6602997779846191, acc: 0.5781, auc: 0.5702, precision: 0.9231, recall: 0.1846\n",
      "2019-01-02T17:03:36.958223, step: 477, loss: 0.6522221565246582, acc: 0.5938, auc: 0.632, precision: 0.8889, recall: 0.1356\n",
      "2019-01-02T17:03:37.341313, step: 478, loss: 0.6214220523834229, acc: 0.5234, auc: 0.6565, precision: 0.9231, recall: 0.1667\n",
      "2019-01-02T17:03:37.734784, step: 479, loss: 0.6706609725952148, acc: 0.6406, auc: 0.5552, precision: 0.875, recall: 0.1346\n",
      "2019-01-02T17:03:38.131899, step: 480, loss: 0.6277824640274048, acc: 0.5625, auc: 0.6538, precision: 0.7273, recall: 0.1311\n",
      "2019-01-02T17:03:38.516162, step: 481, loss: 0.6603513956069946, acc: 0.5469, auc: 0.6203, precision: 0.8889, recall: 0.1231\n",
      "2019-01-02T17:03:38.895548, step: 482, loss: 0.6148957014083862, acc: 0.6016, auc: 0.6329, precision: 1.0, recall: 0.2273\n",
      "2019-01-02T17:03:39.291840, step: 483, loss: 0.643523097038269, acc: 0.5859, auc: 0.6728, precision: 0.7692, recall: 0.1667\n",
      "2019-01-02T17:03:39.666549, step: 484, loss: 0.6445924043655396, acc: 0.5625, auc: 0.5673, precision: 0.8182, recall: 0.1429\n",
      "2019-01-02T17:03:40.058611, step: 485, loss: 0.6609007120132446, acc: 0.5703, auc: 0.589, precision: 0.7778, recall: 0.1167\n",
      "2019-01-02T17:03:40.445197, step: 486, loss: 0.6404311060905457, acc: 0.5234, auc: 0.6735, precision: 0.7692, recall: 0.1471\n",
      "2019-01-02T17:03:40.835239, step: 487, loss: 0.6388034820556641, acc: 0.5312, auc: 0.5724, precision: 0.9167, recall: 0.1571\n",
      "2019-01-02T17:03:41.211938, step: 488, loss: 0.6352798938751221, acc: 0.5391, auc: 0.635, precision: 1.0, recall: 0.1449\n",
      "2019-01-02T17:03:41.617667, step: 489, loss: 0.6898549795150757, acc: 0.5938, auc: 0.5535, precision: 0.6, recall: 0.0566\n",
      "2019-01-02T17:03:42.042628, step: 490, loss: 0.647057831287384, acc: 0.6094, auc: 0.5782, precision: 1.0, recall: 0.1525\n",
      "2019-01-02T17:03:42.450627, step: 491, loss: 0.6411591172218323, acc: 0.6016, auc: 0.6298, precision: 1.0, recall: 0.1053\n",
      "2019-01-02T17:03:42.853975, step: 492, loss: 0.6333783864974976, acc: 0.6172, auc: 0.6392, precision: 1.0, recall: 0.1404\n",
      "2019-01-02T17:03:43.313344, step: 493, loss: 0.6470202207565308, acc: 0.5312, auc: 0.6264, precision: 0.7273, recall: 0.1231\n",
      "2019-01-02T17:03:43.703870, step: 494, loss: 0.6362556219100952, acc: 0.5781, auc: 0.588, precision: 1.0, recall: 0.129\n",
      "2019-01-02T17:03:44.089658, step: 495, loss: 0.6535326838493347, acc: 0.5312, auc: 0.6078, precision: 1.0, recall: 0.0909\n",
      "2019-01-02T17:03:44.485677, step: 496, loss: 0.6607101559638977, acc: 0.5391, auc: 0.6406, precision: 0.7778, recall: 0.1094\n",
      "2019-01-02T17:03:44.881368, step: 497, loss: 0.6119371056556702, acc: 0.5, auc: 0.7119, precision: 0.8667, recall: 0.1733\n",
      "2019-01-02T17:03:45.260599, step: 498, loss: 0.6448121070861816, acc: 0.6094, auc: 0.6279, precision: 1.0, recall: 0.0909\n",
      "2019-01-02T17:03:45.682542, step: 499, loss: 0.6475241184234619, acc: 0.5312, auc: 0.6159, precision: 0.7778, recall: 0.1077\n",
      "2019-01-02T17:03:46.104771, step: 500, loss: 0.661531388759613, acc: 0.5547, auc: 0.5919, precision: 0.6364, recall: 0.1167\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:04:01.814269, step: 500, loss: 0.704825372267992, acc: 0.5094153846153846, auc: 0.5634256410256411, precision: 0.602151282051282, recall: 0.0898153846153846\n",
      "2019-01-02T17:04:02.195726, step: 501, loss: 0.6471989154815674, acc: 0.5781, auc: 0.633, precision: 0.8889, recall: 0.1311\n",
      "2019-01-02T17:04:02.601689, step: 502, loss: 0.6194055676460266, acc: 0.5547, auc: 0.6873, precision: 0.9231, recall: 0.1765\n",
      "2019-01-02T17:04:03.048568, step: 503, loss: 0.6225011944770813, acc: 0.5, auc: 0.6734, precision: 0.9167, recall: 0.1486\n",
      "2019-01-02T17:04:03.474521, step: 504, loss: 0.6337537169456482, acc: 0.625, auc: 0.6221, precision: 1.0, recall: 0.1111\n",
      "2019-01-02T17:04:03.892503, step: 505, loss: 0.6532784700393677, acc: 0.5, auc: 0.6323, precision: 0.6923, recall: 0.1304\n",
      "2019-01-02T17:04:04.302791, step: 506, loss: 0.6312525272369385, acc: 0.5312, auc: 0.6835, precision: 1.0, recall: 0.1549\n",
      "2019-01-02T17:04:04.715842, step: 507, loss: 0.6409289836883545, acc: 0.5, auc: 0.6451, precision: 0.85, recall: 0.2179\n",
      "2019-01-02T17:04:05.119083, step: 508, loss: 0.6900084018707275, acc: 0.6016, auc: 0.5521, precision: 0.5556, recall: 0.0962\n",
      "2019-01-02T17:04:05.526502, step: 509, loss: 0.6512544751167297, acc: 0.5938, auc: 0.5918, precision: 0.6364, recall: 0.1273\n",
      "2019-01-02T17:04:05.915000, step: 510, loss: 0.637763261795044, acc: 0.6094, auc: 0.6281, precision: 0.8824, recall: 0.2381\n",
      "2019-01-02T17:04:06.313704, step: 511, loss: 0.6722912788391113, acc: 0.5, auc: 0.5325, precision: 0.8333, recall: 0.1389\n",
      "2019-01-02T17:04:06.697615, step: 512, loss: 0.651718258857727, acc: 0.5391, auc: 0.6542, precision: 0.8, recall: 0.0645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:04:07.055249, step: 513, loss: 0.6634961366653442, acc: 0.5625, auc: 0.6163, precision: 0.7143, recall: 0.1613\n",
      "2019-01-02T17:04:07.472477, step: 514, loss: 0.6541314721107483, acc: 0.5781, auc: 0.5623, precision: 0.8333, recall: 0.2273\n",
      "2019-01-02T17:04:07.913293, step: 515, loss: 0.6484103202819824, acc: 0.5312, auc: 0.616, precision: 0.8125, recall: 0.1857\n",
      "2019-01-02T17:04:08.364946, step: 516, loss: 0.6800132989883423, acc: 0.5625, auc: 0.551, precision: 0.6923, recall: 0.1475\n",
      "2019-01-02T17:04:08.794072, step: 517, loss: 0.6626937985420227, acc: 0.5625, auc: 0.5961, precision: 0.6667, recall: 0.069\n",
      "2019-01-02T17:04:09.193845, step: 518, loss: 0.6466397047042847, acc: 0.5391, auc: 0.5623, precision: 0.9, recall: 0.1343\n",
      "2019-01-02T17:04:09.598953, step: 519, loss: 0.6786009073257446, acc: 0.5312, auc: 0.5553, precision: 0.6667, recall: 0.0952\n",
      "2019-01-02T17:04:10.009889, step: 520, loss: 0.6652137041091919, acc: 0.5078, auc: 0.6004, precision: 0.7, recall: 0.1045\n",
      "2019-01-02T17:04:10.392409, step: 521, loss: 0.6439817547798157, acc: 0.5, auc: 0.6016, precision: 0.9167, recall: 0.1486\n",
      "2019-01-02T17:04:10.823156, step: 522, loss: 0.6474502682685852, acc: 0.5547, auc: 0.6291, precision: 1.0, recall: 0.0952\n",
      "2019-01-02T17:04:11.233644, step: 523, loss: 0.6492792367935181, acc: 0.6172, auc: 0.6056, precision: 0.7273, recall: 0.1481\n",
      "2019-01-02T17:04:11.644719, step: 524, loss: 0.644696056842804, acc: 0.5547, auc: 0.6684, precision: 0.7273, recall: 0.129\n",
      "2019-01-02T17:04:12.069812, step: 525, loss: 0.6827719211578369, acc: 0.5469, auc: 0.5217, precision: 0.5385, recall: 0.1186\n",
      "2019-01-02T17:04:12.465103, step: 526, loss: 0.6462537050247192, acc: 0.6016, auc: 0.6328, precision: 0.8889, recall: 0.1379\n",
      "2019-01-02T17:04:12.847912, step: 527, loss: 0.6361230611801147, acc: 0.5625, auc: 0.6424, precision: 0.75, recall: 0.1\n",
      "2019-01-02T17:04:13.202107, step: 528, loss: 0.6667506694793701, acc: 0.5156, auc: 0.5443, precision: 0.5385, recall: 0.1111\n",
      "2019-01-02T17:04:13.591159, step: 529, loss: 0.65632164478302, acc: 0.5312, auc: 0.6508, precision: 0.7692, recall: 0.1493\n",
      "2019-01-02T17:04:14.039258, step: 530, loss: 0.645720362663269, acc: 0.5547, auc: 0.5849, precision: 0.875, recall: 0.1111\n",
      "2019-01-02T17:04:14.453958, step: 531, loss: 0.6448239088058472, acc: 0.5391, auc: 0.6126, precision: 1.0, recall: 0.1571\n",
      "2019-01-02T17:04:14.849561, step: 532, loss: 0.6730595231056213, acc: 0.5312, auc: 0.5417, precision: 0.8333, recall: 0.1471\n",
      "2019-01-02T17:04:15.250216, step: 533, loss: 0.6745229959487915, acc: 0.4531, auc: 0.5728, precision: 0.7273, recall: 0.1067\n",
      "2019-01-02T17:04:15.652218, step: 534, loss: 0.6434744596481323, acc: 0.5391, auc: 0.5973, precision: 1.0, recall: 0.1061\n",
      "2019-01-02T17:04:16.037673, step: 535, loss: 0.6332545280456543, acc: 0.6328, auc: 0.6411, precision: 0.8333, recall: 0.2542\n",
      "2019-01-02T17:04:16.416801, step: 536, loss: 0.6494266390800476, acc: 0.5156, auc: 0.6434, precision: 0.9167, recall: 0.1528\n",
      "2019-01-02T17:04:16.829496, step: 537, loss: 0.6393793821334839, acc: 0.5938, auc: 0.6301, precision: 0.7273, recall: 0.1404\n",
      "2019-01-02T17:04:17.289039, step: 538, loss: 0.6545287370681763, acc: 0.5625, auc: 0.6084, precision: 0.8333, recall: 0.1562\n",
      "2019-01-02T17:04:17.708572, step: 539, loss: 0.640735387802124, acc: 0.5078, auc: 0.652, precision: 0.7647, recall: 0.1806\n",
      "2019-01-02T17:04:18.132590, step: 540, loss: 0.6481733322143555, acc: 0.4844, auc: 0.5974, precision: 0.6667, recall: 0.087\n",
      "2019-01-02T17:04:18.537735, step: 541, loss: 0.6553748846054077, acc: 0.5625, auc: 0.643, precision: 0.7778, recall: 0.1148\n",
      "2019-01-02T17:04:18.940476, step: 542, loss: 0.664212167263031, acc: 0.5, auc: 0.5775, precision: 0.7273, recall: 0.1159\n",
      "2019-01-02T17:04:19.323469, step: 543, loss: 0.6699288487434387, acc: 0.5938, auc: 0.5446, precision: 1.0, recall: 0.1034\n",
      "2019-01-02T17:04:19.726348, step: 544, loss: 0.6433987617492676, acc: 0.6406, auc: 0.6379, precision: 0.9, recall: 0.1667\n",
      "2019-01-02T17:04:20.117514, step: 545, loss: 0.653239905834198, acc: 0.5312, auc: 0.5791, precision: 0.7, recall: 0.1094\n",
      "2019-01-02T17:04:20.514472, step: 546, loss: 0.648941159248352, acc: 0.4766, auc: 0.6076, precision: 0.7778, recall: 0.0972\n",
      "2019-01-02T17:04:20.907748, step: 547, loss: 0.6470106244087219, acc: 0.625, auc: 0.6141, precision: 0.7, recall: 0.1346\n",
      "2019-01-02T17:04:21.305452, step: 548, loss: 0.6328385472297668, acc: 0.6094, auc: 0.6042, precision: 0.7778, recall: 0.1273\n",
      "2019-01-02T17:04:21.713648, step: 549, loss: 0.6593930721282959, acc: 0.4688, auc: 0.6022, precision: 0.6667, recall: 0.1111\n",
      "2019-01-02T17:04:22.127493, step: 550, loss: 0.6538511514663696, acc: 0.4844, auc: 0.6214, precision: 1.0, recall: 0.0959\n",
      "2019-01-02T17:04:22.540573, step: 551, loss: 0.6360118389129639, acc: 0.5625, auc: 0.6701, precision: 0.8889, recall: 0.127\n",
      "2019-01-02T17:04:22.949506, step: 552, loss: 0.6436553001403809, acc: 0.5469, auc: 0.6339, precision: 0.6667, recall: 0.129\n",
      "2019-01-02T17:04:23.321711, step: 553, loss: 0.6319526433944702, acc: 0.5234, auc: 0.6091, precision: 0.8571, recall: 0.169\n",
      "2019-01-02T17:04:23.688384, step: 554, loss: 0.6245781779289246, acc: 0.5234, auc: 0.6289, precision: 0.8125, recall: 0.1831\n",
      "2019-01-02T17:04:24.097467, step: 555, loss: 0.6968397498130798, acc: 0.6094, auc: 0.5058, precision: 0.625, recall: 0.0962\n",
      "2019-01-02T17:04:24.534498, step: 556, loss: 0.675976574420929, acc: 0.5078, auc: 0.4812, precision: 0.5556, recall: 0.0781\n",
      "2019-01-02T17:04:24.941084, step: 557, loss: 0.6539673805236816, acc: 0.5078, auc: 0.5733, precision: 1.0, recall: 0.137\n",
      "2019-01-02T17:04:25.347470, step: 558, loss: 0.6117222905158997, acc: 0.6094, auc: 0.6669, precision: 0.8, recall: 0.2034\n",
      "2019-01-02T17:04:25.721399, step: 559, loss: 0.6442499160766602, acc: 0.5938, auc: 0.6207, precision: 0.8889, recall: 0.1356\n",
      "2019-01-02T17:04:26.122830, step: 560, loss: 0.654918909072876, acc: 0.5547, auc: 0.6034, precision: 1.0, recall: 0.1618\n",
      "2019-01-02T17:04:26.506930, step: 561, loss: 0.6598795056343079, acc: 0.5859, auc: 0.6007, precision: 1.0, recall: 0.1167\n",
      "2019-01-02T17:04:26.933549, step: 562, loss: 0.6396098136901855, acc: 0.5469, auc: 0.5754, precision: 0.875, recall: 0.1094\n",
      "2019-01-02T17:04:27.341729, step: 563, loss: 0.6695313453674316, acc: 0.4766, auc: 0.5543, precision: 0.8182, recall: 0.1216\n",
      "2019-01-02T17:04:27.753459, step: 564, loss: 0.6423735022544861, acc: 0.5078, auc: 0.5551, precision: 0.9091, recall: 0.1389\n",
      "2019-01-02T17:04:28.154864, step: 565, loss: 0.6126217842102051, acc: 0.6328, auc: 0.6397, precision: 0.9167, recall: 0.193\n",
      "2019-01-02T17:04:28.583360, step: 566, loss: 0.6792789101600647, acc: 0.5, auc: 0.606, precision: 0.8571, recall: 0.087\n",
      "2019-01-02T17:04:29.026005, step: 567, loss: 0.6047069430351257, acc: 0.6328, auc: 0.6689, precision: 1.0, recall: 0.1607\n",
      "2019-01-02T17:04:29.447687, step: 568, loss: 0.6441300511360168, acc: 0.5781, auc: 0.5856, precision: 0.9091, recall: 0.1587\n",
      "2019-01-02T17:04:29.865565, step: 569, loss: 0.6717044115066528, acc: 0.5234, auc: 0.565, precision: 0.8571, recall: 0.0909\n",
      "2019-01-02T17:04:30.294692, step: 570, loss: 0.6311032772064209, acc: 0.5703, auc: 0.5569, precision: 0.8889, recall: 0.129\n",
      "2019-01-02T17:04:30.720339, step: 571, loss: 0.6218379139900208, acc: 0.5469, auc: 0.6857, precision: 0.8571, recall: 0.0952\n",
      "2019-01-02T17:04:31.133495, step: 572, loss: 0.6418837904930115, acc: 0.4766, auc: 0.6401, precision: 0.625, recall: 0.0725\n",
      "2019-01-02T17:04:31.535355, step: 573, loss: 0.651349663734436, acc: 0.5312, auc: 0.6188, precision: 0.8889, recall: 0.1194\n",
      "2019-01-02T17:04:31.964426, step: 574, loss: 0.6458536386489868, acc: 0.4766, auc: 0.6109, precision: 0.7692, recall: 0.1351\n",
      "2019-01-02T17:04:32.384011, step: 575, loss: 0.6457881927490234, acc: 0.5156, auc: 0.6164, precision: 0.7143, recall: 0.0769\n",
      "2019-01-02T17:04:32.764557, step: 576, loss: 0.6547892689704895, acc: 0.5547, auc: 0.6505, precision: 0.9, recall: 0.1385\n",
      "2019-01-02T17:04:33.176567, step: 577, loss: 0.6545119881629944, acc: 0.6016, auc: 0.5873, precision: 0.9091, recall: 0.1667\n",
      "2019-01-02T17:04:33.571921, step: 578, loss: 0.625603199005127, acc: 0.5781, auc: 0.6676, precision: 0.8667, recall: 0.2\n",
      "2019-01-02T17:04:33.984686, step: 579, loss: 0.6621994972229004, acc: 0.5156, auc: 0.5426, precision: 0.7692, recall: 0.1449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:04:34.382851, step: 580, loss: 0.6683982610702515, acc: 0.5703, auc: 0.5702, precision: 0.6667, recall: 0.1034\n",
      "2019-01-02T17:04:34.819653, step: 581, loss: 0.6807287335395813, acc: 0.5312, auc: 0.546, precision: 0.6667, recall: 0.1538\n",
      "2019-01-02T17:04:35.202826, step: 582, loss: 0.6380206942558289, acc: 0.5703, auc: 0.5899, precision: 1.0, recall: 0.1791\n",
      "2019-01-02T17:04:35.605509, step: 583, loss: 0.6494115591049194, acc: 0.5859, auc: 0.6504, precision: 0.9, recall: 0.1475\n",
      "2019-01-02T17:04:36.021379, step: 584, loss: 0.6834056973457336, acc: 0.5469, auc: 0.5978, precision: 0.6667, recall: 0.129\n",
      "2019-01-02T17:04:36.410965, step: 585, loss: 0.6355092525482178, acc: 0.5547, auc: 0.6056, precision: 0.8, recall: 0.0667\n",
      "2019-01-02T17:04:36.794250, step: 586, loss: 0.6619155406951904, acc: 0.5938, auc: 0.5883, precision: 0.8571, recall: 0.1053\n",
      "2019-01-02T17:04:37.196180, step: 587, loss: 0.6471589803695679, acc: 0.625, auc: 0.6394, precision: 0.7692, recall: 0.1818\n",
      "2019-01-02T17:04:37.564258, step: 588, loss: 0.6769079566001892, acc: 0.5469, auc: 0.5963, precision: 0.8571, recall: 0.0952\n",
      "2019-01-02T17:04:37.979966, step: 589, loss: 0.6511644124984741, acc: 0.5547, auc: 0.6159, precision: 0.7857, recall: 0.1692\n",
      "2019-01-02T17:04:38.377235, step: 590, loss: 0.6949582099914551, acc: 0.5312, auc: 0.5691, precision: 1.0, recall: 0.1176\n",
      "2019-01-02T17:04:38.832246, step: 591, loss: 0.6568058729171753, acc: 0.5781, auc: 0.5287, precision: 0.8, recall: 0.1333\n",
      "2019-01-02T17:04:39.243541, step: 592, loss: 0.6260604858398438, acc: 0.5625, auc: 0.642, precision: 0.9091, recall: 0.1538\n",
      "2019-01-02T17:04:39.646682, step: 593, loss: 0.6352404356002808, acc: 0.5781, auc: 0.6212, precision: 1.0, recall: 0.069\n",
      "2019-01-02T17:04:40.029270, step: 594, loss: 0.6333092451095581, acc: 0.5469, auc: 0.6444, precision: 0.8571, recall: 0.1765\n",
      "2019-01-02T17:04:40.411953, step: 595, loss: 0.6362685561180115, acc: 0.5781, auc: 0.6549, precision: 0.7, recall: 0.1207\n",
      "2019-01-02T17:04:40.845054, step: 596, loss: 0.6585947275161743, acc: 0.5938, auc: 0.5527, precision: 0.6667, recall: 0.1091\n",
      "2019-01-02T17:04:41.232112, step: 597, loss: 0.642479419708252, acc: 0.5547, auc: 0.609, precision: 0.75, recall: 0.0984\n",
      "2019-01-02T17:04:41.618295, step: 598, loss: 0.6613669395446777, acc: 0.4688, auc: 0.6131, precision: 0.8333, recall: 0.1316\n",
      "2019-01-02T17:04:42.030072, step: 599, loss: 0.6646404266357422, acc: 0.5703, auc: 0.5708, precision: 0.625, recall: 0.0877\n",
      "2019-01-02T17:04:42.446453, step: 600, loss: 0.6850524544715881, acc: 0.5078, auc: 0.5552, precision: 0.7857, recall: 0.1549\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T17:04:58.267004, step: 600, loss: 0.7093898990215399, acc: 0.5132282051282052, auc: 0.6083589743589746, precision: 0.6189512820512819, recall: 0.10089999999999999\n",
      "2019-01-02T17:04:58.673646, step: 601, loss: 0.6568691730499268, acc: 0.5547, auc: 0.6002, precision: 0.8462, recall: 0.1667\n",
      "2019-01-02T17:04:59.071211, step: 602, loss: 0.6523838043212891, acc: 0.5703, auc: 0.6252, precision: 0.8, recall: 0.1875\n",
      "2019-01-02T17:04:59.450179, step: 603, loss: 0.6255661845207214, acc: 0.5469, auc: 0.6308, precision: 0.8, recall: 0.1791\n",
      "2019-01-02T17:04:59.889327, step: 604, loss: 0.6482036113739014, acc: 0.5781, auc: 0.6252, precision: 0.7143, recall: 0.1667\n",
      "2019-01-02T17:05:00.281485, step: 605, loss: 0.6736702919006348, acc: 0.5547, auc: 0.5892, precision: 0.7, recall: 0.1148\n",
      "2019-01-02T17:05:00.709981, step: 606, loss: 0.6459425687789917, acc: 0.5859, auc: 0.5509, precision: 1.0, recall: 0.1587\n",
      "2019-01-02T17:05:01.135388, step: 607, loss: 0.6107773780822754, acc: 0.5859, auc: 0.7291, precision: 0.8, recall: 0.1356\n",
      "2019-01-02T17:05:01.525390, step: 608, loss: 0.6698189973831177, acc: 0.4922, auc: 0.5648, precision: 0.8571, recall: 0.0857\n",
      "2019-01-02T17:05:01.935719, step: 609, loss: 0.607187807559967, acc: 0.5469, auc: 0.6542, precision: 1.0, recall: 0.1471\n",
      "2019-01-02T17:05:02.345610, step: 610, loss: 0.6486806273460388, acc: 0.5078, auc: 0.669, precision: 0.9167, recall: 0.1507\n",
      "2019-01-02T17:05:02.770144, step: 611, loss: 0.6432536840438843, acc: 0.5938, auc: 0.571, precision: 0.9286, recall: 0.2031\n",
      "2019-01-02T17:05:03.204487, step: 612, loss: 0.6715639233589172, acc: 0.5703, auc: 0.6117, precision: 0.7143, recall: 0.1639\n",
      "2019-01-02T17:05:03.577681, step: 613, loss: 0.6634034514427185, acc: 0.5, auc: 0.5795, precision: 0.7778, recall: 0.1014\n",
      "2019-01-02T17:05:03.988620, step: 614, loss: 0.6498545408248901, acc: 0.6719, auc: 0.6728, precision: 0.7857, recall: 0.22\n",
      "2019-01-02T17:05:04.374549, step: 615, loss: 0.6329519748687744, acc: 0.5625, auc: 0.6244, precision: 1.0, recall: 0.1642\n",
      "2019-01-02T17:05:04.788268, step: 616, loss: 0.6349385976791382, acc: 0.5547, auc: 0.6645, precision: 0.75, recall: 0.0984\n",
      "2019-01-02T17:05:05.204147, step: 617, loss: 0.6425784826278687, acc: 0.5625, auc: 0.679, precision: 0.9, recall: 0.1406\n",
      "2019-01-02T17:05:05.621669, step: 618, loss: 0.6491157412528992, acc: 0.5781, auc: 0.6087, precision: 0.7778, recall: 0.1186\n",
      "2019-01-02T17:05:06.059390, step: 619, loss: 0.6374121904373169, acc: 0.5469, auc: 0.6503, precision: 0.7692, recall: 0.1538\n",
      "2019-01-02T17:05:06.514439, step: 620, loss: 0.6488370895385742, acc: 0.5781, auc: 0.574, precision: 1.0, recall: 0.1\n",
      "2019-01-02T17:05:06.962140, step: 621, loss: 0.6500717401504517, acc: 0.5234, auc: 0.6137, precision: 0.7692, recall: 0.1471\n",
      "2019-01-02T17:05:07.399394, step: 622, loss: 0.6495262980461121, acc: 0.5391, auc: 0.6132, precision: 0.9091, recall: 0.1471\n",
      "2019-01-02T17:05:07.836639, step: 623, loss: 0.6554940938949585, acc: 0.5625, auc: 0.6719, precision: 0.5714, recall: 0.0702\n",
      "2019-01-02T17:05:08.255570, step: 624, loss: 0.6066426634788513, acc: 0.5625, auc: 0.7278, precision: 0.9375, recall: 0.2143\n",
      "start training model\n",
      "2019-01-02T17:05:08.712782, step: 625, loss: 0.648190975189209, acc: 0.5625, auc: 0.6189, precision: 0.75, recall: 0.1\n",
      "2019-01-02T17:05:09.143395, step: 626, loss: 0.642680823802948, acc: 0.5625, auc: 0.6491, precision: 0.7778, recall: 0.1148\n",
      "2019-01-02T17:05:09.572722, step: 627, loss: 0.5970346927642822, acc: 0.6016, auc: 0.7111, precision: 1.0, recall: 0.1905\n",
      "2019-01-02T17:05:09.977339, step: 628, loss: 0.5934150218963623, acc: 0.5391, auc: 0.7477, precision: 0.9286, recall: 0.1831\n",
      "2019-01-02T17:05:10.409211, step: 629, loss: 0.5862841010093689, acc: 0.5703, auc: 0.7245, precision: 1.0, recall: 0.1791\n",
      "2019-01-02T17:05:10.851528, step: 630, loss: 0.6275976896286011, acc: 0.5469, auc: 0.6056, precision: 0.8824, recall: 0.2113\n",
      "2019-01-02T17:05:11.278510, step: 631, loss: 0.6186256408691406, acc: 0.5703, auc: 0.637, precision: 1.0, recall: 0.1912\n",
      "2019-01-02T17:05:11.663647, step: 632, loss: 0.6144975423812866, acc: 0.5156, auc: 0.7233, precision: 0.8, recall: 0.1176\n",
      "2019-01-02T17:05:12.071378, step: 633, loss: 0.5441975593566895, acc: 0.5703, auc: 0.8005, precision: 1.0, recall: 0.2029\n",
      "2019-01-02T17:05:12.482395, step: 634, loss: 0.6522235870361328, acc: 0.5547, auc: 0.5254, precision: 0.5714, recall: 0.069\n",
      "2019-01-02T17:05:12.930830, step: 635, loss: 0.6436150074005127, acc: 0.5312, auc: 0.6522, precision: 0.875, recall: 0.1061\n",
      "2019-01-02T17:05:13.360376, step: 636, loss: 0.651447594165802, acc: 0.5, auc: 0.625, precision: 0.75, recall: 0.0882\n",
      "2019-01-02T17:05:13.795290, step: 637, loss: 0.6387438774108887, acc: 0.5391, auc: 0.6213, precision: 1.0, recall: 0.1324\n",
      "2019-01-02T17:05:14.234204, step: 638, loss: 0.63861083984375, acc: 0.6172, auc: 0.6229, precision: 0.7778, recall: 0.1296\n",
      "2019-01-02T17:05:14.672161, step: 639, loss: 0.6531920433044434, acc: 0.5312, auc: 0.6026, precision: 1.0, recall: 0.0909\n",
      "2019-01-02T17:05:15.129202, step: 640, loss: 0.648177444934845, acc: 0.6016, auc: 0.5732, precision: 0.8571, recall: 0.1071\n",
      "2019-01-02T17:05:15.573818, step: 641, loss: 0.652361273765564, acc: 0.5469, auc: 0.5978, precision: 0.8889, recall: 0.1231\n",
      "2019-01-02T17:05:16.012273, step: 642, loss: 0.6564905643463135, acc: 0.5078, auc: 0.5549, precision: 0.75, recall: 0.1304\n",
      "2019-01-02T17:05:16.430401, step: 643, loss: 0.595431923866272, acc: 0.5234, auc: 0.6719, precision: 1.0, recall: 0.2278\n",
      "2019-01-02T17:05:16.856017, step: 644, loss: 0.6227150559425354, acc: 0.5859, auc: 0.7021, precision: 0.9091, recall: 0.1613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T17:05:17.290401, step: 645, loss: 0.6177353858947754, acc: 0.6094, auc: 0.6805, precision: 0.8333, recall: 0.1724\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTM(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
