{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import json\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数配置\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 20\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.0005\n",
    "    \n",
    "\n",
    "class ModelConfig(object):\n",
    "    \n",
    "    # 该列表中子列表的三个元素分别是卷积核的数量，卷积核的高度，池化的尺寸\n",
    "    convLayers = [[256, 7, 4],\n",
    "                  [256, 7, 4],\n",
    "                  [256, 3, 4]]\n",
    "#                   [256, 3, None],\n",
    "#                   [256, 3, None],\n",
    "#                   [256, 3, 3]]\n",
    "    fcLayers = [512]\n",
    "    dropoutKeepProb = 0.5\n",
    "    \n",
    "    \n",
    "class Config(object):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "#     alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    \n",
    "    sequenceLength = 1014\n",
    "    batchSize = 128\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledCharTrain.csv\"\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self._alphabet = config.alphabet\n",
    "        self.charEmbedding =None\n",
    "        \n",
    "        self._charToIndex = {}\n",
    "        self._indexToChar = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [[char for char in line if char != \" \"] for line in review]\n",
    "        \n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, charToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in charToIndex:\n",
    "                reviewVec[i] = charToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = charToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._charToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成字符向量和字符-索引映射字典\n",
    "        \"\"\"\n",
    "        \n",
    "        chars = [char for char in self._alphabet]\n",
    "        \n",
    "        vocab, charEmbedding = self._getCharEmbedding(chars)\n",
    "        self.charEmbedding = charEmbedding\n",
    "        \n",
    "        self._charToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToChar = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/charJson/charToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._charToIndex, f)\n",
    "        \n",
    "        with open(\"../data/charJson/indexToChar.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToChar, f)\n",
    "            \n",
    "    def _getCharEmbedding(self, chars):\n",
    "        \"\"\"\n",
    "        按照one的形式将字符映射成向量\n",
    "        \"\"\"\n",
    "        \n",
    "        alphabet = [\"UNK\"] + [char for char in self._alphabet]\n",
    "        vocab = [\"pad\"] + alphabet\n",
    "        charEmbedding = []\n",
    "        charEmbedding.append(np.zeros(len(alphabet), dtype=\"float32\"))\n",
    "        \n",
    "        for i, alpha in enumerate(alphabet):\n",
    "            onehot = np.zeros(len(alphabet), dtype=\"float32\")\n",
    "            \n",
    "            # 生成每个字符对应的向量\n",
    "            onehot[i] = 1\n",
    "            \n",
    "            # 生成字符嵌入的向量矩阵\n",
    "            charEmbedding.append(onehot)\n",
    "                \n",
    "        return vocab, np.array(charEmbedding)\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 1014)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 1014)\n",
      "charEmbedding shape: (71, 70)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"charEmbedding shape: {}\".format(data.charEmbedding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = (len(x) - 1) // batchSize\n",
    "\n",
    "        for i in range(numBatches - 1):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义char-CNN分类器\n",
    "\n",
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    char-CNN用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, charEmbedding):\n",
    "        # placeholders for input, output and dropuot\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "        # 字符嵌入\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            \n",
    "            # 利用one-hot的字符向量作为初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(charEmbedding, dtype=tf.float32, name=\"charEmbedding\") ,name=\"W\")\n",
    "            # 获得字符嵌入\n",
    "            self.embededChars = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 添加一个通道维度\n",
    "            self.embededCharsExpand = tf.expand_dims(self.embededChars, -1)\n",
    "\n",
    "        for i, cl in enumerate(config.model.convLayers):\n",
    "            print(\"开始第\" + str(i + 1) + \"卷积层的处理\")\n",
    "            # 利用命名空间name_scope来实现变量名复用\n",
    "            with tf.name_scope(\"convLayer-%s\"%(i+1)):\n",
    "                # 获取字符的向量长度\n",
    "                filterWidth = self.embededCharsExpand.get_shape()[2].value\n",
    "                \n",
    "                # filterShape = [height, width, in_channels, out_channels]\n",
    "                filterShape = [cl[1], filterWidth, 1, cl[0]]\n",
    "\n",
    "                stdv = 1 / sqrt(cl[0] * cl[1])\n",
    "                \n",
    "                # 初始化w和b的值\n",
    "                wConv = tf.Variable(tf.random_uniform(filterShape, minval=-stdv, maxval=stdv),\n",
    "                                     dtype='float32', name='w')\n",
    "                bConv = tf.Variable(tf.random_uniform(shape=[cl[0]], minval=-stdv, maxval=stdv), name='b')\n",
    "                \n",
    "#                 w_conv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"w\")\n",
    "#                 b_conv = tf.Variable(tf.constant(0.1, shape=[cl[0]]), name=\"b\")\n",
    "                # 构建卷积层，可以直接将卷积核的初始化方法传入（w_conv）\n",
    "                conv = tf.nn.conv2d(self.embededCharsExpand, wConv, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "                # 加上偏差\n",
    "                hConv = tf.nn.bias_add(conv, bConv)\n",
    "                # 可以直接加上relu函数，因为tf.nn.conv2d事实上是做了一个卷积运算，然后在这个运算结果上加上偏差，再导入到relu函数中\n",
    "                \n",
    "                # h_conv = tf.nn.relu(h_conv)\n",
    "                if cl[-1] is not None:\n",
    "                    ksizeShape = [1, cl[2], 1, 1]\n",
    "                    hPool = tf.nn.max_pool(hConv, ksize=ksizeShape, strides=ksizeShape, padding=\"VALID\", name=\"pool\")\n",
    "                else:\n",
    "                    hPool = hConv\n",
    "                    \n",
    "                print(hPool.shape)\n",
    "    \n",
    "                # 对维度进行转换，转换成卷积层的输入维度\n",
    "                self.embededCharsExpand = tf.transpose(hPool, [0, 1, 3, 2], name=\"transpose\")\n",
    "        print(self.embededCharsExpand)\n",
    "        with tf.name_scope(\"reshape\"):\n",
    "            fcDim = self.embededCharsExpand.get_shape()[1].value * self.embededCharsExpand.get_shape()[2].value\n",
    "            self.inputReshape = tf.reshape(self.embededCharsExpand, [-1, fcDim])\n",
    "        \n",
    "        # 保存的是神经元的个数[34*256, 1024, 1024]\n",
    "        weights = [fcDim] + config.model.fcLayers\n",
    "        \n",
    "        for i, fl in enumerate(config.model.fcLayers):\n",
    "            with tf.name_scope(\"fcLayer-%s\"%(i+1)):\n",
    "                print(\"开始第\" + str(i + 1) + \"全连接层的处理\")\n",
    "                stdv = 1 / sqrt(weights[i])\n",
    "                \n",
    "                # 定义全连接层的初始化方法，均匀分布初始化w和b的值\n",
    "                wFc = tf.Variable(tf.random_uniform([weights[i], fl], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "                bFc = tf.Variable(tf.random_uniform(shape=[fl], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"b\")\n",
    "                \n",
    "#                 w_fc = tf.Variable(tf.truncated_normal([weights[i], fl], stddev=0.05), name=\"W\")\n",
    "#                 b_fc = tf.Variable(tf.constant(0.1, shape=[fl]), name=\"b\")\n",
    "                \n",
    "                self.fcInput = tf.nn.relu(tf.matmul(self.inputReshape, wFc) + bFc)\n",
    "                \n",
    "                with tf.name_scope(\"dropOut\"):\n",
    "                    self.fcInputDrop = tf.nn.dropout(self.fcInput, self.dropoutKeepProb)\n",
    "                    \n",
    "            self.inputReshape = self.fcInputDrop\n",
    "            \n",
    "        with tf.name_scope(\"outputLayer\"):\n",
    "            stdv = 1 / sqrt(weights[-1])\n",
    "            # 定义隐层到输出层的权重系数和偏差的初始化方法\n",
    "#             w_out = tf.Variable(tf.truncated_normal([fc_layers[-1], num_classes], stddev=0.1), name=\"W\")\n",
    "#             b_out = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            \n",
    "            wOut = tf.Variable(tf.random_uniform([config.model.fcLayers[-1], 1], minval=-stdv, maxval=stdv), dtype=\"float32\", name=\"w\")\n",
    "            bOut = tf.Variable(tf.random_uniform(shape=[1], minval=-stdv, maxval=stdv), name=\"b\")\n",
    "            # tf.nn.xw_plus_b就是x和w的乘积加上b\n",
    "            self.predictions = tf.nn.xw_plus_b(self.inputReshape, wOut, bOut, name=\"predictions\")\n",
    "            # 进行二元分类\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # 定义损失函数，对预测值进行softmax，再求交叉熵。\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    def batch_norm(self, x):\n",
    "        \n",
    "        gamma = tf.Variable(tf.ones([x.get_shape()[3].value]))\n",
    "        beta = tf.Variable(tf.zeros([x.get_shape()[3].value]))\n",
    "\n",
    "        pop_mean = tf.Variable(tf.zeros([x.get_shape()[3].value]), trainable=False)\n",
    "        pop_variance = tf.Variable(tf.ones([x.get_shape()[3].value]), trainable=False)\n",
    "\n",
    "        epsilon = 1e-3\n",
    "\n",
    "        def batch_norm_training():\n",
    "            # 一定要使用正确的维度确保计算的是每个特征图上的平均值和方差而不是整个网络节点上的统计分布值\n",
    "            batch_mean, batch_variance = tf.nn.moments(layer, [0, 1, 2], keep_dims=False)\n",
    "\n",
    "            decay = 0.99\n",
    "            train_mean = tf.assign(pop_mean, pop_mean*decay + batch_mean*(1 - decay))\n",
    "            train_variance = tf.assign(pop_variance, pop_variance*decay + batch_variance*(1 - decay))\n",
    "\n",
    "            with tf.control_dependencies([train_mean, train_variance]):\n",
    "                return tf.nn.batch_normalization(x, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "\n",
    "        def batch_norm_inference():\n",
    "            return tf.nn.batch_normalization(x, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "        batch_normalized_output = tf.cond(self.is_training, batch_norm_training, batch_norm_inference)\n",
    "        return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    \n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY, average='macro')\n",
    "    recall = recall_score(trueY, binaryPredY, average='macro')\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第1卷积层的处理\n",
      "(?, 252, 1, 256)\n",
      "开始第2卷积层的处理\n",
      "(?, 61, 1, 256)\n",
      "开始第3卷积层的处理\n",
      "(?, 14, 1, 256)\n",
      "Tensor(\"convLayer-3/transpose:0\", shape=(?, 14, 256, 1), dtype=float32)\n",
      "开始第1全连接层的处理\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/w:0/grad/hist is illegal; using convLayer-1/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/w:0/grad/sparsity is illegal; using convLayer-1/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/b:0/grad/hist is illegal; using convLayer-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-1/b:0/grad/sparsity is illegal; using convLayer-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/w:0/grad/hist is illegal; using convLayer-2/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/w:0/grad/sparsity is illegal; using convLayer-2/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/b:0/grad/hist is illegal; using convLayer-2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-2/b:0/grad/sparsity is illegal; using convLayer-2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/w:0/grad/hist is illegal; using convLayer-3/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/w:0/grad/sparsity is illegal; using convLayer-3/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/b:0/grad/hist is illegal; using convLayer-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name convLayer-3/b:0/grad/sparsity is illegal; using convLayer-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/w:0/grad/hist is illegal; using fcLayer-1/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/w:0/grad/sparsity is illegal; using fcLayer-1/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/b:0/grad/hist is illegal; using fcLayer-1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name fcLayer-1/b:0/grad/sparsity is illegal; using fcLayer-1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputLayer/w:0/grad/hist is illegal; using outputLayer/w_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputLayer/w:0/grad/sparsity is illegal; using outputLayer/w_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputLayer/b:0/grad/hist is illegal; using outputLayer/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputLayer/b:0/grad/sparsity is illegal; using outputLayer/b_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/charCNN/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-27T14:15:03.493806, step: 1, loss: 0.6926451921463013, acc: 0.5234, auc: 0.5275, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:03.618864, step: 2, loss: 0.6927921175956726, acc: 0.5234, auc: 0.4957, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:03.725090, step: 3, loss: 0.6927382349967957, acc: 0.5234, auc: 0.5209, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:03.842819, step: 4, loss: 0.6934384703636169, acc: 0.4844, auc: 0.4971, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:03.957297, step: 5, loss: 0.6942311525344849, acc: 0.4453, auc: 0.4826, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:04.075052, step: 6, loss: 0.6926590800285339, acc: 0.5312, auc: 0.4968, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:04.188378, step: 7, loss: 0.6936193108558655, acc: 0.4844, auc: 0.4804, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:04.306726, step: 8, loss: 0.6916887164115906, acc: 0.5547, auc: 0.5807, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:04.431138, step: 9, loss: 0.6932116746902466, acc: 0.4844, auc: 0.5489, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:04.543202, step: 10, loss: 0.6962931752204895, acc: 0.3438, auc: 0.4543, precision: 0.1719, recall: 0.5\n",
      "2018-12-27T14:15:04.660184, step: 11, loss: 0.6917357444763184, acc: 0.5781, auc: 0.5238, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:15:04.798191, step: 12, loss: 0.6932365894317627, acc: 0.4922, auc: 0.4899, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:04.916223, step: 13, loss: 0.6930546760559082, acc: 0.5469, auc: 0.402, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:05.046515, step: 14, loss: 0.6945449113845825, acc: 0.4453, auc: 0.4366, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:05.165241, step: 15, loss: 0.6936889290809631, acc: 0.4844, auc: 0.4636, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:05.288464, step: 16, loss: 0.6934921145439148, acc: 0.4922, auc: 0.4867, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:05.411901, step: 17, loss: 0.6937049031257629, acc: 0.4766, auc: 0.4769, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:05.540633, step: 18, loss: 0.6916448473930359, acc: 0.5625, auc: 0.5682, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:05.657148, step: 19, loss: 0.6925910711288452, acc: 0.5312, auc: 0.4838, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:05.770878, step: 20, loss: 0.6937745213508606, acc: 0.4453, auc: 0.5765, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:05.885469, step: 21, loss: 0.6928873658180237, acc: 0.5312, auc: 0.474, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:06.008428, step: 22, loss: 0.6940023899078369, acc: 0.4688, auc: 0.4623, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:06.127601, step: 23, loss: 0.6919799447059631, acc: 0.5625, auc: 0.5357, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:06.240673, step: 24, loss: 0.6919618844985962, acc: 0.5938, auc: 0.4676, precision: 0.2969, recall: 0.5\n",
      "2018-12-27T14:15:06.355247, step: 25, loss: 0.6926484704017639, acc: 0.4688, auc: 0.6684, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:06.469197, step: 26, loss: 0.6936127543449402, acc: 0.4922, auc: 0.4542, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:06.593101, step: 27, loss: 0.6929613351821899, acc: 0.4922, auc: 0.5519, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:06.727362, step: 28, loss: 0.6933006048202515, acc: 0.4922, auc: 0.5031, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:06.843424, step: 29, loss: 0.6926697492599487, acc: 0.5312, auc: 0.4922, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:06.971751, step: 30, loss: 0.6932116746902466, acc: 0.5234, auc: 0.4279, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:07.079894, step: 31, loss: 0.6934505701065063, acc: 0.5078, auc: 0.4427, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:07.198822, step: 32, loss: 0.6942664384841919, acc: 0.4609, auc: 0.4348, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:07.314380, step: 33, loss: 0.6934667825698853, acc: 0.4922, auc: 0.4769, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:07.432183, step: 34, loss: 0.69487464427948, acc: 0.4297, auc: 0.4262, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:07.557746, step: 35, loss: 0.6936936378479004, acc: 0.4844, auc: 0.4702, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:07.692523, step: 36, loss: 0.6935620307922363, acc: 0.4688, auc: 0.5355, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:07.812148, step: 37, loss: 0.6924431324005127, acc: 0.5547, auc: 0.4626, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:07.925025, step: 38, loss: 0.693742036819458, acc: 0.5, auc: 0.4104, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:08.041827, step: 39, loss: 0.6933332681655884, acc: 0.5078, auc: 0.4701, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:08.171724, step: 40, loss: 0.6928298473358154, acc: 0.5312, auc: 0.4625, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:08.297757, step: 41, loss: 0.6935035586357117, acc: 0.5, auc: 0.468, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:08.413643, step: 42, loss: 0.6926719546318054, acc: 0.5, auc: 0.5957, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:08.530712, step: 43, loss: 0.6923885345458984, acc: 0.5469, auc: 0.5177, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:08.652137, step: 44, loss: 0.6935441493988037, acc: 0.4922, auc: 0.4652, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:08.777682, step: 45, loss: 0.6927417516708374, acc: 0.5312, auc: 0.4973, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:08.890751, step: 46, loss: 0.6937360167503357, acc: 0.4531, auc: 0.5315, precision: 0.2266, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:15:09.016315, step: 47, loss: 0.694043755531311, acc: 0.4922, auc: 0.3963, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:09.133129, step: 48, loss: 0.6945300102233887, acc: 0.4531, auc: 0.4052, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:09.250021, step: 49, loss: 0.6936608552932739, acc: 0.5234, auc: 0.3861, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:09.366834, step: 50, loss: 0.6926628351211548, acc: 0.5547, auc: 0.447, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:09.481715, step: 51, loss: 0.6923799514770508, acc: 0.5469, auc: 0.5074, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:09.600520, step: 52, loss: 0.6922101974487305, acc: 0.5625, auc: 0.4774, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:09.720100, step: 53, loss: 0.692521333694458, acc: 0.5625, auc: 0.4095, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:09.852721, step: 54, loss: 0.6924678087234497, acc: 0.5156, auc: 0.5455, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:09.976903, step: 55, loss: 0.6940584182739258, acc: 0.4609, auc: 0.5072, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:10.104755, step: 56, loss: 0.6921546459197998, acc: 0.5859, auc: 0.4405, precision: 0.293, recall: 0.5\n",
      "2018-12-27T14:15:10.233279, step: 57, loss: 0.6932983994483948, acc: 0.5, auc: 0.5034, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:10.360705, step: 58, loss: 0.6930582523345947, acc: 0.5234, auc: 0.4529, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:10.473513, step: 59, loss: 0.6932399272918701, acc: 0.4922, auc: 0.5319, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:10.589002, step: 60, loss: 0.6941854953765869, acc: 0.4375, auc: 0.5203, precision: 0.2188, recall: 0.5\n",
      "2018-12-27T14:15:10.702686, step: 61, loss: 0.6914858818054199, acc: 0.5781, auc: 0.5205, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:15:10.836053, step: 62, loss: 0.6942851543426514, acc: 0.4453, auc: 0.4996, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:10.951724, step: 63, loss: 0.6935336589813232, acc: 0.4766, auc: 0.5241, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:11.070314, step: 64, loss: 0.6954646110534668, acc: 0.4141, auc: 0.4199, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:15:11.183450, step: 65, loss: 0.6961097717285156, acc: 0.3438, auc: 0.4583, precision: 0.1719, recall: 0.5\n",
      "2018-12-27T14:15:11.309868, step: 66, loss: 0.6920863389968872, acc: 0.5547, auc: 0.5619, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:11.425852, step: 67, loss: 0.6933218836784363, acc: 0.4922, auc: 0.4991, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:11.539081, step: 68, loss: 0.6924984455108643, acc: 0.5547, auc: 0.4712, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:11.654194, step: 69, loss: 0.6926332116127014, acc: 0.5234, auc: 0.5182, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:11.780027, step: 70, loss: 0.6920626163482666, acc: 0.5391, auc: 0.5637, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:11.883212, step: 71, loss: 0.6928108930587769, acc: 0.5391, auc: 0.4736, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:11.994849, step: 72, loss: 0.6922725439071655, acc: 0.5547, auc: 0.4682, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:12.122986, step: 73, loss: 0.6936913728713989, acc: 0.4844, auc: 0.4919, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:12.247040, step: 74, loss: 0.6934311389923096, acc: 0.4844, auc: 0.5154, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:12.361220, step: 75, loss: 0.6929280161857605, acc: 0.5078, auc: 0.5197, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:12.474984, step: 76, loss: 0.6937452554702759, acc: 0.4688, auc: 0.499, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:12.597317, step: 77, loss: 0.6931332349777222, acc: 0.4844, auc: 0.5508, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:12.721442, step: 78, loss: 0.692743718624115, acc: 0.5, auc: 0.5657, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:12.856610, step: 79, loss: 0.69109708070755, acc: 0.6094, auc: 0.5133, precision: 0.3047, recall: 0.5\n",
      "2018-12-27T14:15:12.992744, step: 80, loss: 0.6921542286872864, acc: 0.5625, auc: 0.4745, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:13.119783, step: 81, loss: 0.6925517320632935, acc: 0.5234, auc: 0.5075, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:13.263092, step: 82, loss: 0.6943087577819824, acc: 0.4297, auc: 0.5863, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:13.392441, step: 83, loss: 0.693496823310852, acc: 0.4844, auc: 0.4958, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:13.506361, step: 84, loss: 0.6916171908378601, acc: 0.5469, auc: 0.5946, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:13.624585, step: 85, loss: 0.692556619644165, acc: 0.5312, auc: 0.5127, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:13.737298, step: 86, loss: 0.6934800148010254, acc: 0.5156, auc: 0.4044, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:13.851669, step: 87, loss: 0.6935908794403076, acc: 0.4844, auc: 0.513, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:13.964081, step: 88, loss: 0.6937355995178223, acc: 0.5, auc: 0.3896, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:14.082263, step: 89, loss: 0.694068968296051, acc: 0.4688, auc: 0.4654, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:14.206194, step: 90, loss: 0.6931435465812683, acc: 0.5, auc: 0.4995, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:14.336099, step: 91, loss: 0.6937856674194336, acc: 0.4375, auc: 0.5985, precision: 0.2188, recall: 0.5\n",
      "2018-12-27T14:15:14.471097, step: 92, loss: 0.6922957301139832, acc: 0.5234, auc: 0.5809, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:14.586290, step: 93, loss: 0.693140983581543, acc: 0.4844, auc: 0.5638, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:14.714885, step: 94, loss: 0.6927353143692017, acc: 0.5078, auc: 0.5389, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:14.837589, step: 95, loss: 0.6931705474853516, acc: 0.4766, auc: 0.5363, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:14.953737, step: 96, loss: 0.6940986514091492, acc: 0.4844, auc: 0.3947, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:15.066730, step: 97, loss: 0.6931390762329102, acc: 0.5078, auc: 0.4884, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:15.179972, step: 98, loss: 0.6928349137306213, acc: 0.5547, auc: 0.4589, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:15.301421, step: 99, loss: 0.6938868761062622, acc: 0.4453, auc: 0.5278, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:15.421797, step: 100, loss: 0.6939113140106201, acc: 0.4453, auc: 0.4826, precision: 0.2227, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:15:20.132034, step: 100, loss: 0.6931916883117274, acc: 0.4948605263157897, auc: 0.4971578947368421, precision: 0.24743421052631592, recall: 0.5\n",
      "2018-12-27T14:15:20.250725, step: 101, loss: 0.69294673204422, acc: 0.4922, auc: 0.549, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:20.370568, step: 102, loss: 0.6936360597610474, acc: 0.4688, auc: 0.4451, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:20.487368, step: 103, loss: 0.6930270195007324, acc: 0.4844, auc: 0.4961, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:20.610436, step: 104, loss: 0.6927505731582642, acc: 0.5, auc: 0.5596, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:20.728213, step: 105, loss: 0.6936922073364258, acc: 0.4609, auc: 0.4235, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:20.854984, step: 106, loss: 0.6938207149505615, acc: 0.4922, auc: 0.3922, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:20.982689, step: 107, loss: 0.6929333209991455, acc: 0.4844, auc: 0.5027, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:21.118784, step: 108, loss: 0.6934348940849304, acc: 0.4844, auc: 0.4255, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:21.241004, step: 109, loss: 0.6939693689346313, acc: 0.5625, auc: 0.4804, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:21.374990, step: 110, loss: 0.6932337284088135, acc: 0.5234, auc: 0.4884, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:21.495664, step: 111, loss: 0.6931024789810181, acc: 0.4531, auc: 0.566, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:21.608723, step: 112, loss: 0.6928833723068237, acc: 0.5, auc: 0.5295, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:21.727038, step: 113, loss: 0.6937033534049988, acc: 0.5547, auc: 0.4779, precision: 0.2773, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:15:21.842364, step: 114, loss: 0.6930699944496155, acc: 0.4922, auc: 0.5031, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:21.955183, step: 115, loss: 0.6941208839416504, acc: 0.4297, auc: 0.4127, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:22.076716, step: 116, loss: 0.694159984588623, acc: 0.5312, auc: 0.3735, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:22.207930, step: 117, loss: 0.6935352087020874, acc: 0.4688, auc: 0.4662, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:22.346200, step: 118, loss: 0.6937286257743835, acc: 0.4531, auc: 0.3936, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:22.476446, step: 119, loss: 0.6928616166114807, acc: 0.4453, auc: 0.4539, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:22.592044, step: 120, loss: 0.6934188604354858, acc: 0.4844, auc: 0.4174, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:22.709868, step: 121, loss: 0.6936219930648804, acc: 0.5156, auc: 0.4844, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:22.846983, step: 122, loss: 0.693105936050415, acc: 0.5312, auc: 0.5725, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:22.971816, step: 123, loss: 0.6934030652046204, acc: 0.5547, auc: 0.5145, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:23.087819, step: 124, loss: 0.6929507851600647, acc: 0.5, auc: 0.5342, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:23.201871, step: 125, loss: 0.6933298110961914, acc: 0.4766, auc: 0.5018, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:23.317886, step: 126, loss: 0.6935015916824341, acc: 0.4609, auc: 0.4483, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:23.432493, step: 127, loss: 0.6936652660369873, acc: 0.5391, auc: 0.5055, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:23.556622, step: 128, loss: 0.69317626953125, acc: 0.5, auc: 0.5049, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:23.668981, step: 129, loss: 0.6932352185249329, acc: 0.4219, auc: 0.4937, precision: 0.2109, recall: 0.5\n",
      "2018-12-27T14:15:23.786642, step: 130, loss: 0.6933467388153076, acc: 0.5078, auc: 0.5087, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:23.911174, step: 131, loss: 0.6934524178504944, acc: 0.4766, auc: 0.4368, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:24.040673, step: 132, loss: 0.6943917274475098, acc: 0.5547, auc: 0.4959, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:24.155761, step: 133, loss: 0.6929842233657837, acc: 0.5781, auc: 0.5611, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:15:24.276386, step: 134, loss: 0.6931401491165161, acc: 0.5312, auc: 0.437, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:24.390468, step: 135, loss: 0.693817138671875, acc: 0.4766, auc: 0.5102, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:24.505276, step: 136, loss: 0.6938378810882568, acc: 0.4297, auc: 0.6, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:24.614823, step: 137, loss: 0.6941388845443726, acc: 0.5469, auc: 0.3712, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:24.724156, step: 138, loss: 0.6934755444526672, acc: 0.4844, auc: 0.4812, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:24.835309, step: 139, loss: 0.6922801733016968, acc: 0.5469, auc: 0.5961, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:24.946908, step: 140, loss: 0.6926455497741699, acc: 0.5078, auc: 0.5543, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:25.055093, step: 141, loss: 0.691817581653595, acc: 0.5469, auc: 0.5303, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:25.175023, step: 142, loss: 0.6963115930557251, acc: 0.4141, auc: 0.5384, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:15:25.294386, step: 143, loss: 0.6938981413841248, acc: 0.4453, auc: 0.4356, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:25.402289, step: 144, loss: 0.6931943893432617, acc: 0.5234, auc: 0.5192, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:25.499354, step: 145, loss: 0.6937508583068848, acc: 0.5156, auc: 0.4291, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:25.602518, step: 146, loss: 0.692472517490387, acc: 0.5547, auc: 0.5322, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:25.715588, step: 147, loss: 0.6933625340461731, acc: 0.5, auc: 0.4951, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:25.838110, step: 148, loss: 0.6922191977500916, acc: 0.5391, auc: 0.5058, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:25.949495, step: 149, loss: 0.6926678419113159, acc: 0.5156, auc: 0.5291, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:26.066468, step: 150, loss: 0.6946359872817993, acc: 0.4766, auc: 0.4627, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:26.171581, step: 151, loss: 0.6929663419723511, acc: 0.5391, auc: 0.4181, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:26.288258, step: 152, loss: 0.6921494603157043, acc: 0.5312, auc: 0.5137, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:26.391671, step: 153, loss: 0.6944403648376465, acc: 0.4844, auc: 0.4714, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:26.494254, step: 154, loss: 0.6936490535736084, acc: 0.5078, auc: 0.4405, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:26.597900, step: 155, loss: 0.6951709985733032, acc: 0.4375, auc: 0.463, precision: 0.2188, recall: 0.5\n",
      "start training model\n",
      "2018-12-27T14:15:26.880807, step: 156, loss: 0.693259596824646, acc: 0.4609, auc: 0.4829, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:26.986063, step: 157, loss: 0.6938767433166504, acc: 0.4766, auc: 0.39, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:27.104152, step: 158, loss: 0.6913876533508301, acc: 0.4062, auc: 0.441, precision: 0.2031, recall: 0.5\n",
      "2018-12-27T14:15:27.217159, step: 159, loss: 0.6915309429168701, acc: 0.4688, auc: 0.4912, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:27.322930, step: 160, loss: 0.6968251466751099, acc: 0.5312, auc: 0.487, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:27.423952, step: 161, loss: 0.6950582265853882, acc: 0.5391, auc: 0.4756, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:27.527947, step: 162, loss: 0.6926510334014893, acc: 0.4844, auc: 0.5371, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:27.632322, step: 163, loss: 0.6931991577148438, acc: 0.5391, auc: 0.5387, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:27.755830, step: 164, loss: 0.6937762498855591, acc: 0.4766, auc: 0.4744, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:27.873617, step: 165, loss: 0.6927575469017029, acc: 0.5156, auc: 0.5144, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:27.981041, step: 166, loss: 0.6925917863845825, acc: 0.5, auc: 0.5303, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:28.086695, step: 167, loss: 0.693403959274292, acc: 0.4297, auc: 0.5158, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:28.203249, step: 168, loss: 0.6905026435852051, acc: 0.4297, auc: 0.5215, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:28.299802, step: 169, loss: 0.7023521065711975, acc: 0.5938, auc: 0.4967, precision: 0.2969, recall: 0.5\n",
      "2018-12-27T14:15:28.420395, step: 170, loss: 0.690711259841919, acc: 0.5, auc: 0.6777, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:28.526303, step: 171, loss: 0.6913633346557617, acc: 0.5156, auc: 0.6036, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:28.629475, step: 172, loss: 0.6923218965530396, acc: 0.5234, auc: 0.5209, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:28.747083, step: 173, loss: 0.6917160749435425, acc: 0.5312, auc: 0.5221, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:28.860701, step: 174, loss: 0.690986156463623, acc: 0.5391, auc: 0.5119, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:28.963747, step: 175, loss: 0.688388466835022, acc: 0.5625, auc: 0.566, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:29.063068, step: 176, loss: 0.6961060762405396, acc: 0.4688, auc: 0.6093, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:29.166355, step: 177, loss: 0.6933658123016357, acc: 0.5, auc: 0.4966, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:29.272205, step: 178, loss: 0.6928810477256775, acc: 0.4844, auc: 0.5252, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:29.376800, step: 179, loss: 0.6928441524505615, acc: 0.5547, auc: 0.5172, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:29.466747, step: 180, loss: 0.6950756907463074, acc: 0.4531, auc: 0.548, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:29.555714, step: 181, loss: 0.692964494228363, acc: 0.4609, auc: 0.5139, precision: 0.2305, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:15:29.652119, step: 182, loss: 0.6909480094909668, acc: 0.4766, auc: 0.5738, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:29.753282, step: 183, loss: 0.6898432970046997, acc: 0.4688, auc: 0.5478, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:29.847933, step: 184, loss: 0.6969574093818665, acc: 0.5234, auc: 0.5481, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:29.952252, step: 185, loss: 0.6952588558197021, acc: 0.5078, auc: 0.402, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:30.063121, step: 186, loss: 0.6924149990081787, acc: 0.4766, auc: 0.541, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:30.171099, step: 187, loss: 0.6926394701004028, acc: 0.5234, auc: 0.5356, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:30.266523, step: 188, loss: 0.6923314332962036, acc: 0.5391, auc: 0.5107, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:30.370595, step: 189, loss: 0.6978334188461304, acc: 0.4453, auc: 0.4865, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:30.467894, step: 190, loss: 0.6922746896743774, acc: 0.4375, auc: 0.4978, precision: 0.2188, recall: 0.5\n",
      "2018-12-27T14:15:30.596510, step: 191, loss: 0.6922934055328369, acc: 0.4922, auc: 0.5404, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:30.689581, step: 192, loss: 0.6888045072555542, acc: 0.4531, auc: 0.6108, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:30.785677, step: 193, loss: 0.7005775570869446, acc: 0.5469, auc: 0.4813, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:30.895985, step: 194, loss: 0.6923549771308899, acc: 0.5156, auc: 0.5081, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:30.994121, step: 195, loss: 0.6907776594161987, acc: 0.5938, auc: 0.4441, precision: 0.2969, recall: 0.5\n",
      "2018-12-27T14:15:31.095550, step: 196, loss: 0.6891056299209595, acc: 0.5469, auc: 0.4798, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:31.211034, step: 197, loss: 0.6923519372940063, acc: 0.5, auc: 0.5918, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:31.332938, step: 198, loss: 0.6892284154891968, acc: 0.5234, auc: 0.5728, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:31.433636, step: 199, loss: 0.7001965045928955, acc: 0.5, auc: 0.4827, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:31.539928, step: 200, loss: 0.696215033531189, acc: 0.5391, auc: 0.449, precision: 0.2695, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:15:35.727339, step: 200, loss: 0.6934307860700708, acc: 0.49383421052631576, auc: 0.523934210526316, precision: 0.246921052631579, recall: 0.5\n",
      "2018-12-27T14:15:35.833700, step: 201, loss: 0.6944476962089539, acc: 0.4844, auc: 0.4611, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:35.964198, step: 202, loss: 0.6944295167922974, acc: 0.4531, auc: 0.4847, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:36.072674, step: 203, loss: 0.6945186853408813, acc: 0.5078, auc: 0.5001, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:36.199534, step: 204, loss: 0.6943842172622681, acc: 0.4766, auc: 0.414, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:36.315363, step: 205, loss: 0.6931541562080383, acc: 0.4766, auc: 0.4752, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:36.428318, step: 206, loss: 0.6926023960113525, acc: 0.4609, auc: 0.4129, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:36.538582, step: 207, loss: 0.688230037689209, acc: 0.4453, auc: 0.5154, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:36.638082, step: 208, loss: 0.7173852920532227, acc: 0.5312, auc: 0.4017, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:36.746566, step: 209, loss: 0.692791223526001, acc: 0.5625, auc: 0.4678, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:36.846456, step: 210, loss: 0.6938982605934143, acc: 0.5234, auc: 0.4287, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:36.964104, step: 211, loss: 0.6958150267601013, acc: 0.4844, auc: 0.4978, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:37.082142, step: 212, loss: 0.6904053688049316, acc: 0.625, auc: 0.4987, precision: 0.3125, recall: 0.5\n",
      "2018-12-27T14:15:37.179602, step: 213, loss: 0.6965039372444153, acc: 0.4922, auc: 0.5136, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:37.278371, step: 214, loss: 0.6935994625091553, acc: 0.4844, auc: 0.5469, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:37.371903, step: 215, loss: 0.6940621733665466, acc: 0.5156, auc: 0.4692, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:37.466969, step: 216, loss: 0.6947154402732849, acc: 0.4531, auc: 0.5365, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:37.569902, step: 217, loss: 0.6995221972465515, acc: 0.5391, auc: 0.4611, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:37.661490, step: 218, loss: 0.6917281150817871, acc: 0.5781, auc: 0.4775, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:15:37.747341, step: 219, loss: 0.7003241777420044, acc: 0.4141, auc: 0.5633, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:15:37.842505, step: 220, loss: 0.6934255361557007, acc: 0.5156, auc: 0.4543, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:37.932239, step: 221, loss: 0.6931270360946655, acc: 0.5312, auc: 0.425, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:38.041598, step: 222, loss: 0.6965080499649048, acc: 0.4219, auc: 0.491, precision: 0.2109, recall: 0.5\n",
      "2018-12-27T14:15:38.166392, step: 223, loss: 0.6943719387054443, acc: 0.5234, auc: 0.423, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:38.273963, step: 224, loss: 0.6921577453613281, acc: 0.5625, auc: 0.5119, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:38.373847, step: 225, loss: 0.6957001686096191, acc: 0.4531, auc: 0.4906, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:38.482782, step: 226, loss: 0.6930849552154541, acc: 0.5156, auc: 0.4836, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:38.594898, step: 227, loss: 0.6951675415039062, acc: 0.4141, auc: 0.4564, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:15:38.718830, step: 228, loss: 0.6911509037017822, acc: 0.4375, auc: 0.4583, precision: 0.2188, recall: 0.5\n",
      "2018-12-27T14:15:38.824482, step: 229, loss: 0.7161191701889038, acc: 0.5469, auc: 0.5709, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:38.915237, step: 230, loss: 0.6930245757102966, acc: 0.5703, auc: 0.5064, precision: 0.2852, recall: 0.5\n",
      "2018-12-27T14:15:39.009639, step: 231, loss: 0.693112850189209, acc: 0.4844, auc: 0.5992, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:39.109061, step: 232, loss: 0.6957414150238037, acc: 0.4297, auc: 0.5166, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:39.198543, step: 233, loss: 0.6949480175971985, acc: 0.4922, auc: 0.3465, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:39.296444, step: 234, loss: 0.6925140619277954, acc: 0.5156, auc: 0.574, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:39.387546, step: 235, loss: 0.6932975053787231, acc: 0.5078, auc: 0.505, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:39.481216, step: 236, loss: 0.6935114860534668, acc: 0.5, auc: 0.4573, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:39.593809, step: 237, loss: 0.6936826109886169, acc: 0.4688, auc: 0.4446, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:39.696981, step: 238, loss: 0.6937934160232544, acc: 0.5312, auc: 0.4961, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:39.812730, step: 239, loss: 0.6921231746673584, acc: 0.5469, auc: 0.5539, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:39.937098, step: 240, loss: 0.6932403445243835, acc: 0.4844, auc: 0.5872, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:40.054872, step: 241, loss: 0.6933512687683105, acc: 0.4922, auc: 0.5084, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:40.166138, step: 242, loss: 0.691936731338501, acc: 0.4297, auc: 0.5993, precision: 0.2148, recall: 0.5\n",
      "2018-12-27T14:15:40.275786, step: 243, loss: 0.7008764147758484, acc: 0.5938, auc: 0.538, precision: 0.2969, recall: 0.5\n",
      "2018-12-27T14:15:40.382606, step: 244, loss: 0.6932462453842163, acc: 0.5, auc: 0.5, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:40.503342, step: 245, loss: 0.6937077045440674, acc: 0.4688, auc: 0.5667, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:40.606728, step: 246, loss: 0.6928296089172363, acc: 0.4688, auc: 0.5517, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:40.718809, step: 247, loss: 0.6931637525558472, acc: 0.5234, auc: 0.5243, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:40.840673, step: 248, loss: 0.6941020488739014, acc: 0.4531, auc: 0.5177, precision: 0.2266, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:15:40.941624, step: 249, loss: 0.6915631294250488, acc: 0.4844, auc: 0.5767, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:41.053937, step: 250, loss: 0.6907268762588501, acc: 0.4844, auc: 0.5496, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:41.165045, step: 251, loss: 0.6918008327484131, acc: 0.4766, auc: 0.5168, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:41.284059, step: 252, loss: 0.6911479830741882, acc: 0.5625, auc: 0.6101, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:41.419983, step: 253, loss: 0.6882790327072144, acc: 0.5625, auc: 0.5129, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:41.535734, step: 254, loss: 0.6875301599502563, acc: 0.5234, auc: 0.6491, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:41.654952, step: 255, loss: 0.6947717666625977, acc: 0.5078, auc: 0.4987, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:41.775182, step: 256, loss: 0.6922111511230469, acc: 0.4688, auc: 0.5576, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:41.899115, step: 257, loss: 0.6909971833229065, acc: 0.5391, auc: 0.5404, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:42.014298, step: 258, loss: 0.6970340013504028, acc: 0.4922, auc: 0.5304, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:42.123276, step: 259, loss: 0.6914290189743042, acc: 0.5234, auc: 0.5344, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:42.243870, step: 260, loss: 0.6945099830627441, acc: 0.4609, auc: 0.522, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:42.343646, step: 261, loss: 0.6916347146034241, acc: 0.5391, auc: 0.5991, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:42.452604, step: 262, loss: 0.691234827041626, acc: 0.4531, auc: 0.5621, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:42.570150, step: 263, loss: 0.6907575726509094, acc: 0.4766, auc: 0.5373, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:42.711437, step: 264, loss: 0.6937804222106934, acc: 0.5312, auc: 0.6069, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:42.833659, step: 265, loss: 0.6922798156738281, acc: 0.5312, auc: 0.5113, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:42.968380, step: 266, loss: 0.6986411809921265, acc: 0.4453, auc: 0.6234, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:43.082309, step: 267, loss: 0.6927352547645569, acc: 0.5156, auc: 0.5652, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:43.199264, step: 268, loss: 0.6906061172485352, acc: 0.5703, auc: 0.5243, precision: 0.2852, recall: 0.5\n",
      "2018-12-27T14:15:43.325802, step: 269, loss: 0.708099365234375, acc: 0.4766, auc: 0.4162, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:43.431858, step: 270, loss: 0.6926766633987427, acc: 0.5312, auc: 0.5299, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:43.538023, step: 271, loss: 0.6934257745742798, acc: 0.4922, auc: 0.5165, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:43.650558, step: 272, loss: 0.6912143230438232, acc: 0.4922, auc: 0.556, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:43.779227, step: 273, loss: 0.692385733127594, acc: 0.5469, auc: 0.6224, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:43.898733, step: 274, loss: 0.6919373273849487, acc: 0.5078, auc: 0.5382, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:44.013368, step: 275, loss: 0.6893238425254822, acc: 0.5547, auc: 0.4678, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:44.118856, step: 276, loss: 0.6912710666656494, acc: 0.5234, auc: 0.5258, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:44.220061, step: 277, loss: 0.6936100721359253, acc: 0.4922, auc: 0.588, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:44.336566, step: 278, loss: 0.685302734375, acc: 0.5703, auc: 0.6386, precision: 0.2852, recall: 0.5\n",
      "2018-12-27T14:15:44.452613, step: 279, loss: 0.6943074464797974, acc: 0.5, auc: 0.5869, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:44.589153, step: 280, loss: 0.6846306920051575, acc: 0.4766, auc: 0.6699, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:44.723826, step: 281, loss: 0.6953518390655518, acc: 0.5312, auc: 0.602, precision: 0.5938, recall: 0.5245\n",
      "2018-12-27T14:15:44.843521, step: 282, loss: 0.6848992109298706, acc: 0.5234, auc: 0.6102, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:44.959990, step: 283, loss: 0.6861594319343567, acc: 0.5469, auc: 0.5347, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:45.068119, step: 284, loss: 0.7064591646194458, acc: 0.4453, auc: 0.5804, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:45.172228, step: 285, loss: 0.6893342733383179, acc: 0.5078, auc: 0.5961, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:45.286212, step: 286, loss: 0.6878243684768677, acc: 0.5234, auc: 0.6132, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:45.424026, step: 287, loss: 0.6788961887359619, acc: 0.4453, auc: 0.6921, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:45.544485, step: 288, loss: 0.7158997058868408, acc: 0.6016, auc: 0.5828, precision: 0.6277, recall: 0.5707\n",
      "2018-12-27T14:15:45.666925, step: 289, loss: 0.7079072594642639, acc: 0.4922, auc: 0.4479, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:45.783095, step: 290, loss: 0.6896131634712219, acc: 0.5234, auc: 0.538, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:45.903118, step: 291, loss: 0.6838335990905762, acc: 0.5391, auc: 0.7035, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:46.020314, step: 292, loss: 0.6938700675964355, acc: 0.5234, auc: 0.5219, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:46.138581, step: 293, loss: 0.6843876838684082, acc: 0.4844, auc: 0.6263, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:46.269461, step: 294, loss: 0.6886862516403198, acc: 0.5469, auc: 0.5701, precision: 0.2756, recall: 0.493\n",
      "2018-12-27T14:15:46.392028, step: 295, loss: 0.6826773285865784, acc: 0.4531, auc: 0.6966, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:46.524897, step: 296, loss: 0.6711276769638062, acc: 0.5078, auc: 0.6723, precision: 0.6724, recall: 0.5358\n",
      "2018-12-27T14:15:46.642828, step: 297, loss: 0.6958611011505127, acc: 0.4844, auc: 0.5818, precision: 0.4643, recall: 0.4844\n",
      "2018-12-27T14:15:46.773929, step: 298, loss: 0.6787659525871277, acc: 0.5, auc: 0.5973, precision: 0.748, recall: 0.5077\n",
      "2018-12-27T14:15:46.912167, step: 299, loss: 0.6830102205276489, acc: 0.4531, auc: 0.569, precision: 0.4762, recall: 0.4985\n",
      "2018-12-27T14:15:47.028489, step: 300, loss: 0.8070124387741089, acc: 0.5078, auc: 0.5034, precision: 0.4227, recall: 0.4929\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:15:51.596625, step: 300, loss: 0.6903023029628553, acc: 0.4934157894736843, auc: 0.5954473684210527, precision: 0.24670526315789476, recall: 0.5\n",
      "2018-12-27T14:15:51.705875, step: 301, loss: 0.6909112930297852, acc: 0.5, auc: 0.5679, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:51.816909, step: 302, loss: 0.6764072179794312, acc: 0.5625, auc: 0.7044, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:15:51.931310, step: 303, loss: 0.6918120384216309, acc: 0.4922, auc: 0.6286, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:52.046619, step: 304, loss: 0.6908060908317566, acc: 0.5391, auc: 0.5166, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:52.159666, step: 305, loss: 0.6917663812637329, acc: 0.4453, auc: 0.6543, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:52.278390, step: 306, loss: 0.6922144889831543, acc: 0.4844, auc: 0.4888, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:52.402440, step: 307, loss: 0.6820667386054993, acc: 0.5859, auc: 0.6775, precision: 0.293, recall: 0.5\n",
      "2018-12-27T14:15:52.526245, step: 308, loss: 0.7011003494262695, acc: 0.4922, auc: 0.6232, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:52.640759, step: 309, loss: 0.6813720464706421, acc: 0.3047, auc: 0.6883, precision: 0.1523, recall: 0.5\n",
      "2018-12-27T14:15:52.747073, step: 310, loss: 0.6882323026657104, acc: 0.4688, auc: 0.6422, precision: 0.2344, recall: 0.5\n",
      "start training model\n",
      "2018-12-27T14:15:53.001517, step: 311, loss: 0.6843260526657104, acc: 0.4922, auc: 0.7211, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:53.117823, step: 312, loss: 0.6894711256027222, acc: 0.5, auc: 0.5847, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:53.223617, step: 313, loss: 0.6738166809082031, acc: 0.3906, auc: 0.7, precision: 0.1953, recall: 0.5\n",
      "2018-12-27T14:15:53.336366, step: 314, loss: 0.6729379892349243, acc: 0.4609, auc: 0.658, precision: 0.5703, recall: 0.5207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:15:53.449483, step: 315, loss: 0.6847705841064453, acc: 0.5, auc: 0.6431, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:53.587932, step: 316, loss: 0.687771201133728, acc: 0.4922, auc: 0.6085, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:53.715059, step: 317, loss: 0.7087767124176025, acc: 0.5938, auc: 0.6714, precision: 0.5967, recall: 0.515\n",
      "2018-12-27T14:15:53.832097, step: 318, loss: 0.70097416639328, acc: 0.5, auc: 0.6309, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:53.951031, step: 319, loss: 0.6858265399932861, acc: 0.4844, auc: 0.6339, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:54.084355, step: 320, loss: 0.6791890859603882, acc: 0.4609, auc: 0.707, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:15:54.214012, step: 321, loss: 0.6849315762519836, acc: 0.4922, auc: 0.6242, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:54.347146, step: 322, loss: 0.6774753928184509, acc: 0.5234, auc: 0.646, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:54.482208, step: 323, loss: 0.6753089427947998, acc: 0.4531, auc: 0.7288, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:54.611849, step: 324, loss: 0.72623610496521, acc: 0.6328, auc: 0.6481, precision: 0.628, recall: 0.6019\n",
      "2018-12-27T14:15:54.731320, step: 325, loss: 0.7064955234527588, acc: 0.4688, auc: 0.5488, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:54.863837, step: 326, loss: 0.6749603748321533, acc: 0.5469, auc: 0.77, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:54.995375, step: 327, loss: 0.6750855445861816, acc: 0.5078, auc: 0.6908, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:55.122150, step: 328, loss: 0.6618125438690186, acc: 0.5, auc: 0.769, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:55.266717, step: 329, loss: 0.6598242521286011, acc: 0.5859, auc: 0.7396, precision: 0.293, recall: 0.5\n",
      "2018-12-27T14:15:55.394024, step: 330, loss: 0.812863826751709, acc: 0.5, auc: 0.7219, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:15:55.525255, step: 331, loss: 0.6930088996887207, acc: 0.4766, auc: 0.5221, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:55.658985, step: 332, loss: 0.6676571369171143, acc: 0.4219, auc: 0.7322, precision: 0.2109, recall: 0.5\n",
      "2018-12-27T14:15:55.799412, step: 333, loss: 0.6642175912857056, acc: 0.4766, auc: 0.6669, precision: 0.5901, recall: 0.5288\n",
      "2018-12-27T14:15:55.926718, step: 334, loss: 0.6669605374336243, acc: 0.5078, auc: 0.6549, precision: 0.6237, recall: 0.5456\n",
      "2018-12-27T14:15:56.049579, step: 335, loss: 0.6497455835342407, acc: 0.6016, auc: 0.727, precision: 0.6936, recall: 0.6339\n",
      "2018-12-27T14:15:56.190209, step: 336, loss: 0.6736077666282654, acc: 0.4766, auc: 0.6359, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:56.322793, step: 337, loss: 0.7357808947563171, acc: 0.5938, auc: 0.6652, precision: 0.6275, recall: 0.6069\n",
      "2018-12-27T14:15:56.455032, step: 338, loss: 0.7044661641120911, acc: 0.4844, auc: 0.5828, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:56.600925, step: 339, loss: 0.6841683387756348, acc: 0.5234, auc: 0.6075, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:15:56.740670, step: 340, loss: 0.6899366974830627, acc: 0.4844, auc: 0.6395, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:56.885546, step: 341, loss: 0.6900400519371033, acc: 0.4922, auc: 0.5919, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:57.030483, step: 342, loss: 0.6772605180740356, acc: 0.5469, auc: 0.6333, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:57.172372, step: 343, loss: 0.6744927167892456, acc: 0.5391, auc: 0.6637, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:15:57.299451, step: 344, loss: 0.6578171253204346, acc: 0.4766, auc: 0.7649, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:15:57.463648, step: 345, loss: 0.6618213653564453, acc: 0.6016, auc: 0.659, precision: 0.6552, recall: 0.627\n",
      "2018-12-27T14:15:57.605273, step: 346, loss: 0.6585900783538818, acc: 0.4922, auc: 0.6787, precision: 0.6544, recall: 0.5278\n",
      "2018-12-27T14:15:57.743425, step: 347, loss: 0.7797090411186218, acc: 0.5547, auc: 0.6465, precision: 0.6712, recall: 0.5833\n",
      "2018-12-27T14:15:57.871180, step: 348, loss: 0.7793948650360107, acc: 0.3984, auc: 0.6738, precision: 0.1992, recall: 0.5\n",
      "2018-12-27T14:15:58.011514, step: 349, loss: 0.6821709871292114, acc: 0.5312, auc: 0.6279, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:15:58.127190, step: 350, loss: 0.6769854426383972, acc: 0.4531, auc: 0.6653, precision: 0.2266, recall: 0.5\n",
      "2018-12-27T14:15:58.255628, step: 351, loss: 0.6699869632720947, acc: 0.5156, auc: 0.7436, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:15:58.380429, step: 352, loss: 0.6671084761619568, acc: 0.5078, auc: 0.6933, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:15:58.503919, step: 353, loss: 0.6651020050048828, acc: 0.4453, auc: 0.6664, precision: 0.2227, recall: 0.5\n",
      "2018-12-27T14:15:58.630666, step: 354, loss: 0.7057933211326599, acc: 0.6406, auc: 0.6592, precision: 0.6398, recall: 0.6398\n",
      "2018-12-27T14:15:58.760581, step: 355, loss: 0.6769407987594604, acc: 0.5547, auc: 0.6192, precision: 0.2773, recall: 0.5\n",
      "2018-12-27T14:15:58.891801, step: 356, loss: 0.6870625019073486, acc: 0.4688, auc: 0.6838, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:59.035853, step: 357, loss: 0.6641145944595337, acc: 0.4688, auc: 0.6998, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:15:59.174515, step: 358, loss: 0.6678290367126465, acc: 0.5859, auc: 0.6729, precision: 0.7792, recall: 0.5656\n",
      "2018-12-27T14:15:59.309379, step: 359, loss: 0.6714464426040649, acc: 0.5469, auc: 0.6163, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:15:59.449218, step: 360, loss: 0.6505875587463379, acc: 0.5938, auc: 0.7207, precision: 0.7077, recall: 0.5377\n",
      "2018-12-27T14:15:59.571727, step: 361, loss: 0.6910489797592163, acc: 0.4844, auc: 0.6219, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:15:59.698376, step: 362, loss: 0.6703761219978333, acc: 0.6562, auc: 0.7258, precision: 0.6605, recall: 0.653\n",
      "2018-12-27T14:15:59.825196, step: 363, loss: 0.7020959854125977, acc: 0.4922, auc: 0.5795, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:15:59.959137, step: 364, loss: 0.6728191375732422, acc: 0.5781, auc: 0.6276, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:16:00.086651, step: 365, loss: 0.6598281860351562, acc: 0.5, auc: 0.6865, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:16:00.208270, step: 366, loss: 0.6678332686424255, acc: 0.5703, auc: 0.6244, precision: 0.7782, recall: 0.5339\n",
      "2018-12-27T14:16:00.337685, step: 367, loss: 0.6777613162994385, acc: 0.4609, auc: 0.6669, precision: 0.2323, recall: 0.4917\n",
      "2018-12-27T14:16:00.472180, step: 368, loss: 0.651839017868042, acc: 0.7109, auc: 0.765, precision: 0.718, recall: 0.7034\n",
      "2018-12-27T14:16:00.606004, step: 369, loss: 0.6622957587242126, acc: 0.5, auc: 0.7014, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:16:00.732856, step: 370, loss: 0.6621477603912354, acc: 0.625, auc: 0.6957, precision: 0.6786, recall: 0.5794\n",
      "2018-12-27T14:16:00.860494, step: 371, loss: 0.6788821816444397, acc: 0.5156, auc: 0.6703, precision: 0.7559, recall: 0.5079\n",
      "2018-12-27T14:16:00.991813, step: 372, loss: 0.668728768825531, acc: 0.6797, auc: 0.6868, precision: 0.6833, recall: 0.6797\n",
      "2018-12-27T14:16:01.121124, step: 373, loss: 0.671026349067688, acc: 0.4922, auc: 0.7184, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:16:01.268262, step: 374, loss: 0.6648483872413635, acc: 0.6719, auc: 0.7494, precision: 0.6957, recall: 0.6454\n",
      "2018-12-27T14:16:01.406065, step: 375, loss: 0.7202391624450684, acc: 0.4688, auc: 0.6225, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:16:01.541315, step: 376, loss: 0.6672872304916382, acc: 0.5469, auc: 0.6772, precision: 0.589, recall: 0.5347\n",
      "2018-12-27T14:16:01.696802, step: 377, loss: 0.6608931422233582, acc: 0.6172, auc: 0.6893, precision: 0.7514, recall: 0.586\n",
      "2018-12-27T14:16:01.836645, step: 378, loss: 0.6835453510284424, acc: 0.4844, auc: 0.6144, precision: 0.7402, recall: 0.5075\n",
      "2018-12-27T14:16:01.978464, step: 379, loss: 0.6034339666366577, acc: 0.6797, auc: 0.7691, precision: 0.7156, recall: 0.7173\n",
      "2018-12-27T14:16:02.124782, step: 380, loss: 0.790949821472168, acc: 0.5156, auc: 0.7176, precision: 0.593, recall: 0.5597\n",
      "2018-12-27T14:16:02.244470, step: 381, loss: 0.6757869720458984, acc: 0.5469, auc: 0.6931, precision: 0.2734, recall: 0.5\n",
      "2018-12-27T14:16:02.379231, step: 382, loss: 0.6915221810340881, acc: 0.5, auc: 0.6052, precision: 0.25, recall: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:16:02.524281, step: 383, loss: 0.6842052340507507, acc: 0.5312, auc: 0.5868, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:16:02.659839, step: 384, loss: 0.6582115888595581, acc: 0.5703, auc: 0.723, precision: 0.2852, recall: 0.5\n",
      "2018-12-27T14:16:02.789703, step: 385, loss: 0.6728149652481079, acc: 0.5234, auc: 0.7113, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:16:02.925251, step: 386, loss: 0.6704679727554321, acc: 0.4141, auc: 0.6969, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:16:03.048664, step: 387, loss: 0.7823117971420288, acc: 0.5, auc: 0.6977, precision: 0.6162, recall: 0.5576\n",
      "2018-12-27T14:16:03.174810, step: 388, loss: 0.7008987665176392, acc: 0.5391, auc: 0.5161, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:16:03.310722, step: 389, loss: 0.6710542440414429, acc: 0.5156, auc: 0.6933, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:16:03.449654, step: 390, loss: 0.67512047290802, acc: 0.4609, auc: 0.7463, precision: 0.2305, recall: 0.5\n",
      "2018-12-27T14:16:03.589149, step: 391, loss: 0.6789418458938599, acc: 0.5, auc: 0.6242, precision: 0.748, recall: 0.5077\n",
      "2018-12-27T14:16:03.719259, step: 392, loss: 0.648391604423523, acc: 0.4922, auc: 0.7598, precision: 0.7441, recall: 0.5076\n",
      "2018-12-27T14:16:03.855424, step: 393, loss: 0.6372647285461426, acc: 0.5703, auc: 0.7111, precision: 0.6573, recall: 0.5193\n",
      "2018-12-27T14:16:03.995044, step: 394, loss: 0.6635563969612122, acc: 0.5859, auc: 0.6616, precision: 0.293, recall: 0.5\n",
      "2018-12-27T14:16:04.123627, step: 395, loss: 0.7091314792633057, acc: 0.5859, auc: 0.642, precision: 0.5877, recall: 0.5876\n",
      "2018-12-27T14:16:04.252171, step: 396, loss: 0.6547432541847229, acc: 0.5312, auc: 0.7051, precision: 0.2656, recall: 0.5\n",
      "2018-12-27T14:16:04.392599, step: 397, loss: 0.6485092639923096, acc: 0.5391, auc: 0.698, precision: 0.7677, recall: 0.5083\n",
      "2018-12-27T14:16:04.537806, step: 398, loss: 0.653710126876831, acc: 0.5625, auc: 0.6565, precision: 0.5317, recall: 0.502\n",
      "2018-12-27T14:16:04.685470, step: 399, loss: 0.6428371667861938, acc: 0.5781, auc: 0.6627, precision: 0.6613, recall: 0.5198\n",
      "2018-12-27T14:16:04.820242, step: 400, loss: 0.6644092798233032, acc: 0.4766, auc: 0.7037, precision: 0.732, recall: 0.5214\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:16:09.898135, step: 400, loss: 0.7096755332068393, acc: 0.6009342105263156, auc: 0.6538578947368423, precision: 0.6120447368421053, recall: 0.5993947368421051\n",
      "2018-12-27T14:16:10.015269, step: 401, loss: 0.651447057723999, acc: 0.6953, auc: 0.7562, precision: 0.6966, recall: 0.6724\n",
      "2018-12-27T14:16:10.166521, step: 402, loss: 0.6498083472251892, acc: 0.6406, auc: 0.7551, precision: 0.7161, recall: 0.6359\n",
      "2018-12-27T14:16:10.303559, step: 403, loss: 0.6608743071556091, acc: 0.4922, auc: 0.6812, precision: 0.6169, recall: 0.5142\n",
      "2018-12-27T14:16:10.437084, step: 404, loss: 0.7839621305465698, acc: 0.6016, auc: 0.6549, precision: 0.6693, recall: 0.654\n",
      "2018-12-27T14:16:10.573283, step: 405, loss: 0.7402302026748657, acc: 0.4922, auc: 0.7104, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:16:10.715265, step: 406, loss: 0.6827366948127747, acc: 0.4688, auc: 0.6007, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:16:10.858423, step: 407, loss: 0.6648628115653992, acc: 0.4766, auc: 0.745, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:16:10.990678, step: 408, loss: 0.6695265769958496, acc: 0.6016, auc: 0.756, precision: 0.6734, recall: 0.5216\n",
      "2018-12-27T14:16:11.126377, step: 409, loss: 0.6722493171691895, acc: 0.4531, auc: 0.6721, precision: 0.2283, recall: 0.4915\n",
      "2018-12-27T14:16:11.266484, step: 410, loss: 0.6551660299301147, acc: 0.4844, auc: 0.6973, precision: 0.7402, recall: 0.5075\n",
      "2018-12-27T14:16:11.397714, step: 411, loss: 0.6524794697761536, acc: 0.5078, auc: 0.7082, precision: 0.7418, recall: 0.5435\n",
      "2018-12-27T14:16:11.541408, step: 412, loss: 0.6500216126441956, acc: 0.5781, auc: 0.7143, precision: 0.6685, recall: 0.589\n",
      "2018-12-27T14:16:11.672698, step: 413, loss: 0.653394341468811, acc: 0.4688, auc: 0.7185, precision: 0.6, recall: 0.5238\n",
      "2018-12-27T14:16:11.810132, step: 414, loss: 0.6933079957962036, acc: 0.6172, auc: 0.726, precision: 0.6359, recall: 0.6199\n",
      "2018-12-27T14:16:11.958561, step: 415, loss: 0.604619026184082, acc: 0.5781, auc: 0.7728, precision: 0.2891, recall: 0.5\n",
      "2018-12-27T14:16:12.101015, step: 416, loss: 0.6824758052825928, acc: 0.4453, auc: 0.67, precision: 0.2244, recall: 0.4914\n",
      "2018-12-27T14:16:12.230914, step: 417, loss: 0.6956156492233276, acc: 0.625, auc: 0.7008, precision: 0.6309, recall: 0.6286\n",
      "2018-12-27T14:16:12.376867, step: 418, loss: 0.6434865593910217, acc: 0.5391, auc: 0.7296, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:16:12.528723, step: 419, loss: 0.6558101177215576, acc: 0.5391, auc: 0.6743, precision: 0.5198, recall: 0.5012\n",
      "2018-12-27T14:16:12.670952, step: 420, loss: 0.6558646559715271, acc: 0.5703, auc: 0.6883, precision: 0.6803, recall: 0.5874\n",
      "2018-12-27T14:16:12.814305, step: 421, loss: 0.6588429808616638, acc: 0.5547, auc: 0.7023, precision: 0.6839, recall: 0.5672\n",
      "2018-12-27T14:16:12.956891, step: 422, loss: 0.6311842203140259, acc: 0.5625, auc: 0.7032, precision: 0.6315, recall: 0.5777\n",
      "2018-12-27T14:16:13.086633, step: 423, loss: 0.6448363065719604, acc: 0.5625, auc: 0.6737, precision: 0.7195, recall: 0.569\n",
      "2018-12-27T14:16:13.229624, step: 424, loss: 0.6393280029296875, acc: 0.6172, auc: 0.7123, precision: 0.6398, recall: 0.6201\n",
      "2018-12-27T14:16:13.370685, step: 425, loss: 0.7083149552345276, acc: 0.5234, auc: 0.6959, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:16:13.477961, step: 426, loss: 0.7619072198867798, acc: 0.5156, auc: 0.7135, precision: 0.3871, recall: 0.4863\n",
      "2018-12-27T14:16:13.600271, step: 427, loss: 0.662417471408844, acc: 0.5391, auc: 0.6414, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:16:13.730452, step: 428, loss: 0.6385902166366577, acc: 0.6094, auc: 0.7169, precision: 0.3047, recall: 0.5\n",
      "2018-12-27T14:16:13.867157, step: 429, loss: 0.7127672433853149, acc: 0.4922, auc: 0.7131, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:16:13.997049, step: 430, loss: 0.6608974933624268, acc: 0.5703, auc: 0.7378, precision: 0.6826, recall: 0.5574\n",
      "2018-12-27T14:16:14.127551, step: 431, loss: 0.6461111903190613, acc: 0.5312, auc: 0.7644, precision: 0.7619, recall: 0.5161\n",
      "2018-12-27T14:16:14.257455, step: 432, loss: 0.634367823600769, acc: 0.5078, auc: 0.7539, precision: 0.6724, recall: 0.5358\n",
      "2018-12-27T14:16:14.385680, step: 433, loss: 0.6335754990577698, acc: 0.5312, auc: 0.737, precision: 0.6612, recall: 0.5508\n",
      "2018-12-27T14:16:14.530167, step: 434, loss: 0.6323360204696655, acc: 0.6094, auc: 0.7196, precision: 0.6743, recall: 0.6225\n",
      "2018-12-27T14:16:14.682315, step: 435, loss: 0.6442201137542725, acc: 0.5391, auc: 0.6721, precision: 0.6095, recall: 0.5603\n",
      "2018-12-27T14:16:14.812939, step: 436, loss: 0.6620521545410156, acc: 0.6328, auc: 0.6965, precision: 0.6333, recall: 0.6321\n",
      "2018-12-27T14:16:14.949072, step: 437, loss: 0.7383476495742798, acc: 0.5156, auc: 0.6447, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:16:15.078578, step: 438, loss: 0.6902930736541748, acc: 0.6406, auc: 0.6868, precision: 0.6392, recall: 0.6392\n",
      "2018-12-27T14:16:15.207213, step: 439, loss: 0.6453063488006592, acc: 0.5781, auc: 0.7344, precision: 0.784, recall: 0.5263\n",
      "2018-12-27T14:16:15.353971, step: 440, loss: 0.6658189296722412, acc: 0.4844, auc: 0.673, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:16:15.485864, step: 441, loss: 0.6423225402832031, acc: 0.5234, auc: 0.716, precision: 0.6666, recall: 0.5613\n",
      "2018-12-27T14:16:15.615622, step: 442, loss: 0.6483575701713562, acc: 0.6719, auc: 0.7187, precision: 0.6932, recall: 0.6409\n",
      "2018-12-27T14:16:15.749753, step: 443, loss: 0.6537817120552063, acc: 0.5469, auc: 0.7052, precision: 0.5238, recall: 0.5015\n",
      "2018-12-27T14:16:15.880403, step: 444, loss: 0.6402269601821899, acc: 0.5859, auc: 0.7107, precision: 0.6458, recall: 0.5859\n",
      "2018-12-27T14:16:16.021754, step: 445, loss: 0.6586012840270996, acc: 0.5703, auc: 0.665, precision: 0.6803, recall: 0.5874\n",
      "2018-12-27T14:16:16.153403, step: 446, loss: 0.596318244934082, acc: 0.6875, auc: 0.8096, precision: 0.6875, recall: 0.6892\n",
      "2018-12-27T14:16:16.302229, step: 447, loss: 0.6333818435668945, acc: 0.5234, auc: 0.7377, precision: 0.655, recall: 0.5803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:16:16.445769, step: 448, loss: 0.9231550097465515, acc: 0.5156, auc: 0.67, precision: 0.6341, recall: 0.5527\n",
      "2018-12-27T14:16:16.573518, step: 449, loss: 0.7292705774307251, acc: 0.4766, auc: 0.7228, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:16:16.715254, step: 450, loss: 0.6723434329032898, acc: 0.4453, auc: 0.6272, precision: 0.7205, recall: 0.5069\n",
      "2018-12-27T14:16:16.848075, step: 451, loss: 0.659316897392273, acc: 0.5312, auc: 0.6868, precision: 0.6368, recall: 0.55\n",
      "2018-12-27T14:16:16.981423, step: 452, loss: 0.662194013595581, acc: 0.5703, auc: 0.6523, precision: 0.7238, recall: 0.5703\n",
      "2018-12-27T14:16:17.101792, step: 453, loss: 0.6510580778121948, acc: 0.5469, auc: 0.7153, precision: 0.693, recall: 0.5399\n",
      "2018-12-27T14:16:17.235987, step: 454, loss: 0.6327630877494812, acc: 0.6562, auc: 0.7295, precision: 0.7583, recall: 0.5636\n",
      "2018-12-27T14:16:17.390873, step: 455, loss: 0.6756077408790588, acc: 0.5078, auc: 0.6767, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:16:17.532266, step: 456, loss: 0.6232550144195557, acc: 0.5938, auc: 0.762, precision: 0.7014, recall: 0.6195\n",
      "2018-12-27T14:16:17.664286, step: 457, loss: 0.6382077932357788, acc: 0.6484, auc: 0.7179, precision: 0.6709, recall: 0.6385\n",
      "2018-12-27T14:16:17.795755, step: 458, loss: 0.6437780261039734, acc: 0.5312, auc: 0.7289, precision: 0.7638, recall: 0.5082\n",
      "2018-12-27T14:16:17.925956, step: 459, loss: 0.6390807628631592, acc: 0.6562, auc: 0.7224, precision: 0.6711, recall: 0.651\n",
      "2018-12-27T14:16:18.053901, step: 460, loss: 0.625147819519043, acc: 0.5547, auc: 0.7429, precision: 0.6384, recall: 0.5606\n",
      "2018-12-27T14:16:18.187910, step: 461, loss: 0.6150241494178772, acc: 0.6406, auc: 0.7384, precision: 0.6468, recall: 0.6442\n",
      "2018-12-27T14:16:18.325457, step: 462, loss: 0.6683785319328308, acc: 0.5234, auc: 0.6879, precision: 0.5602, recall: 0.509\n",
      "2018-12-27T14:16:18.471953, step: 463, loss: 0.7012728452682495, acc: 0.6172, auc: 0.6507, precision: 0.6302, recall: 0.5983\n",
      "2018-12-27T14:16:18.619141, step: 464, loss: 0.6846521496772766, acc: 0.4531, auc: 0.7165, precision: 0.7222, recall: 0.5139\n",
      "2018-12-27T14:16:18.761930, step: 465, loss: 0.6350500583648682, acc: 0.6484, auc: 0.761, precision: 0.7022, recall: 0.6484\n",
      "start training model\n",
      "2018-12-27T14:16:19.071303, step: 466, loss: 0.6494489312171936, acc: 0.5859, auc: 0.6649, precision: 0.6197, recall: 0.5582\n",
      "2018-12-27T14:16:19.209263, step: 467, loss: 0.6149756908416748, acc: 0.6328, auc: 0.6966, precision: 0.815, recall: 0.5104\n",
      "2018-12-27T14:16:19.342486, step: 468, loss: 0.6122669577598572, acc: 0.5625, auc: 0.7848, precision: 0.776, recall: 0.5254\n",
      "2018-12-27T14:16:19.472909, step: 469, loss: 0.5857278108596802, acc: 0.6562, auc: 0.745, precision: 0.6588, recall: 0.6627\n",
      "2018-12-27T14:16:19.615587, step: 470, loss: 0.623680830001831, acc: 0.6875, auc: 0.748, precision: 0.6874, recall: 0.6874\n",
      "2018-12-27T14:16:19.759066, step: 471, loss: 0.76793372631073, acc: 0.4922, auc: 0.7072, precision: 0.5773, recall: 0.5071\n",
      "2018-12-27T14:16:19.908116, step: 472, loss: 0.6961979866027832, acc: 0.6172, auc: 0.6929, precision: 0.6428, recall: 0.6005\n",
      "2018-12-27T14:16:20.051753, step: 473, loss: 0.6495833396911621, acc: 0.5078, auc: 0.7527, precision: 0.5853, recall: 0.5078\n",
      "2018-12-27T14:16:20.196807, step: 474, loss: 0.6582642197608948, acc: 0.4844, auc: 0.6657, precision: 0.6768, recall: 0.5572\n",
      "2018-12-27T14:16:20.338119, step: 475, loss: 0.6629234552383423, acc: 0.6484, auc: 0.7455, precision: 0.6523, recall: 0.647\n",
      "2018-12-27T14:16:20.494305, step: 476, loss: 0.6258145570755005, acc: 0.5312, auc: 0.7851, precision: 0.62, recall: 0.5377\n",
      "2018-12-27T14:16:20.640639, step: 477, loss: 0.6464323997497559, acc: 0.6484, auc: 0.7141, precision: 0.6779, recall: 0.6285\n",
      "2018-12-27T14:16:20.785034, step: 478, loss: 0.6610419750213623, acc: 0.4453, auc: 0.7401, precision: 0.7137, recall: 0.5267\n",
      "2018-12-27T14:16:20.915999, step: 479, loss: 0.6562361121177673, acc: 0.7109, auc: 0.8005, precision: 0.7566, recall: 0.7109\n",
      "2018-12-27T14:16:21.044328, step: 480, loss: 0.6182883977890015, acc: 0.5703, auc: 0.746, precision: 0.7782, recall: 0.5339\n",
      "2018-12-27T14:16:21.175199, step: 481, loss: 0.6514958739280701, acc: 0.5703, auc: 0.6345, precision: 0.6002, recall: 0.5318\n",
      "2018-12-27T14:16:21.306878, step: 482, loss: 0.6445846557617188, acc: 0.5781, auc: 0.7191, precision: 0.6797, recall: 0.552\n",
      "2018-12-27T14:16:21.445209, step: 483, loss: 0.6026613712310791, acc: 0.625, auc: 0.7801, precision: 0.747, recall: 0.5659\n",
      "2018-12-27T14:16:21.587312, step: 484, loss: 0.628050684928894, acc: 0.5, auc: 0.8032, precision: 0.7398, recall: 0.5362\n",
      "2018-12-27T14:16:21.705438, step: 485, loss: 0.7255651950836182, acc: 0.6016, auc: 0.7937, precision: 0.6892, recall: 0.6117\n",
      "2018-12-27T14:16:21.828649, step: 486, loss: 0.6552962064743042, acc: 0.5, auc: 0.7859, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:16:21.969716, step: 487, loss: 0.6340377926826477, acc: 0.5859, auc: 0.739, precision: 0.7366, recall: 0.598\n",
      "2018-12-27T14:16:22.107715, step: 488, loss: 0.6411156058311462, acc: 0.5703, auc: 0.6927, precision: 0.6297, recall: 0.5376\n",
      "2018-12-27T14:16:22.238134, step: 489, loss: 0.640667200088501, acc: 0.5078, auc: 0.7603, precision: 0.7397, recall: 0.55\n",
      "2018-12-27T14:16:22.366287, step: 490, loss: 0.6815687417984009, acc: 0.5781, auc: 0.6627, precision: 0.573, recall: 0.5694\n",
      "2018-12-27T14:16:22.502486, step: 491, loss: 0.6010885834693909, acc: 0.4844, auc: 0.7813, precision: 0.7155, recall: 0.5769\n",
      "2018-12-27T14:16:22.629904, step: 492, loss: 0.6507052779197693, acc: 0.7109, auc: 0.8077, precision: 0.7095, recall: 0.7171\n",
      "2018-12-27T14:16:22.757171, step: 493, loss: 0.8074231743812561, acc: 0.4219, auc: 0.6802, precision: 0.2109, recall: 0.5\n",
      "2018-12-27T14:16:22.883333, step: 494, loss: 0.6452632546424866, acc: 0.7266, auc: 0.7793, precision: 0.7271, recall: 0.7266\n",
      "2018-12-27T14:16:23.018734, step: 495, loss: 0.6298255920410156, acc: 0.6172, auc: 0.7493, precision: 0.787, recall: 0.6048\n",
      "2018-12-27T14:16:23.154003, step: 496, loss: 0.6464575529098511, acc: 0.4688, auc: 0.7286, precision: 0.719, recall: 0.5467\n",
      "2018-12-27T14:16:23.290305, step: 497, loss: 0.6829937100410461, acc: 0.6719, auc: 0.7443, precision: 0.6793, recall: 0.6806\n",
      "2018-12-27T14:16:23.421740, step: 498, loss: 0.6295591592788696, acc: 0.5703, auc: 0.7445, precision: 0.7782, recall: 0.5339\n",
      "2018-12-27T14:16:23.552974, step: 499, loss: 0.6095981001853943, acc: 0.6406, auc: 0.7286, precision: 0.7211, recall: 0.5716\n",
      "2018-12-27T14:16:23.714846, step: 500, loss: 0.6466034054756165, acc: 0.4844, auc: 0.7864, precision: 0.2422, recall: 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:16:28.931363, step: 500, loss: 0.6544856084020514, acc: 0.6509052631578948, auc: 0.7057973684210526, precision: 0.6529447368421054, recall: 0.6518052631578948\n",
      "2018-12-27T14:16:29.061376, step: 501, loss: 0.6271203756332397, acc: 0.7266, auc: 0.7887, precision: 0.7258, recall: 0.724\n",
      "2018-12-27T14:16:29.191491, step: 502, loss: 0.6228786706924438, acc: 0.5469, auc: 0.7515, precision: 0.7085, recall: 0.5601\n",
      "2018-12-27T14:16:29.324105, step: 503, loss: 0.6294595003128052, acc: 0.7031, auc: 0.7443, precision: 0.706, recall: 0.6845\n",
      "2018-12-27T14:16:29.453794, step: 504, loss: 0.6512066125869751, acc: 0.5859, auc: 0.6793, precision: 0.7846, recall: 0.5431\n",
      "2018-12-27T14:16:29.629060, step: 505, loss: 0.6428346633911133, acc: 0.6016, auc: 0.7087, precision: 0.6587, recall: 0.5967\n",
      "2018-12-27T14:16:29.774254, step: 506, loss: 0.6083947420120239, acc: 0.625, auc: 0.7436, precision: 0.7074, recall: 0.6344\n",
      "2018-12-27T14:16:29.929555, step: 507, loss: 0.5964857935905457, acc: 0.6484, auc: 0.7678, precision: 0.7241, recall: 0.6612\n",
      "2018-12-27T14:16:30.072580, step: 508, loss: 0.6279783248901367, acc: 0.6406, auc: 0.6936, precision: 0.6549, recall: 0.6507\n",
      "2018-12-27T14:16:30.208303, step: 509, loss: 0.6073142290115356, acc: 0.625, auc: 0.7417, precision: 0.6591, recall: 0.6373\n",
      "2018-12-27T14:16:30.350514, step: 510, loss: 0.5956945419311523, acc: 0.7266, auc: 0.8032, precision: 0.7212, recall: 0.7177\n",
      "2018-12-27T14:16:30.477049, step: 511, loss: 1.0392587184906006, acc: 0.4141, auc: 0.7575, precision: 0.207, recall: 0.5\n",
      "2018-12-27T14:16:30.614069, step: 512, loss: 0.6467507481575012, acc: 0.6719, auc: 0.7423, precision: 0.6998, recall: 0.6539\n",
      "2018-12-27T14:16:30.751914, step: 513, loss: 0.6386415958404541, acc: 0.5, auc: 0.7168, precision: 0.7311, recall: 0.5616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:16:30.882651, step: 514, loss: 0.661662220954895, acc: 0.6328, auc: 0.7467, precision: 0.6328, recall: 0.6344\n",
      "2018-12-27T14:16:31.022420, step: 515, loss: 0.659207820892334, acc: 0.4766, auc: 0.7005, precision: 0.5693, recall: 0.5064\n",
      "2018-12-27T14:16:31.167909, step: 516, loss: 0.6556898951530457, acc: 0.6328, auc: 0.6914, precision: 0.6293, recall: 0.6154\n",
      "2018-12-27T14:16:31.319710, step: 517, loss: 0.6328533887863159, acc: 0.5547, auc: 0.7282, precision: 0.7042, recall: 0.5479\n",
      "2018-12-27T14:16:31.472951, step: 518, loss: 0.6208959221839905, acc: 0.6562, auc: 0.732, precision: 0.6987, recall: 0.6112\n",
      "2018-12-27T14:16:31.601705, step: 519, loss: 0.6282481551170349, acc: 0.5625, auc: 0.7228, precision: 0.6254, recall: 0.5363\n",
      "2018-12-27T14:16:31.744359, step: 520, loss: 0.6374441981315613, acc: 0.6094, auc: 0.7016, precision: 0.6691, recall: 0.6389\n",
      "2018-12-27T14:16:31.885117, step: 521, loss: 0.5910627841949463, acc: 0.7344, auc: 0.7842, precision: 0.7377, recall: 0.7367\n",
      "2018-12-27T14:16:32.016243, step: 522, loss: 0.6345099806785583, acc: 0.5703, auc: 0.7004, precision: 0.6231, recall: 0.565\n",
      "2018-12-27T14:16:32.145364, step: 523, loss: 0.5990609526634216, acc: 0.6797, auc: 0.7676, precision: 0.6819, recall: 0.6816\n",
      "2018-12-27T14:16:32.274307, step: 524, loss: 0.6312631368637085, acc: 0.5703, auc: 0.7255, precision: 0.6865, recall: 0.6075\n",
      "2018-12-27T14:16:32.411791, step: 525, loss: 0.8075453042984009, acc: 0.6094, auc: 0.6768, precision: 0.6543, recall: 0.6323\n",
      "2018-12-27T14:16:32.540070, step: 526, loss: 0.7302501797676086, acc: 0.4844, auc: 0.7329, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:16:32.665639, step: 527, loss: 0.6327601671218872, acc: 0.5234, auc: 0.7603, precision: 0.6806, recall: 0.5374\n",
      "2018-12-27T14:16:32.793574, step: 528, loss: 0.6444455981254578, acc: 0.6562, auc: 0.7067, precision: 0.6848, recall: 0.6496\n",
      "2018-12-27T14:16:32.921660, step: 529, loss: 0.6486955881118774, acc: 0.4531, auc: 0.752, precision: 0.6339, recall: 0.5245\n",
      "2018-12-27T14:16:33.055536, step: 530, loss: 0.6785115003585815, acc: 0.6797, auc: 0.7199, precision: 0.6964, recall: 0.6773\n",
      "2018-12-27T14:16:33.185054, step: 531, loss: 0.6879727244377136, acc: 0.4453, auc: 0.717, precision: 0.7205, recall: 0.5069\n",
      "2018-12-27T14:16:33.312333, step: 532, loss: 0.6223371624946594, acc: 0.5781, auc: 0.7585, precision: 0.7318, recall: 0.6075\n",
      "2018-12-27T14:16:33.444778, step: 533, loss: 0.6246258020401001, acc: 0.6016, auc: 0.7131, precision: 0.6222, recall: 0.6132\n",
      "2018-12-27T14:16:33.574708, step: 534, loss: 0.6270565390586853, acc: 0.6406, auc: 0.7104, precision: 0.6725, recall: 0.6371\n",
      "2018-12-27T14:16:33.711711, step: 535, loss: 0.6052838563919067, acc: 0.6562, auc: 0.7709, precision: 0.7083, recall: 0.6281\n",
      "2018-12-27T14:16:33.852556, step: 536, loss: 0.6445337533950806, acc: 0.5859, auc: 0.6508, precision: 0.5482, recall: 0.5194\n",
      "2018-12-27T14:16:33.972299, step: 537, loss: 0.66926109790802, acc: 0.4609, auc: 0.7502, precision: 0.7218, recall: 0.5274\n",
      "2018-12-27T14:16:34.098743, step: 538, loss: 0.7076572179794312, acc: 0.5938, auc: 0.7087, precision: 0.6159, recall: 0.5938\n",
      "2018-12-27T14:16:34.232952, step: 539, loss: 0.656836748123169, acc: 0.5625, auc: 0.6508, precision: 0.2812, recall: 0.5\n",
      "2018-12-27T14:16:34.367368, step: 540, loss: 0.6181458830833435, acc: 0.5859, auc: 0.7411, precision: 0.7828, recall: 0.5508\n",
      "2018-12-27T14:16:34.498911, step: 541, loss: 0.5989311337471008, acc: 0.6406, auc: 0.7818, precision: 0.7318, recall: 0.5863\n",
      "2018-12-27T14:16:34.649923, step: 542, loss: 0.5861383676528931, acc: 0.625, auc: 0.778, precision: 0.7508, recall: 0.5734\n",
      "2018-12-27T14:16:34.785064, step: 543, loss: 0.60518479347229, acc: 0.6641, auc: 0.7529, precision: 0.7329, recall: 0.6552\n",
      "2018-12-27T14:16:34.920357, step: 544, loss: 0.6256111860275269, acc: 0.7344, auc: 0.7448, precision: 0.7402, recall: 0.7232\n",
      "2018-12-27T14:16:35.064067, step: 545, loss: 0.762014627456665, acc: 0.5391, auc: 0.6817, precision: 0.2695, recall: 0.5\n",
      "2018-12-27T14:16:35.195832, step: 546, loss: 0.6088865995407104, acc: 0.7422, auc: 0.7919, precision: 0.7404, recall: 0.7347\n",
      "2018-12-27T14:16:35.325770, step: 547, loss: 0.6533384919166565, acc: 0.5156, auc: 0.6842, precision: 0.6161, recall: 0.5512\n",
      "2018-12-27T14:16:35.461704, step: 548, loss: 0.6401008367538452, acc: 0.7188, auc: 0.7698, precision: 0.717, recall: 0.717\n",
      "2018-12-27T14:16:35.606848, step: 549, loss: 0.6775287389755249, acc: 0.4844, auc: 0.6808, precision: 0.7317, recall: 0.5352\n",
      "2018-12-27T14:16:35.751496, step: 550, loss: 0.5992486476898193, acc: 0.6953, auc: 0.7744, precision: 0.7193, recall: 0.7001\n",
      "2018-12-27T14:16:35.894451, step: 551, loss: 0.6120141744613647, acc: 0.6016, auc: 0.7392, precision: 0.6525, recall: 0.6172\n",
      "2018-12-27T14:16:36.043747, step: 552, loss: 0.6670846343040466, acc: 0.6797, auc: 0.7399, precision: 0.675, recall: 0.6796\n",
      "2018-12-27T14:16:36.175236, step: 553, loss: 0.7087780237197876, acc: 0.4844, auc: 0.8221, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:16:36.337168, step: 554, loss: 0.6314776539802551, acc: 0.5469, auc: 0.7634, precision: 0.7583, recall: 0.5606\n",
      "2018-12-27T14:16:36.485300, step: 555, loss: 0.6632527112960815, acc: 0.6328, auc: 0.6473, precision: 0.6452, recall: 0.5982\n",
      "2018-12-27T14:16:36.622887, step: 556, loss: 0.6504799127578735, acc: 0.4922, auc: 0.7447, precision: 0.7336, recall: 0.5423\n",
      "2018-12-27T14:16:36.763058, step: 557, loss: 0.6163692474365234, acc: 0.6484, auc: 0.7302, precision: 0.6626, recall: 0.6634\n",
      "2018-12-27T14:16:36.907898, step: 558, loss: 0.6271358132362366, acc: 0.6797, auc: 0.7219, precision: 0.6871, recall: 0.6865\n",
      "2018-12-27T14:16:37.039929, step: 559, loss: 0.6289016008377075, acc: 0.5625, auc: 0.7181, precision: 0.6254, recall: 0.5363\n",
      "2018-12-27T14:16:37.184359, step: 560, loss: 0.5734758973121643, acc: 0.6797, auc: 0.8241, precision: 0.7342, recall: 0.6968\n",
      "2018-12-27T14:16:37.326781, step: 561, loss: 0.6278795003890991, acc: 0.6641, auc: 0.7145, precision: 0.6675, recall: 0.6672\n",
      "2018-12-27T14:16:37.492109, step: 562, loss: 0.5975908041000366, acc: 0.6094, auc: 0.7727, precision: 0.7934, recall: 0.5614\n",
      "2018-12-27T14:16:37.636670, step: 563, loss: 0.6005427241325378, acc: 0.6797, auc: 0.7901, precision: 0.6779, recall: 0.6791\n",
      "2018-12-27T14:16:37.767823, step: 564, loss: 0.7140333652496338, acc: 0.5469, auc: 0.7654, precision: 0.7717, recall: 0.5085\n",
      "2018-12-27T14:16:37.900142, step: 565, loss: 0.6292799711227417, acc: 0.7188, auc: 0.7321, precision: 0.738, recall: 0.698\n",
      "2018-12-27T14:16:38.029602, step: 566, loss: 0.6017196178436279, acc: 0.6172, auc: 0.7713, precision: 0.7543, recall: 0.6053\n",
      "2018-12-27T14:16:38.161541, step: 567, loss: 0.6138073801994324, acc: 0.5703, auc: 0.7299, precision: 0.6302, recall: 0.5988\n",
      "2018-12-27T14:16:38.295193, step: 568, loss: 0.6401402950286865, acc: 0.6406, auc: 0.6932, precision: 0.6403, recall: 0.6406\n",
      "2018-12-27T14:16:38.434435, step: 569, loss: 0.6160820126533508, acc: 0.6484, auc: 0.7087, precision: 0.8125, recall: 0.5755\n",
      "2018-12-27T14:16:38.563384, step: 570, loss: 0.6046079397201538, acc: 0.6328, auc: 0.7542, precision: 0.7631, recall: 0.6093\n",
      "2018-12-27T14:16:38.704312, step: 571, loss: 0.5682468414306641, acc: 0.7891, auc: 0.8255, precision: 0.7972, recall: 0.777\n",
      "2018-12-27T14:16:38.835567, step: 572, loss: 0.6840682029724121, acc: 0.5703, auc: 0.7153, precision: 0.2852, recall: 0.5\n",
      "2018-12-27T14:16:38.978479, step: 573, loss: 0.6134421229362488, acc: 0.6953, auc: 0.758, precision: 0.6948, recall: 0.695\n",
      "2018-12-27T14:16:39.129688, step: 574, loss: 0.6246038675308228, acc: 0.5703, auc: 0.7549, precision: 0.7054, recall: 0.5426\n",
      "2018-12-27T14:16:39.282110, step: 575, loss: 0.6410425305366516, acc: 0.6641, auc: 0.7048, precision: 0.6663, recall: 0.6654\n",
      "2018-12-27T14:16:39.429380, step: 576, loss: 0.6227997541427612, acc: 0.5312, auc: 0.7534, precision: 0.62, recall: 0.5377\n",
      "2018-12-27T14:16:39.578406, step: 577, loss: 0.6216930150985718, acc: 0.6797, auc: 0.7879, precision: 0.6863, recall: 0.6762\n",
      "2018-12-27T14:16:39.717694, step: 578, loss: 0.6503832936286926, acc: 0.5312, auc: 0.7358, precision: 0.5159, recall: 0.501\n",
      "2018-12-27T14:16:39.847372, step: 579, loss: 0.5818109512329102, acc: 0.6719, auc: 0.7923, precision: 0.7041, recall: 0.6652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:16:39.978529, step: 580, loss: 0.5946835279464722, acc: 0.5859, auc: 0.7639, precision: 0.6406, recall: 0.6061\n",
      "2018-12-27T14:16:40.118597, step: 581, loss: 0.5660707950592041, acc: 0.75, auc: 0.8135, precision: 0.75, recall: 0.7471\n",
      "2018-12-27T14:16:40.249780, step: 582, loss: 0.6754781007766724, acc: 0.5312, auc: 0.6855, precision: 0.6848, recall: 0.5382\n",
      "2018-12-27T14:16:40.393953, step: 583, loss: 0.6682962775230408, acc: 0.6562, auc: 0.746, precision: 0.679, recall: 0.6636\n",
      "2018-12-27T14:16:40.532491, step: 584, loss: 0.7022364735603333, acc: 0.5469, auc: 0.7114, precision: 0.2756, recall: 0.493\n",
      "2018-12-27T14:16:40.669994, step: 585, loss: 0.6070795059204102, acc: 0.5781, auc: 0.7977, precision: 0.7318, recall: 0.5904\n",
      "2018-12-27T14:16:40.818023, step: 586, loss: 0.5590149164199829, acc: 0.7578, auc: 0.8609, precision: 0.7668, recall: 0.7436\n",
      "2018-12-27T14:16:40.963731, step: 587, loss: 0.691648006439209, acc: 0.5, auc: 0.7032, precision: 0.744, recall: 0.5224\n",
      "2018-12-27T14:16:41.093820, step: 588, loss: 0.592719316482544, acc: 0.6875, auc: 0.7336, precision: 0.6803, recall: 0.6852\n",
      "2018-12-27T14:16:41.222048, step: 589, loss: 0.5755783915519714, acc: 0.7188, auc: 0.7954, precision: 0.7465, recall: 0.7305\n",
      "2018-12-27T14:16:41.364784, step: 590, loss: 0.6332964301109314, acc: 0.6641, auc: 0.7123, precision: 0.6724, recall: 0.6544\n",
      "2018-12-27T14:16:41.494082, step: 591, loss: 0.7093859910964966, acc: 0.4922, auc: 0.6219, precision: 0.6788, recall: 0.5526\n",
      "2018-12-27T14:16:41.628215, step: 592, loss: 0.7319422364234924, acc: 0.6641, auc: 0.731, precision: 0.708, recall: 0.6798\n",
      "2018-12-27T14:16:41.756600, step: 593, loss: 0.6671352982521057, acc: 0.4922, auc: 0.752, precision: 0.7441, recall: 0.5076\n",
      "2018-12-27T14:16:41.906744, step: 594, loss: 0.6632438898086548, acc: 0.4688, auc: 0.7035, precision: 0.6276, recall: 0.5194\n",
      "2018-12-27T14:16:42.047210, step: 595, loss: 0.6159199476242065, acc: 0.6719, auc: 0.7349, precision: 0.674, recall: 0.6766\n",
      "2018-12-27T14:16:42.174426, step: 596, loss: 0.6030611991882324, acc: 0.6406, auc: 0.8043, precision: 0.7659, recall: 0.5982\n",
      "2018-12-27T14:16:42.302648, step: 597, loss: 0.6227298974990845, acc: 0.5703, auc: 0.7106, precision: 0.6384, recall: 0.6006\n",
      "2018-12-27T14:16:42.445490, step: 598, loss: 0.7097146511077881, acc: 0.5859, auc: 0.6836, precision: 0.5963, recall: 0.5926\n",
      "2018-12-27T14:16:42.590482, step: 599, loss: 0.6833875179290771, acc: 0.4766, auc: 0.7871, precision: 0.2383, recall: 0.5\n",
      "2018-12-27T14:16:42.732164, step: 600, loss: 0.60805344581604, acc: 0.6406, auc: 0.7821, precision: 0.8067, recall: 0.5818\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:16:47.873177, step: 600, loss: 0.6245038132918509, acc: 0.569286842105263, auc: 0.7403736842105265, precision: 0.6962921052631579, recall: 0.5742763157894737\n",
      "2018-12-27T14:16:48.017897, step: 601, loss: 0.585449755191803, acc: 0.6484, auc: 0.8103, precision: 0.7706, recall: 0.6484\n",
      "2018-12-27T14:16:48.167628, step: 602, loss: 0.606888473033905, acc: 0.7422, auc: 0.8016, precision: 0.7361, recall: 0.732\n",
      "2018-12-27T14:16:48.300054, step: 603, loss: 0.6829063892364502, acc: 0.4688, auc: 0.7983, precision: 0.4841, recall: 0.499\n",
      "2018-12-27T14:16:48.432018, step: 604, loss: 0.6049823760986328, acc: 0.6172, auc: 0.7441, precision: 0.6518, recall: 0.627\n",
      "2018-12-27T14:16:48.573246, step: 605, loss: 0.6390867233276367, acc: 0.7344, auc: 0.7072, precision: 0.7396, recall: 0.6887\n",
      "2018-12-27T14:16:48.710104, step: 606, loss: 0.64634108543396, acc: 0.4922, auc: 0.7999, precision: 0.7421, recall: 0.5149\n",
      "2018-12-27T14:16:48.851661, step: 607, loss: 0.6310527920722961, acc: 0.6016, auc: 0.7032, precision: 0.6174, recall: 0.6084\n",
      "2018-12-27T14:16:48.986202, step: 608, loss: 0.5859102606773376, acc: 0.6328, auc: 0.8027, precision: 0.698, recall: 0.6282\n",
      "2018-12-27T14:16:49.114987, step: 609, loss: 0.5800827741622925, acc: 0.7031, auc: 0.7848, precision: 0.7166, recall: 0.7098\n",
      "2018-12-27T14:16:49.246061, step: 610, loss: 0.5983854532241821, acc: 0.5625, auc: 0.7488, precision: 0.6346, recall: 0.5824\n",
      "2018-12-27T14:16:49.378756, step: 611, loss: 0.6341681480407715, acc: 0.7188, auc: 0.7761, precision: 0.7298, recall: 0.7204\n",
      "2018-12-27T14:16:49.511955, step: 612, loss: 0.789080798625946, acc: 0.4922, auc: 0.643, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:16:49.642705, step: 613, loss: 0.5977871417999268, acc: 0.6953, auc: 0.7587, precision: 0.6977, recall: 0.7015\n",
      "2018-12-27T14:16:49.774344, step: 614, loss: 0.6125082969665527, acc: 0.75, auc: 0.7928, precision: 0.7581, recall: 0.7293\n",
      "2018-12-27T14:16:49.929740, step: 615, loss: 0.6464947462081909, acc: 0.4531, auc: 0.8216, precision: 0.7154, recall: 0.5333\n",
      "2018-12-27T14:16:50.073612, step: 616, loss: 0.6976581811904907, acc: 0.6719, auc: 0.7585, precision: 0.6835, recall: 0.6868\n",
      "2018-12-27T14:16:50.214556, step: 617, loss: 0.6743244528770447, acc: 0.5156, auc: 0.7126, precision: 0.2578, recall: 0.5\n",
      "2018-12-27T14:16:50.360306, step: 618, loss: 0.6418921947479248, acc: 0.5547, auc: 0.7348, precision: 0.7702, recall: 0.5328\n",
      "2018-12-27T14:16:50.508179, step: 619, loss: 0.5765236616134644, acc: 0.6406, auc: 0.8379, precision: 0.73, recall: 0.6357\n",
      "2018-12-27T14:16:50.655408, step: 620, loss: 0.5722613334655762, acc: 0.7188, auc: 0.8144, precision: 0.7512, recall: 0.6946\n",
      "start training model\n",
      "2018-12-27T14:16:50.939739, step: 621, loss: 0.5996913313865662, acc: 0.625, auc: 0.7881, precision: 0.7913, recall: 0.6066\n",
      "2018-12-27T14:16:51.073746, step: 622, loss: 0.5652960538864136, acc: 0.7188, auc: 0.7882, precision: 0.7143, recall: 0.7143\n",
      "2018-12-27T14:16:51.213575, step: 623, loss: 0.512315571308136, acc: 0.75, auc: 0.8757, precision: 0.8532, recall: 0.6863\n",
      "2018-12-27T14:16:51.363375, step: 624, loss: 0.5191524028778076, acc: 0.7031, auc: 0.8493, precision: 0.7602, recall: 0.6915\n",
      "2018-12-27T14:16:51.516246, step: 625, loss: 0.8251515626907349, acc: 0.5625, auc: 0.6836, precision: 0.5871, recall: 0.5625\n",
      "2018-12-27T14:16:51.657048, step: 626, loss: 0.774804413318634, acc: 0.4844, auc: 0.781, precision: 0.2422, recall: 0.5\n",
      "2018-12-27T14:16:51.793583, step: 627, loss: 0.6068288087844849, acc: 0.7031, auc: 0.7695, precision: 0.6995, recall: 0.6995\n",
      "2018-12-27T14:16:51.921631, step: 628, loss: 0.6241558790206909, acc: 0.5859, auc: 0.7404, precision: 0.7364, recall: 0.6093\n",
      "2018-12-27T14:16:52.049841, step: 629, loss: 0.5941742658615112, acc: 0.6953, auc: 0.7858, precision: 0.7656, recall: 0.6993\n",
      "2018-12-27T14:16:52.184241, step: 630, loss: 0.5473566055297852, acc: 0.7344, auc: 0.8419, precision: 0.7457, recall: 0.7389\n",
      "2018-12-27T14:16:52.325217, step: 631, loss: 0.573462963104248, acc: 0.7109, auc: 0.798, precision: 0.775, recall: 0.7147\n",
      "2018-12-27T14:16:52.453483, step: 632, loss: 0.5342013835906982, acc: 0.7266, auc: 0.837, precision: 0.7393, recall: 0.7226\n",
      "2018-12-27T14:16:52.584706, step: 633, loss: 0.6094474792480469, acc: 0.5156, auc: 0.7695, precision: 0.6341, recall: 0.5527\n",
      "2018-12-27T14:16:52.712138, step: 634, loss: 0.7588704824447632, acc: 0.625, auc: 0.8395, precision: 0.7146, recall: 0.6088\n",
      "2018-12-27T14:16:52.851267, step: 635, loss: 0.7409399151802063, acc: 0.5078, auc: 0.7563, precision: 0.2539, recall: 0.5\n",
      "2018-12-27T14:16:53.010881, step: 636, loss: 0.6131799817085266, acc: 0.5234, auc: 0.7932, precision: 0.6708, recall: 0.5305\n",
      "2018-12-27T14:16:53.142913, step: 637, loss: 0.5995034575462341, acc: 0.6016, auc: 0.7696, precision: 0.7454, recall: 0.6131\n",
      "2018-12-27T14:16:53.279010, step: 638, loss: 0.5983413457870483, acc: 0.6406, auc: 0.7717, precision: 0.6771, recall: 0.6329\n",
      "2018-12-27T14:16:53.406162, step: 639, loss: 0.5732003450393677, acc: 0.6172, auc: 0.8018, precision: 0.683, recall: 0.6446\n",
      "2018-12-27T14:16:53.543520, step: 640, loss: 0.5736477971076965, acc: 0.7656, auc: 0.8197, precision: 0.7753, recall: 0.7607\n",
      "2018-12-27T14:16:53.681095, step: 641, loss: 0.5857185125350952, acc: 0.6406, auc: 0.7842, precision: 0.7463, recall: 0.63\n",
      "2018-12-27T14:16:53.813635, step: 642, loss: 0.5037444829940796, acc: 0.7891, auc: 0.8755, precision: 0.8021, recall: 0.7819\n",
      "2018-12-27T14:16:53.956517, step: 643, loss: 0.6542769074440002, acc: 0.5469, auc: 0.744, precision: 0.6855, recall: 0.5773\n",
      "2018-12-27T14:16:54.099058, step: 644, loss: 0.7926759719848633, acc: 0.5938, auc: 0.775, precision: 0.6859, recall: 0.6137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:16:54.237954, step: 645, loss: 0.741730272769928, acc: 0.5156, auc: 0.7433, precision: 0.7559, recall: 0.5079\n",
      "2018-12-27T14:16:54.384358, step: 646, loss: 0.6115795969963074, acc: 0.6172, auc: 0.7414, precision: 0.6666, recall: 0.585\n",
      "2018-12-27T14:16:54.518456, step: 647, loss: 0.6252807378768921, acc: 0.5547, auc: 0.7405, precision: 0.6839, recall: 0.5672\n",
      "2018-12-27T14:16:54.645426, step: 648, loss: 0.594409704208374, acc: 0.7266, auc: 0.8132, precision: 0.7309, recall: 0.7093\n",
      "2018-12-27T14:16:54.774988, step: 649, loss: 0.5839013457298279, acc: 0.5547, auc: 0.8228, precision: 0.7168, recall: 0.6124\n",
      "2018-12-27T14:16:54.912194, step: 650, loss: 0.5892829298973083, acc: 0.6719, auc: 0.7542, precision: 0.6785, recall: 0.6809\n",
      "2018-12-27T14:16:55.040797, step: 651, loss: 0.5907092690467834, acc: 0.6875, auc: 0.8005, precision: 0.6939, recall: 0.686\n",
      "2018-12-27T14:16:55.179806, step: 652, loss: 0.5776315927505493, acc: 0.6016, auc: 0.7753, precision: 0.6587, recall: 0.5967\n",
      "2018-12-27T14:16:55.321690, step: 653, loss: 0.5556995868682861, acc: 0.7344, auc: 0.8254, precision: 0.7724, recall: 0.7372\n",
      "2018-12-27T14:16:55.472368, step: 654, loss: 0.5821253061294556, acc: 0.7109, auc: 0.7592, precision: 0.7564, recall: 0.7142\n",
      "2018-12-27T14:16:55.608060, step: 655, loss: 0.5652675628662109, acc: 0.6328, auc: 0.7845, precision: 0.6719, recall: 0.651\n",
      "2018-12-27T14:16:55.748997, step: 656, loss: 0.6166208982467651, acc: 0.7109, auc: 0.7652, precision: 0.711, recall: 0.7099\n",
      "2018-12-27T14:16:55.877917, step: 657, loss: 0.6125060319900513, acc: 0.5312, auc: 0.8492, precision: 0.6371, recall: 0.5166\n",
      "2018-12-27T14:16:56.022880, step: 658, loss: 0.6461606025695801, acc: 0.6719, auc: 0.7105, precision: 0.6712, recall: 0.6712\n",
      "2018-12-27T14:16:56.163932, step: 659, loss: 0.5843527913093567, acc: 0.6484, auc: 0.7979, precision: 0.7936, recall: 0.6484\n",
      "2018-12-27T14:16:56.305743, step: 660, loss: 0.5485321879386902, acc: 0.7578, auc: 0.8437, precision: 0.7578, recall: 0.7576\n",
      "2018-12-27T14:16:56.449746, step: 661, loss: 0.5849359631538391, acc: 0.6484, auc: 0.7479, precision: 0.7178, recall: 0.5654\n",
      "2018-12-27T14:16:56.594531, step: 662, loss: 0.5993257761001587, acc: 0.6016, auc: 0.7608, precision: 0.6721, recall: 0.6434\n",
      "2018-12-27T14:16:56.728292, step: 663, loss: 0.6283373236656189, acc: 0.6641, auc: 0.7698, precision: 0.6654, recall: 0.6663\n",
      "2018-12-27T14:16:56.868034, step: 664, loss: 0.5980280041694641, acc: 0.5703, auc: 0.8268, precision: 0.7817, recall: 0.5175\n",
      "2018-12-27T14:16:56.996186, step: 665, loss: 0.5676335096359253, acc: 0.7422, auc: 0.8312, precision: 0.7549, recall: 0.7314\n",
      "2018-12-27T14:16:57.124294, step: 666, loss: 0.5488719344139099, acc: 0.6016, auc: 0.874, precision: 0.7857, recall: 0.575\n",
      "2018-12-27T14:16:57.263504, step: 667, loss: 0.5857229232788086, acc: 0.6797, auc: 0.789, precision: 0.6792, recall: 0.6799\n",
      "2018-12-27T14:16:57.406602, step: 668, loss: 0.5744505524635315, acc: 0.5859, auc: 0.8466, precision: 0.7792, recall: 0.5656\n",
      "2018-12-27T14:16:57.548079, step: 669, loss: 0.6185020208358765, acc: 0.7031, auc: 0.7846, precision: 0.7059, recall: 0.7059\n",
      "2018-12-27T14:16:57.676870, step: 670, loss: 0.6356520652770996, acc: 0.5469, auc: 0.792, precision: 0.7623, recall: 0.5469\n",
      "2018-12-27T14:16:57.808057, step: 671, loss: 0.5650851130485535, acc: 0.7188, auc: 0.8069, precision: 0.7188, recall: 0.7196\n",
      "2018-12-27T14:16:57.935494, step: 672, loss: 0.5422250032424927, acc: 0.6953, auc: 0.8375, precision: 0.7354, recall: 0.6809\n",
      "2018-12-27T14:16:58.066195, step: 673, loss: 0.6164126396179199, acc: 0.5859, auc: 0.7493, precision: 0.6821, recall: 0.6155\n",
      "2018-12-27T14:16:58.194361, step: 674, loss: 0.7067363262176514, acc: 0.6484, auc: 0.8187, precision: 0.7364, recall: 0.662\n",
      "2018-12-27T14:16:58.339738, step: 675, loss: 0.5863525867462158, acc: 0.5859, auc: 0.8701, precision: 0.7913, recall: 0.5093\n",
      "2018-12-27T14:16:58.476835, step: 676, loss: 0.578447699546814, acc: 0.7109, auc: 0.7819, precision: 0.758, recall: 0.662\n",
      "2018-12-27T14:16:58.606831, step: 677, loss: 0.6078405380249023, acc: 0.5703, auc: 0.7885, precision: 0.7764, recall: 0.5417\n",
      "2018-12-27T14:16:58.735117, step: 678, loss: 0.6439950466156006, acc: 0.6641, auc: 0.729, precision: 0.6542, recall: 0.6494\n",
      "2018-12-27T14:16:58.877368, step: 679, loss: 0.6510018110275269, acc: 0.5156, auc: 0.7761, precision: 0.754, recall: 0.5156\n",
      "2018-12-27T14:16:59.016709, step: 680, loss: 0.6177108287811279, acc: 0.6641, auc: 0.7361, precision: 0.6733, recall: 0.6418\n",
      "2018-12-27T14:16:59.148702, step: 681, loss: 0.5998717546463013, acc: 0.6172, auc: 0.7717, precision: 0.7832, recall: 0.6172\n",
      "2018-12-27T14:16:59.283451, step: 682, loss: 0.5269885063171387, acc: 0.7656, auc: 0.8429, precision: 0.7743, recall: 0.7695\n",
      "2018-12-27T14:16:59.418597, step: 683, loss: 0.5995166897773743, acc: 0.6484, auc: 0.7465, precision: 0.6588, recall: 0.6463\n",
      "2018-12-27T14:16:59.541759, step: 684, loss: 0.5203409790992737, acc: 0.7109, auc: 0.8693, precision: 0.7893, recall: 0.6982\n",
      "2018-12-27T14:16:59.681358, step: 685, loss: 0.5887433290481567, acc: 0.7266, auc: 0.7793, precision: 0.7266, recall: 0.7266\n",
      "2018-12-27T14:16:59.830360, step: 686, loss: 0.5650931596755981, acc: 0.6094, auc: 0.8267, precision: 0.75, recall: 0.6094\n",
      "2018-12-27T14:16:59.964799, step: 687, loss: 0.6335088014602661, acc: 0.6875, auc: 0.8139, precision: 0.7121, recall: 0.6847\n",
      "2018-12-27T14:17:00.115464, step: 688, loss: 0.6547341346740723, acc: 0.5391, auc: 0.811, precision: 0.7677, recall: 0.5083\n",
      "2018-12-27T14:17:00.264296, step: 689, loss: 0.5883302688598633, acc: 0.6719, auc: 0.7667, precision: 0.6806, recall: 0.6793\n",
      "2018-12-27T14:17:00.412722, step: 690, loss: 0.5993716716766357, acc: 0.625, auc: 0.7613, precision: 0.6591, recall: 0.6373\n",
      "2018-12-27T14:17:00.559028, step: 691, loss: 0.5325036644935608, acc: 0.7266, auc: 0.8553, precision: 0.7721, recall: 0.7415\n",
      "2018-12-27T14:17:00.701054, step: 692, loss: 0.5699179172515869, acc: 0.7266, auc: 0.7828, precision: 0.7345, recall: 0.7337\n",
      "2018-12-27T14:17:00.841061, step: 693, loss: 0.5571722984313965, acc: 0.6641, auc: 0.7907, precision: 0.7056, recall: 0.6604\n",
      "2018-12-27T14:17:00.981595, step: 694, loss: 0.5178771615028381, acc: 0.75, auc: 0.838, precision: 0.7584, recall: 0.7539\n",
      "2018-12-27T14:17:01.129781, step: 695, loss: 0.5302723050117493, acc: 0.7344, auc: 0.8466, precision: 0.8171, recall: 0.7176\n",
      "2018-12-27T14:17:01.260058, step: 696, loss: 0.53480064868927, acc: 0.7266, auc: 0.8091, precision: 0.7153, recall: 0.7242\n",
      "2018-12-27T14:17:01.396032, step: 697, loss: 0.5529869794845581, acc: 0.6719, auc: 0.7935, precision: 0.7395, recall: 0.6719\n",
      "2018-12-27T14:17:01.533467, step: 698, loss: 0.6453223824501038, acc: 0.7188, auc: 0.8059, precision: 0.7544, recall: 0.7216\n",
      "2018-12-27T14:17:01.662009, step: 699, loss: 0.9299578666687012, acc: 0.4922, auc: 0.8701, precision: 0.2461, recall: 0.5\n",
      "2018-12-27T14:17:01.820339, step: 700, loss: 0.640539288520813, acc: 0.6719, auc: 0.7487, precision: 0.6679, recall: 0.6679\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:17:07.001118, step: 700, loss: 0.66609525209979, acc: 0.5076078947368422, auc: 0.7507500000000001, precision: 0.6364657894736842, recall: 0.5132763157894736\n",
      "2018-12-27T14:17:07.125468, step: 701, loss: 0.6602706909179688, acc: 0.5078, auc: 0.7776, precision: 0.748, recall: 0.5227\n",
      "2018-12-27T14:17:07.263075, step: 702, loss: 0.5531102418899536, acc: 0.6875, auc: 0.8264, precision: 0.7319, recall: 0.6875\n",
      "2018-12-27T14:17:07.399657, step: 703, loss: 0.6024518609046936, acc: 0.7109, auc: 0.764, precision: 0.7326, recall: 0.6929\n",
      "2018-12-27T14:17:07.528065, step: 704, loss: 0.5470705032348633, acc: 0.7031, auc: 0.8155, precision: 0.8021, recall: 0.6792\n",
      "2018-12-27T14:17:07.655715, step: 705, loss: 0.5157588720321655, acc: 0.7812, auc: 0.8642, precision: 0.8068, recall: 0.7689\n",
      "2018-12-27T14:17:07.786756, step: 706, loss: 0.5924006104469299, acc: 0.5703, auc: 0.8128, precision: 0.6394, recall: 0.5851\n",
      "2018-12-27T14:17:07.926138, step: 707, loss: 0.6932597160339355, acc: 0.6328, auc: 0.7809, precision: 0.6711, recall: 0.6456\n",
      "2018-12-27T14:17:08.059267, step: 708, loss: 0.7147026062011719, acc: 0.4766, auc: 0.8064, precision: 0.7341, recall: 0.5145\n",
      "2018-12-27T14:17:08.190133, step: 709, loss: 0.5817463397979736, acc: 0.6562, auc: 0.7971, precision: 0.6794, recall: 0.6382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:17:08.319782, step: 710, loss: 0.5798598527908325, acc: 0.6797, auc: 0.7632, precision: 0.725, recall: 0.6179\n",
      "2018-12-27T14:17:08.448201, step: 711, loss: 0.5169565677642822, acc: 0.6953, auc: 0.8755, precision: 0.776, recall: 0.7075\n",
      "2018-12-27T14:17:08.598926, step: 712, loss: 0.5845777988433838, acc: 0.7266, auc: 0.7851, precision: 0.7249, recall: 0.7243\n",
      "2018-12-27T14:17:08.726129, step: 713, loss: 0.5883897542953491, acc: 0.5625, auc: 0.8351, precision: 0.7, recall: 0.612\n",
      "2018-12-27T14:17:08.872630, step: 714, loss: 0.6207131743431091, acc: 0.6953, auc: 0.8159, precision: 0.7407, recall: 0.6665\n",
      "2018-12-27T14:17:09.008750, step: 715, loss: 0.5667927861213684, acc: 0.5859, auc: 0.8345, precision: 0.7735, recall: 0.5859\n",
      "2018-12-27T14:17:09.126797, step: 716, loss: 0.5198733806610107, acc: 0.7578, auc: 0.8644, precision: 0.8076, recall: 0.7729\n",
      "2018-12-27T14:17:09.246770, step: 717, loss: 0.5882861614227295, acc: 0.7188, auc: 0.7725, precision: 0.7323, recall: 0.7268\n",
      "2018-12-27T14:17:09.361303, step: 718, loss: 0.5781236290931702, acc: 0.6797, auc: 0.7644, precision: 0.6727, recall: 0.6422\n",
      "2018-12-27T14:17:09.507703, step: 719, loss: 0.5986806154251099, acc: 0.5859, auc: 0.8, precision: 0.7754, recall: 0.5794\n",
      "2018-12-27T14:17:09.656494, step: 720, loss: 0.5957796573638916, acc: 0.7344, auc: 0.762, precision: 0.7344, recall: 0.7346\n",
      "2018-12-27T14:17:09.798810, step: 721, loss: 0.5610383152961731, acc: 0.6641, auc: 0.8317, precision: 0.785, recall: 0.6972\n",
      "2018-12-27T14:17:09.942885, step: 722, loss: 0.5761992931365967, acc: 0.75, auc: 0.8051, precision: 0.7554, recall: 0.7341\n",
      "2018-12-27T14:17:10.083936, step: 723, loss: 0.5498687028884888, acc: 0.6406, auc: 0.8236, precision: 0.7276, recall: 0.6252\n",
      "2018-12-27T14:17:10.231025, step: 724, loss: 0.48652929067611694, acc: 0.7891, auc: 0.8708, precision: 0.7876, recall: 0.7908\n",
      "2018-12-27T14:17:10.378709, step: 725, loss: 0.5371666550636292, acc: 0.7188, auc: 0.8109, precision: 0.7544, recall: 0.7126\n",
      "2018-12-27T14:17:10.510834, step: 726, loss: 0.464647114276886, acc: 0.7422, auc: 0.8787, precision: 0.7669, recall: 0.7572\n",
      "2018-12-27T14:17:10.640221, step: 727, loss: 0.5404629111289978, acc: 0.75, auc: 0.8292, precision: 0.7504, recall: 0.748\n",
      "2018-12-27T14:17:10.766774, step: 728, loss: 0.7382782697677612, acc: 0.5312, auc: 0.7766, precision: 0.6848, recall: 0.5382\n",
      "2018-12-27T14:17:10.898493, step: 729, loss: 0.6607515811920166, acc: 0.7266, auc: 0.7576, precision: 0.7824, recall: 0.6726\n",
      "2018-12-27T14:17:11.022305, step: 730, loss: 0.7152590751647949, acc: 0.5, auc: 0.6768, precision: 0.25, recall: 0.5\n",
      "2018-12-27T14:17:11.160391, step: 731, loss: 0.5331799983978271, acc: 0.8047, auc: 0.8428, precision: 0.8062, recall: 0.8078\n",
      "2018-12-27T14:17:11.295732, step: 732, loss: 0.5431726574897766, acc: 0.7031, auc: 0.82, precision: 0.7083, recall: 0.7043\n",
      "2018-12-27T14:17:11.420024, step: 733, loss: 0.5478125810623169, acc: 0.6719, auc: 0.8093, precision: 0.7126, recall: 0.6719\n",
      "2018-12-27T14:17:11.566082, step: 734, loss: 0.5708810687065125, acc: 0.6953, auc: 0.7693, precision: 0.7146, recall: 0.7016\n",
      "2018-12-27T14:17:11.699054, step: 735, loss: 0.5286397337913513, acc: 0.7734, auc: 0.8077, precision: 0.7933, recall: 0.7498\n",
      "2018-12-27T14:17:11.827555, step: 736, loss: 0.5709741115570068, acc: 0.6172, auc: 0.8115, precision: 0.7141, recall: 0.5736\n",
      "2018-12-27T14:17:11.955672, step: 737, loss: 0.6232480406761169, acc: 0.7266, auc: 0.8276, precision: 0.7424, recall: 0.7353\n",
      "2018-12-27T14:17:12.082842, step: 738, loss: 0.6987873315811157, acc: 0.4688, auc: 0.8066, precision: 0.2344, recall: 0.5\n",
      "2018-12-27T14:17:12.208091, step: 739, loss: 0.5698878765106201, acc: 0.7266, auc: 0.8011, precision: 0.7173, recall: 0.6999\n",
      "2018-12-27T14:17:12.334687, step: 740, loss: 0.5877487063407898, acc: 0.6797, auc: 0.7862, precision: 0.7444, recall: 0.6911\n",
      "2018-12-27T14:17:12.471931, step: 741, loss: 0.5524591207504272, acc: 0.7031, auc: 0.8071, precision: 0.7167, recall: 0.7031\n",
      "2018-12-27T14:17:12.611948, step: 742, loss: 0.5746699571609497, acc: 0.6875, auc: 0.7825, precision: 0.7743, recall: 0.6875\n",
      "2018-12-27T14:17:12.749113, step: 743, loss: 0.5248639583587646, acc: 0.7969, auc: 0.8495, precision: 0.8006, recall: 0.7875\n",
      "2018-12-27T14:17:12.871091, step: 744, loss: 0.5623818039894104, acc: 0.6328, auc: 0.8148, precision: 0.8074, recall: 0.5566\n",
      "2018-12-27T14:17:13.000197, step: 745, loss: 0.5748273134231567, acc: 0.7422, auc: 0.8238, precision: 0.7524, recall: 0.7388\n",
      "2018-12-27T14:17:13.131822, step: 746, loss: 0.6361511945724487, acc: 0.5547, auc: 0.7779, precision: 0.6492, recall: 0.5182\n",
      "2018-12-27T14:17:13.286183, step: 747, loss: 0.5542721748352051, acc: 0.7422, auc: 0.8317, precision: 0.745, recall: 0.7413\n",
      "2018-12-27T14:17:13.419780, step: 748, loss: 0.5691686868667603, acc: 0.5703, auc: 0.8223, precision: 0.6775, recall: 0.5819\n",
      "2018-12-27T14:17:13.548475, step: 749, loss: 0.662968635559082, acc: 0.6797, auc: 0.7811, precision: 0.7083, recall: 0.6662\n",
      "2018-12-27T14:17:13.693600, step: 750, loss: 0.6324527859687805, acc: 0.5625, auc: 0.8059, precision: 0.7742, recall: 0.5333\n",
      "2018-12-27T14:17:13.820828, step: 751, loss: 0.553899347782135, acc: 0.75, auc: 0.8239, precision: 0.7591, recall: 0.7485\n",
      "2018-12-27T14:17:13.956070, step: 752, loss: 0.5171637535095215, acc: 0.6953, auc: 0.8506, precision: 0.7464, recall: 0.6489\n",
      "2018-12-27T14:17:14.093364, step: 753, loss: 0.5216995477676392, acc: 0.7188, auc: 0.862, precision: 0.7946, recall: 0.6974\n",
      "2018-12-27T14:17:14.220551, step: 754, loss: 0.5216159820556641, acc: 0.7422, auc: 0.8319, precision: 0.7716, recall: 0.7339\n",
      "2018-12-27T14:17:14.347251, step: 755, loss: 0.5185366272926331, acc: 0.7344, auc: 0.846, precision: 0.7883, recall: 0.741\n",
      "2018-12-27T14:17:14.485284, step: 756, loss: 0.6014490127563477, acc: 0.7422, auc: 0.8675, precision: 0.7705, recall: 0.7628\n",
      "2018-12-27T14:17:14.632061, step: 757, loss: 0.9137958288192749, acc: 0.5234, auc: 0.7815, precision: 0.2617, recall: 0.5\n",
      "2018-12-27T14:17:14.763406, step: 758, loss: 0.5602778792381287, acc: 0.7109, auc: 0.8057, precision: 0.7262, recall: 0.7196\n",
      "2018-12-27T14:17:14.894567, step: 759, loss: 0.5853270292282104, acc: 0.7344, auc: 0.8008, precision: 0.7357, recall: 0.7316\n",
      "2018-12-27T14:17:15.032850, step: 760, loss: 0.6212426424026489, acc: 0.5469, auc: 0.8076, precision: 0.7603, recall: 0.5538\n",
      "2018-12-27T14:17:15.158939, step: 761, loss: 0.5428394079208374, acc: 0.75, auc: 0.8359, precision: 0.7522, recall: 0.75\n",
      "2018-12-27T14:17:15.286356, step: 762, loss: 0.5206577777862549, acc: 0.7188, auc: 0.8297, precision: 0.7354, recall: 0.6751\n",
      "2018-12-27T14:17:15.429158, step: 763, loss: 0.4844454824924469, acc: 0.7109, auc: 0.8892, precision: 0.79, recall: 0.6889\n",
      "2018-12-27T14:17:15.558318, step: 764, loss: 0.5195925831794739, acc: 0.7266, auc: 0.8199, precision: 0.739, recall: 0.7328\n",
      "2018-12-27T14:17:15.692578, step: 765, loss: 0.4653717577457428, acc: 0.7891, auc: 0.8967, precision: 0.8059, recall: 0.7891\n",
      "2018-12-27T14:17:15.829536, step: 766, loss: 0.5293709635734558, acc: 0.7422, auc: 0.8073, precision: 0.7635, recall: 0.7226\n",
      "2018-12-27T14:17:15.957530, step: 767, loss: 0.5468763113021851, acc: 0.6484, auc: 0.8449, precision: 0.7272, recall: 0.6979\n",
      "2018-12-27T14:17:16.096082, step: 768, loss: 0.8923137187957764, acc: 0.5938, auc: 0.8101, precision: 0.6528, recall: 0.5774\n",
      "2018-12-27T14:17:16.238972, step: 769, loss: 0.7424004077911377, acc: 0.4688, auc: 0.7653, precision: 0.728, recall: 0.5211\n",
      "2018-12-27T14:17:16.379628, step: 770, loss: 0.5351899862289429, acc: 0.75, auc: 0.8461, precision: 0.7534, recall: 0.7524\n",
      "2018-12-27T14:17:16.511792, step: 771, loss: 0.5893087983131409, acc: 0.6875, auc: 0.7675, precision: 0.6906, recall: 0.6503\n",
      "2018-12-27T14:17:16.641152, step: 772, loss: 0.5620388984680176, acc: 0.6172, auc: 0.8306, precision: 0.7293, recall: 0.6599\n",
      "2018-12-27T14:17:16.778886, step: 773, loss: 0.6022701263427734, acc: 0.75, auc: 0.8028, precision: 0.7531, recall: 0.7556\n",
      "2018-12-27T14:17:16.921744, step: 774, loss: 0.5312907695770264, acc: 0.6172, auc: 0.8747, precision: 0.7888, recall: 0.5984\n",
      "2018-12-27T14:17:17.047048, step: 775, loss: 0.5268320441246033, acc: 0.7812, auc: 0.8408, precision: 0.7917, recall: 0.7797\n",
      "start training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:17:17.333435, step: 776, loss: 0.4991742968559265, acc: 0.6797, auc: 0.8847, precision: 0.7409, recall: 0.7073\n",
      "2018-12-27T14:17:17.466367, step: 777, loss: 0.46964946389198303, acc: 0.8359, auc: 0.9017, precision: 0.8382, recall: 0.8328\n",
      "2018-12-27T14:17:17.594352, step: 778, loss: 0.46580350399017334, acc: 0.75, auc: 0.8831, precision: 0.8145, recall: 0.7087\n",
      "2018-12-27T14:17:17.732384, step: 779, loss: 0.41327396035194397, acc: 0.7891, auc: 0.9215, precision: 0.8001, recall: 0.7934\n",
      "2018-12-27T14:17:17.867969, step: 780, loss: 0.4391658902168274, acc: 0.875, auc: 0.9233, precision: 0.8754, recall: 0.8735\n",
      "2018-12-27T14:17:17.993485, step: 781, loss: 0.6154745817184448, acc: 0.5781, auc: 0.8195, precision: 0.7088, recall: 0.6062\n",
      "2018-12-27T14:17:18.115174, step: 782, loss: 0.6804176568984985, acc: 0.6719, auc: 0.8703, precision: 0.766, recall: 0.6572\n",
      "2018-12-27T14:17:18.243352, step: 783, loss: 0.6695218682289124, acc: 0.5312, auc: 0.8336, precision: 0.7638, recall: 0.5082\n",
      "2018-12-27T14:17:18.373681, step: 784, loss: 0.4639410674571991, acc: 0.7891, auc: 0.9008, precision: 0.8196, recall: 0.7669\n",
      "2018-12-27T14:17:18.502198, step: 785, loss: 0.5060847401618958, acc: 0.7109, auc: 0.8435, precision: 0.7626, recall: 0.6776\n",
      "2018-12-27T14:17:18.631296, step: 786, loss: 0.49435171484947205, acc: 0.7031, auc: 0.8618, precision: 0.75, recall: 0.7157\n",
      "2018-12-27T14:17:18.760452, step: 787, loss: 0.5504236221313477, acc: 0.7812, auc: 0.8461, precision: 0.781, recall: 0.7815\n",
      "2018-12-27T14:17:18.887679, step: 788, loss: 0.5221450328826904, acc: 0.6641, auc: 0.8795, precision: 0.7806, recall: 0.7055\n",
      "2018-12-27T14:17:19.014849, step: 789, loss: 0.479175329208374, acc: 0.8047, auc: 0.9073, precision: 0.8112, recall: 0.801\n",
      "2018-12-27T14:17:19.149173, step: 790, loss: 0.48183536529541016, acc: 0.7188, auc: 0.8845, precision: 0.7679, recall: 0.7517\n",
      "2018-12-27T14:17:19.285839, step: 791, loss: 0.5071268081665039, acc: 0.7734, auc: 0.8386, precision: 0.7697, recall: 0.7655\n",
      "2018-12-27T14:17:19.424399, step: 792, loss: 0.4443887770175934, acc: 0.7891, auc: 0.9134, precision: 0.8039, recall: 0.8026\n",
      "2018-12-27T14:17:19.580630, step: 793, loss: 0.5071095824241638, acc: 0.7969, auc: 0.8658, precision: 0.7969, recall: 0.7972\n",
      "2018-12-27T14:17:19.717853, step: 794, loss: 0.5151845216751099, acc: 0.6719, auc: 0.9008, precision: 0.8056, recall: 0.6613\n",
      "2018-12-27T14:17:19.849255, step: 795, loss: 0.5473788976669312, acc: 0.7812, auc: 0.8796, precision: 0.794, recall: 0.7859\n",
      "2018-12-27T14:17:19.978627, step: 796, loss: 0.48127856850624084, acc: 0.6641, auc: 0.8924, precision: 0.7443, recall: 0.6446\n",
      "2018-12-27T14:17:20.114194, step: 797, loss: 0.5351935625076294, acc: 0.7734, auc: 0.8495, precision: 0.7734, recall: 0.7767\n",
      "2018-12-27T14:17:20.249952, step: 798, loss: 0.5668652057647705, acc: 0.5625, auc: 0.8974, precision: 0.7647, recall: 0.5692\n",
      "2018-12-27T14:17:20.377415, step: 799, loss: 0.5076605081558228, acc: 0.7812, auc: 0.8652, precision: 0.7836, recall: 0.7786\n",
      "2018-12-27T14:17:20.503081, step: 800, loss: 0.4596223831176758, acc: 0.7188, auc: 0.8934, precision: 0.7708, recall: 0.7039\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:17:25.527752, step: 800, loss: 0.5796853719573272, acc: 0.6850315789473683, auc: 0.7662973684210526, precision: 0.6990473684210528, recall: 0.6867552631578944\n",
      "2018-12-27T14:17:25.653344, step: 801, loss: 0.44134777784347534, acc: 0.7891, auc: 0.8991, precision: 0.8065, recall: 0.7852\n",
      "2018-12-27T14:17:25.795079, step: 802, loss: 0.40448084473609924, acc: 0.8359, auc: 0.9267, precision: 0.8659, recall: 0.8404\n",
      "2018-12-27T14:17:25.922586, step: 803, loss: 0.49405932426452637, acc: 0.8125, auc: 0.9054, precision: 0.8223, recall: 0.8223\n",
      "2018-12-27T14:17:26.050889, step: 804, loss: 0.7972604632377625, acc: 0.5547, auc: 0.8779, precision: 0.7664, recall: 0.5476\n",
      "2018-12-27T14:17:26.178772, step: 805, loss: 0.5121796131134033, acc: 0.7734, auc: 0.8923, precision: 0.7781, recall: 0.7762\n",
      "2018-12-27T14:17:26.327145, step: 806, loss: 0.5058159828186035, acc: 0.6562, auc: 0.9003, precision: 0.7639, recall: 0.6925\n",
      "2018-12-27T14:17:26.472472, step: 807, loss: 0.5807231664657593, acc: 0.7266, auc: 0.8779, precision: 0.767, recall: 0.7295\n",
      "2018-12-27T14:17:26.609543, step: 808, loss: 0.5775497555732727, acc: 0.6172, auc: 0.8381, precision: 0.7924, recall: 0.5847\n",
      "2018-12-27T14:17:26.748920, step: 809, loss: 0.42453038692474365, acc: 0.8359, auc: 0.9204, precision: 0.8438, recall: 0.8278\n",
      "2018-12-27T14:17:26.883834, step: 810, loss: 0.4736330211162567, acc: 0.75, auc: 0.8781, precision: 0.8101, recall: 0.7465\n",
      "2018-12-27T14:17:27.020186, step: 811, loss: 0.40418392419815063, acc: 0.875, auc: 0.9324, precision: 0.8777, recall: 0.8791\n",
      "2018-12-27T14:17:27.156493, step: 812, loss: 0.4834643006324768, acc: 0.7656, auc: 0.8581, precision: 0.789, recall: 0.7559\n",
      "2018-12-27T14:17:27.286722, step: 813, loss: 0.4393305778503418, acc: 0.7422, auc: 0.8911, precision: 0.7718, recall: 0.7609\n",
      "2018-12-27T14:17:27.423129, step: 814, loss: 0.4839368462562561, acc: 0.8047, auc: 0.8804, precision: 0.8068, recall: 0.8074\n",
      "2018-12-27T14:17:27.560615, step: 815, loss: 0.5062787532806396, acc: 0.6797, auc: 0.8911, precision: 0.8066, recall: 0.6746\n",
      "2018-12-27T14:17:27.693350, step: 816, loss: 0.5317179560661316, acc: 0.7422, auc: 0.8671, precision: 0.7635, recall: 0.7226\n",
      "2018-12-27T14:17:27.822482, step: 817, loss: 0.5380211472511292, acc: 0.7031, auc: 0.8364, precision: 0.8021, recall: 0.6792\n",
      "2018-12-27T14:17:27.952015, step: 818, loss: 0.46662789583206177, acc: 0.8281, auc: 0.8848, precision: 0.8271, recall: 0.8271\n",
      "2018-12-27T14:17:28.081959, step: 819, loss: 0.4533119797706604, acc: 0.7188, auc: 0.9115, precision: 0.8043, recall: 0.75\n",
      "2018-12-27T14:17:28.218727, step: 820, loss: 0.48875802755355835, acc: 0.8125, auc: 0.8767, precision: 0.8135, recall: 0.8098\n",
      "2018-12-27T14:17:28.352166, step: 821, loss: 0.41760170459747314, acc: 0.7188, auc: 0.9323, precision: 0.8116, recall: 0.6859\n",
      "2018-12-27T14:17:28.479625, step: 822, loss: 0.5042686462402344, acc: 0.7812, auc: 0.8743, precision: 0.7812, recall: 0.7857\n",
      "2018-12-27T14:17:28.651473, step: 823, loss: 0.5595396757125854, acc: 0.6484, auc: 0.8559, precision: 0.8043, recall: 0.6121\n",
      "2018-12-27T14:17:28.796186, step: 824, loss: 0.5802234411239624, acc: 0.7891, auc: 0.8419, precision: 0.7936, recall: 0.796\n",
      "2018-12-27T14:17:28.951847, step: 825, loss: 0.5499081015586853, acc: 0.6406, auc: 0.8875, precision: 0.7909, recall: 0.6406\n",
      "2018-12-27T14:17:29.109627, step: 826, loss: 0.48761552572250366, acc: 0.7891, auc: 0.8834, precision: 0.796, recall: 0.7725\n",
      "2018-12-27T14:17:29.255073, step: 827, loss: 0.4360986053943634, acc: 0.7578, auc: 0.9352, precision: 0.8524, recall: 0.713\n",
      "2018-12-27T14:17:29.404261, step: 828, loss: 0.4315860867500305, acc: 0.8359, auc: 0.9132, precision: 0.8434, recall: 0.8336\n",
      "2018-12-27T14:17:29.530542, step: 829, loss: 0.3855922818183899, acc: 0.8203, auc: 0.9457, precision: 0.8469, recall: 0.8137\n",
      "2018-12-27T14:17:29.668812, step: 830, loss: 0.3448830246925354, acc: 0.8438, auc: 0.9404, precision: 0.8593, recall: 0.847\n",
      "2018-12-27T14:17:29.813117, step: 831, loss: 0.46661219000816345, acc: 0.7969, auc: 0.884, precision: 0.7971, recall: 0.7971\n",
      "2018-12-27T14:17:29.951633, step: 832, loss: 0.6919453144073486, acc: 0.5312, auc: 0.9248, precision: 0.7345, recall: 0.6\n",
      "2018-12-27T14:17:30.091577, step: 833, loss: 0.6527532339096069, acc: 0.6875, auc: 0.9229, precision: 0.7946, recall: 0.6321\n",
      "2018-12-27T14:17:30.217265, step: 834, loss: 0.6346966028213501, acc: 0.5312, auc: 0.9031, precision: 0.7638, recall: 0.5082\n",
      "2018-12-27T14:17:30.356990, step: 835, loss: 0.5043007135391235, acc: 0.7812, auc: 0.8633, precision: 0.7954, recall: 0.7812\n",
      "2018-12-27T14:17:30.497149, step: 836, loss: 0.4967162609100342, acc: 0.7344, auc: 0.8675, precision: 0.7932, recall: 0.7542\n",
      "2018-12-27T14:17:30.624503, step: 837, loss: 0.46234869956970215, acc: 0.8438, auc: 0.9238, precision: 0.8427, recall: 0.8496\n",
      "2018-12-27T14:17:30.765992, step: 838, loss: 0.5629023909568787, acc: 0.6406, auc: 0.8158, precision: 0.7238, recall: 0.6139\n",
      "2018-12-27T14:17:30.885322, step: 839, loss: 0.497877299785614, acc: 0.7734, auc: 0.8634, precision: 0.774, recall: 0.769\n",
      "2018-12-27T14:17:31.001348, step: 840, loss: 0.4666687846183777, acc: 0.7734, auc: 0.884, precision: 0.814, recall: 0.7734\n",
      "2018-12-27T14:17:31.127628, step: 841, loss: 0.4683881402015686, acc: 0.8516, auc: 0.8953, precision: 0.8516, recall: 0.8523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:17:31.258488, step: 842, loss: 0.4368463158607483, acc: 0.7734, auc: 0.9182, precision: 0.8237, recall: 0.7914\n",
      "2018-12-27T14:17:31.386833, step: 843, loss: 0.44361141324043274, acc: 0.7969, auc: 0.9167, precision: 0.8016, recall: 0.7922\n",
      "2018-12-27T14:17:31.522226, step: 844, loss: 0.40993309020996094, acc: 0.7656, auc: 0.9236, precision: 0.8029, recall: 0.7401\n",
      "2018-12-27T14:17:31.658974, step: 845, loss: 0.44107240438461304, acc: 0.7812, auc: 0.886, precision: 0.7959, recall: 0.7735\n",
      "2018-12-27T14:17:31.806876, step: 846, loss: 0.39305350184440613, acc: 0.8125, auc: 0.9155, precision: 0.8473, recall: 0.81\n",
      "2018-12-27T14:17:31.945839, step: 847, loss: 0.4341808557510376, acc: 0.8516, auc: 0.899, precision: 0.8509, recall: 0.8515\n",
      "2018-12-27T14:17:32.090033, step: 848, loss: 0.43951261043548584, acc: 0.6953, auc: 0.9472, precision: 0.7947, recall: 0.7292\n",
      "2018-12-27T14:17:32.228494, step: 849, loss: 0.9312034249305725, acc: 0.625, auc: 0.8777, precision: 0.7367, recall: 0.6497\n",
      "2018-12-27T14:17:32.371663, step: 850, loss: 0.6015941500663757, acc: 0.5625, auc: 0.8983, precision: 0.7586, recall: 0.5882\n",
      "2018-12-27T14:17:32.530654, step: 851, loss: 0.6468802094459534, acc: 0.6875, auc: 0.8449, precision: 0.7398, recall: 0.701\n",
      "2018-12-27T14:17:32.672847, step: 852, loss: 0.5342182517051697, acc: 0.625, auc: 0.873, precision: 0.7298, recall: 0.5957\n",
      "2018-12-27T14:17:32.799778, step: 853, loss: 0.4582008421421051, acc: 0.8281, auc: 0.8953, precision: 0.8591, recall: 0.8211\n",
      "2018-12-27T14:17:32.931669, step: 854, loss: 0.4729207158088684, acc: 0.75, auc: 0.86, precision: 0.7745, recall: 0.7225\n",
      "2018-12-27T14:17:33.058902, step: 855, loss: 0.4266965091228485, acc: 0.7734, auc: 0.9191, precision: 0.8031, recall: 0.7828\n",
      "2018-12-27T14:17:33.190907, step: 856, loss: 0.4381853938102722, acc: 0.8516, auc: 0.904, precision: 0.8525, recall: 0.8507\n",
      "2018-12-27T14:17:33.320031, step: 857, loss: 0.44665324687957764, acc: 0.7266, auc: 0.8968, precision: 0.7731, recall: 0.7387\n",
      "2018-12-27T14:17:33.448776, step: 858, loss: 0.45895087718963623, acc: 0.8125, auc: 0.8975, precision: 0.821, recall: 0.8084\n",
      "2018-12-27T14:17:33.577430, step: 859, loss: 0.43087032437324524, acc: 0.75, auc: 0.9171, precision: 0.7971, recall: 0.7763\n",
      "2018-12-27T14:17:33.708034, step: 860, loss: 0.49554529786109924, acc: 0.7891, auc: 0.8625, precision: 0.7897, recall: 0.7886\n",
      "2018-12-27T14:17:33.833282, step: 861, loss: 0.46872836351394653, acc: 0.6875, auc: 0.886, precision: 0.7466, recall: 0.6611\n",
      "2018-12-27T14:17:33.960608, step: 862, loss: 0.44954943656921387, acc: 0.7656, auc: 0.8727, precision: 0.7564, recall: 0.7622\n",
      "2018-12-27T14:17:34.096510, step: 863, loss: 0.4678890109062195, acc: 0.7656, auc: 0.883, precision: 0.7651, recall: 0.7651\n",
      "2018-12-27T14:17:34.222356, step: 864, loss: 0.46263110637664795, acc: 0.7344, auc: 0.8841, precision: 0.8045, recall: 0.6902\n",
      "2018-12-27T14:17:34.347235, step: 865, loss: 0.43343257904052734, acc: 0.8281, auc: 0.9251, precision: 0.8265, recall: 0.8281\n",
      "2018-12-27T14:17:34.473446, step: 866, loss: 0.5860780477523804, acc: 0.6094, auc: 0.8848, precision: 0.7706, recall: 0.6377\n",
      "2018-12-27T14:17:34.597405, step: 867, loss: 0.7558554410934448, acc: 0.6484, auc: 0.8601, precision: 0.7621, recall: 0.6814\n",
      "2018-12-27T14:17:34.727039, step: 868, loss: 0.5285038948059082, acc: 0.6484, auc: 0.8689, precision: 0.7724, recall: 0.626\n",
      "2018-12-27T14:17:34.854878, step: 869, loss: 0.4981125593185425, acc: 0.7578, auc: 0.8498, precision: 0.7826, recall: 0.7416\n",
      "2018-12-27T14:17:34.992085, step: 870, loss: 0.47091442346572876, acc: 0.75, auc: 0.8773, precision: 0.8229, recall: 0.7424\n",
      "2018-12-27T14:17:35.113819, step: 871, loss: 0.47520238161087036, acc: 0.8047, auc: 0.8843, precision: 0.8052, recall: 0.8032\n",
      "2018-12-27T14:17:35.242593, step: 872, loss: 0.5068050622940063, acc: 0.7188, auc: 0.859, precision: 0.7816, recall: 0.6986\n",
      "2018-12-27T14:17:35.375163, step: 873, loss: 0.4838237166404724, acc: 0.7734, auc: 0.8552, precision: 0.7739, recall: 0.7737\n",
      "2018-12-27T14:17:35.504918, step: 874, loss: 0.48745548725128174, acc: 0.75, auc: 0.8785, precision: 0.8021, recall: 0.7839\n",
      "2018-12-27T14:17:35.640396, step: 875, loss: 0.49208903312683105, acc: 0.7969, auc: 0.9456, precision: 0.8292, recall: 0.8113\n",
      "2018-12-27T14:17:35.768495, step: 876, loss: 0.4822736978530884, acc: 0.6562, auc: 0.904, precision: 0.8103, recall: 0.6071\n",
      "2018-12-27T14:17:35.899713, step: 877, loss: 0.39399388432502747, acc: 0.8594, auc: 0.9325, precision: 0.8634, recall: 0.8537\n",
      "2018-12-27T14:17:36.041512, step: 878, loss: 0.48110708594322205, acc: 0.7344, auc: 0.8542, precision: 0.7591, recall: 0.7235\n",
      "2018-12-27T14:17:36.186247, step: 879, loss: 0.45401620864868164, acc: 0.8203, auc: 0.8823, precision: 0.8312, recall: 0.8106\n",
      "2018-12-27T14:17:36.331263, step: 880, loss: 0.48891758918762207, acc: 0.7109, auc: 0.8631, precision: 0.7855, recall: 0.7187\n",
      "2018-12-27T14:17:36.458629, step: 881, loss: 0.4750039577484131, acc: 0.8047, auc: 0.8738, precision: 0.8048, recall: 0.8047\n",
      "2018-12-27T14:17:36.591113, step: 882, loss: 0.4832363724708557, acc: 0.6797, auc: 0.8957, precision: 0.7835, recall: 0.6889\n",
      "2018-12-27T14:17:36.742967, step: 883, loss: 0.5605824589729309, acc: 0.7578, auc: 0.857, precision: 0.7767, recall: 0.7617\n",
      "2018-12-27T14:17:36.876892, step: 884, loss: 0.5272267460823059, acc: 0.6328, auc: 0.901, precision: 0.7974, recall: 0.6017\n",
      "2018-12-27T14:17:37.009237, step: 885, loss: 0.47223883867263794, acc: 0.8438, auc: 0.8934, precision: 0.8452, recall: 0.8412\n",
      "2018-12-27T14:17:37.145770, step: 886, loss: 0.449817955493927, acc: 0.75, auc: 0.8987, precision: 0.7986, recall: 0.7531\n",
      "2018-12-27T14:17:37.308710, step: 887, loss: 0.41742274165153503, acc: 0.8125, auc: 0.8969, precision: 0.8099, recall: 0.8154\n",
      "2018-12-27T14:17:37.433491, step: 888, loss: 0.3728500306606293, acc: 0.8203, auc: 0.9251, precision: 0.8304, recall: 0.8272\n",
      "2018-12-27T14:17:37.563128, step: 889, loss: 0.36612871289253235, acc: 0.8828, auc: 0.9413, precision: 0.8827, recall: 0.8829\n",
      "2018-12-27T14:17:37.689981, step: 890, loss: 0.480959951877594, acc: 0.7344, auc: 0.8782, precision: 0.7853, recall: 0.7499\n",
      "2018-12-27T14:17:37.816501, step: 891, loss: 0.5149067640304565, acc: 0.7812, auc: 0.9516, precision: 0.8406, recall: 0.7712\n",
      "2018-12-27T14:17:37.957415, step: 892, loss: 0.7156315445899963, acc: 0.5234, auc: 0.9025, precision: 0.7437, recall: 0.5643\n",
      "2018-12-27T14:17:38.096797, step: 893, loss: 0.431654691696167, acc: 0.7969, auc: 0.9077, precision: 0.7911, recall: 0.7759\n",
      "2018-12-27T14:17:38.229472, step: 894, loss: 0.4594782888889313, acc: 0.75, auc: 0.8733, precision: 0.7713, recall: 0.7659\n",
      "2018-12-27T14:17:38.356543, step: 895, loss: 0.4428960680961609, acc: 0.8828, auc: 0.9406, precision: 0.8868, recall: 0.8876\n",
      "2018-12-27T14:17:38.484848, step: 896, loss: 0.6090700626373291, acc: 0.6484, auc: 0.815, precision: 0.7132, recall: 0.618\n",
      "2018-12-27T14:17:38.625346, step: 897, loss: 0.43715548515319824, acc: 0.7969, auc: 0.8909, precision: 0.7961, recall: 0.7961\n",
      "2018-12-27T14:17:38.760358, step: 898, loss: 0.4730730652809143, acc: 0.8125, auc: 0.869, precision: 0.8246, recall: 0.8094\n",
      "2018-12-27T14:17:38.886077, step: 899, loss: 0.5242452621459961, acc: 0.6797, auc: 0.8476, precision: 0.7886, recall: 0.6647\n",
      "2018-12-27T14:17:39.024526, step: 900, loss: 0.5233858823776245, acc: 0.7812, auc: 0.9143, precision: 0.8023, recall: 0.7993\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:17:44.043852, step: 900, loss: 0.7104301060500898, acc: 0.5538605263157894, auc: 0.7611184210526316, precision: 0.705457894736842, recall: 0.5596421052631577\n",
      "2018-12-27T14:17:44.170212, step: 901, loss: 0.5172877311706543, acc: 0.6328, auc: 0.9062, precision: 0.7991, recall: 0.5948\n",
      "2018-12-27T14:17:44.324078, step: 902, loss: 0.43124327063560486, acc: 0.8359, auc: 0.8972, precision: 0.8462, recall: 0.8359\n",
      "2018-12-27T14:17:44.449525, step: 903, loss: 0.45503705739974976, acc: 0.75, auc: 0.8841, precision: 0.7879, recall: 0.7608\n",
      "2018-12-27T14:17:44.575442, step: 904, loss: 0.48037344217300415, acc: 0.7812, auc: 0.8953, precision: 0.7835, recall: 0.7877\n",
      "2018-12-27T14:17:44.713150, step: 905, loss: 0.5557374954223633, acc: 0.6328, auc: 0.8808, precision: 0.7567, recall: 0.6628\n",
      "2018-12-27T14:17:44.839061, step: 906, loss: 0.4697106182575226, acc: 0.8047, auc: 0.8946, precision: 0.8084, recall: 0.8005\n",
      "2018-12-27T14:17:44.964716, step: 907, loss: 0.3596833348274231, acc: 0.8047, auc: 0.9646, precision: 0.8511, recall: 0.8018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:17:45.101184, step: 908, loss: 0.45801660418510437, acc: 0.7891, auc: 0.88, precision: 0.7896, recall: 0.7835\n",
      "2018-12-27T14:17:45.237210, step: 909, loss: 0.458112895488739, acc: 0.7422, auc: 0.8962, precision: 0.8048, recall: 0.7422\n",
      "2018-12-27T14:17:45.374813, step: 910, loss: 0.46077460050582886, acc: 0.7891, auc: 0.9076, precision: 0.7899, recall: 0.7924\n",
      "2018-12-27T14:17:45.508514, step: 911, loss: 0.5716449618339539, acc: 0.625, auc: 0.8708, precision: 0.7589, recall: 0.6193\n",
      "2018-12-27T14:17:45.633198, step: 912, loss: 0.5344277620315552, acc: 0.7891, auc: 0.883, precision: 0.795, recall: 0.7853\n",
      "2018-12-27T14:17:45.765710, step: 913, loss: 0.4954147934913635, acc: 0.7031, auc: 0.8679, precision: 0.7602, recall: 0.6915\n",
      "2018-12-27T14:17:45.902387, step: 914, loss: 0.46125829219818115, acc: 0.7891, auc: 0.8808, precision: 0.7816, recall: 0.7799\n",
      "2018-12-27T14:17:46.029540, step: 915, loss: 0.44884592294692993, acc: 0.7188, auc: 0.9158, precision: 0.7907, recall: 0.7226\n",
      "2018-12-27T14:17:46.165731, step: 916, loss: 0.47356095910072327, acc: 0.7891, auc: 0.8784, precision: 0.7891, recall: 0.7891\n",
      "2018-12-27T14:17:46.319742, step: 917, loss: 0.3905363976955414, acc: 0.7812, auc: 0.9314, precision: 0.8208, recall: 0.8083\n",
      "2018-12-27T14:17:46.453053, step: 918, loss: 0.473890483379364, acc: 0.7578, auc: 0.8952, precision: 0.7776, recall: 0.7534\n",
      "2018-12-27T14:17:46.578731, step: 919, loss: 0.42234280705451965, acc: 0.75, auc: 0.9118, precision: 0.8295, recall: 0.7113\n",
      "2018-12-27T14:17:46.704798, step: 920, loss: 0.41877612471580505, acc: 0.8281, auc: 0.8967, precision: 0.8381, recall: 0.8322\n",
      "2018-12-27T14:17:46.830576, step: 921, loss: 0.42342716455459595, acc: 0.8047, auc: 0.8998, precision: 0.8138, recall: 0.7907\n",
      "2018-12-27T14:17:46.955368, step: 922, loss: 0.4590122103691101, acc: 0.7109, auc: 0.9274, precision: 0.8033, recall: 0.7021\n",
      "2018-12-27T14:17:47.095884, step: 923, loss: 0.5858575105667114, acc: 0.7578, auc: 0.9382, precision: 0.8434, recall: 0.7417\n",
      "2018-12-27T14:17:47.220801, step: 924, loss: 0.5597031116485596, acc: 0.5469, auc: 0.9492, precision: 0.7387, recall: 0.6133\n",
      "2018-12-27T14:17:47.346350, step: 925, loss: 0.4244612455368042, acc: 0.875, auc: 0.9355, precision: 0.8819, recall: 0.8729\n",
      "2018-12-27T14:17:47.485482, step: 926, loss: 0.49130797386169434, acc: 0.7656, auc: 0.8452, precision: 0.7723, recall: 0.7643\n",
      "2018-12-27T14:17:47.614278, step: 927, loss: 0.3947902321815491, acc: 0.7656, auc: 0.9238, precision: 0.8191, recall: 0.7624\n",
      "2018-12-27T14:17:47.739697, step: 928, loss: 0.44668278098106384, acc: 0.8125, auc: 0.881, precision: 0.8115, recall: 0.8094\n",
      "2018-12-27T14:17:47.872394, step: 929, loss: 0.30229780077934265, acc: 0.8828, auc: 0.9744, precision: 0.8899, recall: 0.8839\n",
      "2018-12-27T14:17:48.009579, step: 930, loss: 0.41370177268981934, acc: 0.7891, auc: 0.8909, precision: 0.7912, recall: 0.7917\n",
      "start training model\n",
      "2018-12-27T14:17:48.292249, step: 931, loss: 0.2689155340194702, acc: 0.8828, auc: 0.9838, precision: 0.8909, recall: 0.8877\n",
      "2018-12-27T14:17:48.445898, step: 932, loss: 0.24076002836227417, acc: 0.9297, auc: 0.98, precision: 0.9298, recall: 0.9297\n",
      "2018-12-27T14:17:48.597393, step: 933, loss: 0.27607274055480957, acc: 0.8672, auc: 0.9721, precision: 0.8906, recall: 0.8634\n",
      "2018-12-27T14:17:48.736500, step: 934, loss: 0.26850783824920654, acc: 0.9375, auc: 0.9922, precision: 0.9414, recall: 0.9375\n",
      "2018-12-27T14:17:48.860756, step: 935, loss: 0.35602325201034546, acc: 0.7891, auc: 0.959, precision: 0.8663, recall: 0.75\n",
      "2018-12-27T14:17:48.984629, step: 936, loss: 0.3789733946323395, acc: 0.8672, auc: 0.9722, precision: 0.8768, recall: 0.8756\n",
      "2018-12-27T14:17:49.118324, step: 937, loss: 0.5480455160140991, acc: 0.6172, auc: 0.9625, precision: 0.7667, recall: 0.6597\n",
      "2018-12-27T14:17:49.250126, step: 938, loss: 0.4206576347351074, acc: 0.8359, auc: 0.9617, precision: 0.8504, recall: 0.8359\n",
      "2018-12-27T14:17:49.384694, step: 939, loss: 0.3775600492954254, acc: 0.7266, auc: 0.9746, precision: 0.8118, recall: 0.75\n",
      "2018-12-27T14:17:49.522910, step: 940, loss: 0.4851355254650116, acc: 0.8438, auc: 0.9227, precision: 0.8889, recall: 0.8276\n",
      "2018-12-27T14:17:49.666256, step: 941, loss: 0.3084642291069031, acc: 0.8438, auc: 0.9678, precision: 0.881, recall: 0.8438\n",
      "2018-12-27T14:17:49.802412, step: 942, loss: 0.33076944947242737, acc: 0.875, auc: 0.9477, precision: 0.8752, recall: 0.8752\n",
      "2018-12-27T14:17:49.938016, step: 943, loss: 0.3152538537979126, acc: 0.8672, auc: 0.9755, precision: 0.8851, recall: 0.8724\n",
      "2018-12-27T14:17:50.067302, step: 944, loss: 0.23310720920562744, acc: 0.9375, auc: 0.9941, precision: 0.9396, recall: 0.937\n",
      "2018-12-27T14:17:50.212444, step: 945, loss: 0.2964584231376648, acc: 0.8672, auc: 0.9609, precision: 0.8847, recall: 0.8639\n",
      "2018-12-27T14:17:50.337317, step: 946, loss: 0.2571170926094055, acc: 0.9062, auc: 0.978, precision: 0.9071, recall: 0.9071\n",
      "2018-12-27T14:17:50.468529, step: 947, loss: 0.30044713616371155, acc: 0.8516, auc: 0.9602, precision: 0.8609, recall: 0.8541\n",
      "2018-12-27T14:17:50.606861, step: 948, loss: 0.26479077339172363, acc: 0.8828, auc: 0.976, precision: 0.8861, recall: 0.8771\n",
      "2018-12-27T14:17:50.733121, step: 949, loss: 0.35458701848983765, acc: 0.7812, auc: 0.9564, precision: 0.8197, recall: 0.797\n",
      "2018-12-27T14:17:50.864658, step: 950, loss: 0.498056024312973, acc: 0.7734, auc: 0.9396, precision: 0.8255, recall: 0.7638\n",
      "2018-12-27T14:17:50.984514, step: 951, loss: 0.4511064291000366, acc: 0.6641, auc: 0.9594, precision: 0.7952, recall: 0.6742\n",
      "2018-12-27T14:17:51.131324, step: 952, loss: 0.4006262719631195, acc: 0.875, auc: 0.9096, precision: 0.8754, recall: 0.8735\n",
      "2018-12-27T14:17:51.301468, step: 953, loss: 0.27246418595314026, acc: 0.8906, auc: 0.9773, precision: 0.8925, recall: 0.8956\n",
      "2018-12-27T14:17:51.430748, step: 954, loss: 0.3051595389842987, acc: 0.9062, auc: 0.9663, precision: 0.9221, recall: 0.8948\n",
      "2018-12-27T14:17:51.556515, step: 955, loss: 0.2268282175064087, acc: 0.9453, auc: 0.9865, precision: 0.9494, recall: 0.9434\n",
      "2018-12-27T14:17:51.682306, step: 956, loss: 0.20773546397686005, acc: 0.9297, auc: 0.9924, precision: 0.9321, recall: 0.9328\n",
      "2018-12-27T14:17:51.805906, step: 957, loss: 0.30440136790275574, acc: 0.8906, auc: 0.964, precision: 0.8911, recall: 0.8919\n",
      "2018-12-27T14:17:51.939922, step: 958, loss: 0.3136872947216034, acc: 0.8047, auc: 0.9914, precision: 0.8547, recall: 0.8134\n",
      "2018-12-27T14:17:52.084251, step: 959, loss: 0.3513014018535614, acc: 0.8828, auc: 0.9635, precision: 0.8967, recall: 0.8785\n",
      "2018-12-27T14:17:52.213249, step: 960, loss: 0.342958003282547, acc: 0.8203, auc: 0.9583, precision: 0.8614, recall: 0.8309\n",
      "2018-12-27T14:17:52.351520, step: 961, loss: 0.2762274742126465, acc: 0.9219, auc: 0.9778, precision: 0.9221, recall: 0.9221\n",
      "2018-12-27T14:17:52.495076, step: 962, loss: 0.3153735399246216, acc: 0.8438, auc: 0.9469, precision: 0.8693, recall: 0.8375\n",
      "2018-12-27T14:17:52.638468, step: 963, loss: 0.21053986251354218, acc: 0.9219, auc: 0.9889, precision: 0.9278, recall: 0.9177\n",
      "2018-12-27T14:17:52.790667, step: 964, loss: 0.23502634465694427, acc: 0.9062, auc: 0.9792, precision: 0.9188, recall: 0.9024\n",
      "2018-12-27T14:17:52.917183, step: 965, loss: 0.22999554872512817, acc: 0.9375, auc: 0.9915, precision: 0.9388, recall: 0.938\n",
      "2018-12-27T14:17:53.046030, step: 966, loss: 0.41232675313949585, acc: 0.7422, auc: 0.9875, precision: 0.8366, recall: 0.725\n",
      "2018-12-27T14:17:53.175635, step: 967, loss: 0.4152100682258606, acc: 0.8594, auc: 0.9697, precision: 0.8759, recall: 0.87\n",
      "2018-12-27T14:17:53.304110, step: 968, loss: 0.32278960943222046, acc: 0.8047, auc: 0.9765, precision: 0.8656, recall: 0.7917\n",
      "2018-12-27T14:17:53.436464, step: 969, loss: 0.2973291873931885, acc: 0.9219, auc: 0.9638, precision: 0.9206, recall: 0.9206\n",
      "2018-12-27T14:17:53.560141, step: 970, loss: 0.3008423149585724, acc: 0.8594, auc: 0.9545, precision: 0.8763, recall: 0.8473\n",
      "2018-12-27T14:17:53.683726, step: 971, loss: 0.2236049771308899, acc: 0.9141, auc: 0.9841, precision: 0.9178, recall: 0.9157\n",
      "2018-12-27T14:17:53.823056, step: 972, loss: 0.21533048152923584, acc: 0.9141, auc: 0.9853, precision: 0.9141, recall: 0.9132\n",
      "2018-12-27T14:17:53.951315, step: 973, loss: 0.30427688360214233, acc: 0.8438, auc: 0.9536, precision: 0.8709, recall: 0.833\n",
      "2018-12-27T14:17:54.074530, step: 974, loss: 0.1742122769355774, acc: 0.9766, auc: 0.998, precision: 0.9746, recall: 0.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:17:54.214001, step: 975, loss: 0.20305564999580383, acc: 0.8984, auc: 0.9868, precision: 0.9077, recall: 0.9023\n",
      "2018-12-27T14:17:54.356277, step: 976, loss: 0.34216710925102234, acc: 0.8672, auc: 0.9549, precision: 0.8764, recall: 0.8623\n",
      "2018-12-27T14:17:54.481766, step: 977, loss: 0.25839927792549133, acc: 0.8672, auc: 0.9751, precision: 0.9023, recall: 0.8534\n",
      "2018-12-27T14:17:54.605940, step: 978, loss: 0.2778133153915405, acc: 0.8984, auc: 0.9633, precision: 0.8987, recall: 0.8912\n",
      "2018-12-27T14:17:54.744077, step: 979, loss: 0.2765924334526062, acc: 0.8203, auc: 0.9807, precision: 0.8614, recall: 0.815\n",
      "2018-12-27T14:17:54.867412, step: 980, loss: 0.27535924315452576, acc: 0.9219, auc: 0.9895, precision: 0.9306, recall: 0.9242\n",
      "2018-12-27T14:17:54.990384, step: 981, loss: 0.28680306673049927, acc: 0.8359, auc: 0.9846, precision: 0.8807, recall: 0.8279\n",
      "2018-12-27T14:17:55.118807, step: 982, loss: 0.23789700865745544, acc: 0.9219, auc: 0.977, precision: 0.9228, recall: 0.9228\n",
      "2018-12-27T14:17:55.246628, step: 983, loss: 0.31685924530029297, acc: 0.8359, auc: 0.9583, precision: 0.8684, recall: 0.8359\n",
      "2018-12-27T14:17:55.369521, step: 984, loss: 0.2971704602241516, acc: 0.875, auc: 0.965, precision: 0.8823, recall: 0.8718\n",
      "2018-12-27T14:17:55.494078, step: 985, loss: 0.22971659898757935, acc: 0.8828, auc: 0.9829, precision: 0.9062, recall: 0.881\n",
      "2018-12-27T14:17:55.620427, step: 986, loss: 0.20941945910453796, acc: 0.9375, auc: 0.9826, precision: 0.9402, recall: 0.9359\n",
      "2018-12-27T14:17:55.771577, step: 987, loss: 0.20526599884033203, acc: 0.9062, auc: 0.9835, precision: 0.9159, recall: 0.8995\n",
      "2018-12-27T14:17:55.928621, step: 988, loss: 0.39419832825660706, acc: 0.8281, auc: 0.9136, precision: 0.8284, recall: 0.8281\n",
      "2018-12-27T14:17:56.054792, step: 989, loss: 0.281549334526062, acc: 0.8125, auc: 0.9732, precision: 0.8601, recall: 0.7978\n",
      "2018-12-27T14:17:56.182341, step: 990, loss: 0.33895906805992126, acc: 0.8906, auc: 0.9612, precision: 0.9011, recall: 0.8894\n",
      "2018-12-27T14:17:56.305904, step: 991, loss: 0.2948397994041443, acc: 0.8047, auc: 0.9805, precision: 0.8567, recall: 0.786\n",
      "2018-12-27T14:17:56.439736, step: 992, loss: 0.30771052837371826, acc: 0.8906, auc: 0.9856, precision: 0.9079, recall: 0.8939\n",
      "2018-12-27T14:17:56.567299, step: 993, loss: 0.4391724467277527, acc: 0.7344, auc: 0.9587, precision: 0.8229, recall: 0.7424\n",
      "2018-12-27T14:17:56.693062, step: 994, loss: 0.3581797778606415, acc: 0.875, auc: 0.9276, precision: 0.8735, recall: 0.8735\n",
      "2018-12-27T14:17:56.839265, step: 995, loss: 0.27475082874298096, acc: 0.9062, auc: 0.959, precision: 0.907, recall: 0.9094\n",
      "2018-12-27T14:17:56.967143, step: 996, loss: 0.324542760848999, acc: 0.9141, auc: 0.9682, precision: 0.9105, recall: 0.9182\n",
      "2018-12-27T14:17:57.116525, step: 997, loss: 0.30649128556251526, acc: 0.8125, auc: 0.9725, precision: 0.8489, recall: 0.8225\n",
      "2018-12-27T14:17:57.240876, step: 998, loss: 0.21139737963676453, acc: 0.9453, auc: 0.9932, precision: 0.9454, recall: 0.9453\n",
      "2018-12-27T14:17:57.378898, step: 999, loss: 0.22965852916240692, acc: 0.9375, auc: 0.9822, precision: 0.9372, recall: 0.93\n",
      "2018-12-27T14:17:57.519137, step: 1000, loss: 0.2626131474971771, acc: 0.8516, auc: 0.9771, precision: 0.872, recall: 0.8516\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:18:02.294610, step: 1000, loss: 0.6909640196122622, acc: 0.6961368421052632, auc: 0.7680947368421055, precision: 0.7008394736842106, recall: 0.6959842105263158\n",
      "2018-12-27T14:18:02.417535, step: 1001, loss: 0.28794607520103455, acc: 0.9062, auc: 0.9837, precision: 0.9097, recall: 0.9138\n",
      "2018-12-27T14:18:02.554909, step: 1002, loss: 0.29713577032089233, acc: 0.8359, auc: 0.9713, precision: 0.8906, recall: 0.8019\n",
      "2018-12-27T14:18:02.671481, step: 1003, loss: 0.28157246112823486, acc: 0.8672, auc: 0.9568, precision: 0.8681, recall: 0.8668\n",
      "2018-12-27T14:18:02.811287, step: 1004, loss: 0.2236516922712326, acc: 0.8984, auc: 0.9822, precision: 0.9071, recall: 0.9085\n",
      "2018-12-27T14:18:02.949991, step: 1005, loss: 0.26196032762527466, acc: 0.8828, auc: 0.9668, precision: 0.8834, recall: 0.8832\n",
      "2018-12-27T14:18:03.083727, step: 1006, loss: 0.2122519314289093, acc: 0.8672, auc: 0.9799, precision: 0.8839, recall: 0.874\n",
      "2018-12-27T14:18:03.210501, step: 1007, loss: 0.3118193447589874, acc: 0.8906, auc: 0.9917, precision: 0.9054, recall: 0.8971\n",
      "2018-12-27T14:18:03.332698, step: 1008, loss: 0.37352997064590454, acc: 0.75, auc: 0.9818, precision: 0.8416, recall: 0.7288\n",
      "2018-12-27T14:18:03.463801, step: 1009, loss: 0.38272619247436523, acc: 0.8359, auc: 0.9439, precision: 0.8481, recall: 0.8244\n",
      "2018-12-27T14:18:03.583597, step: 1010, loss: 0.36652010679244995, acc: 0.7656, auc: 0.968, precision: 0.837, recall: 0.7727\n",
      "2018-12-27T14:18:03.708519, step: 1011, loss: 0.21731558442115784, acc: 0.9375, auc: 0.9865, precision: 0.9405, recall: 0.9353\n",
      "2018-12-27T14:18:03.832754, step: 1012, loss: 0.27543458342552185, acc: 0.8984, auc: 0.955, precision: 0.9015, recall: 0.9008\n",
      "2018-12-27T14:18:03.960777, step: 1013, loss: 0.23097345232963562, acc: 0.8906, auc: 0.9773, precision: 0.8931, recall: 0.892\n",
      "2018-12-27T14:18:04.102905, step: 1014, loss: 0.2882344126701355, acc: 0.8906, auc: 0.9483, precision: 0.8995, recall: 0.8837\n",
      "2018-12-27T14:18:04.250486, step: 1015, loss: 0.24361667037010193, acc: 0.9062, auc: 0.965, precision: 0.9052, recall: 0.9084\n",
      "2018-12-27T14:18:04.380265, step: 1016, loss: 0.2282065898180008, acc: 0.9375, auc: 0.9851, precision: 0.9402, recall: 0.9359\n",
      "2018-12-27T14:18:04.504053, step: 1017, loss: 0.2722574472427368, acc: 0.8281, auc: 0.9699, precision: 0.8599, recall: 0.8186\n",
      "2018-12-27T14:18:04.635896, step: 1018, loss: 0.25545376539230347, acc: 0.9219, auc: 0.9851, precision: 0.9245, recall: 0.9233\n",
      "2018-12-27T14:18:04.771158, step: 1019, loss: 0.2480422556400299, acc: 0.8203, auc: 0.9845, precision: 0.8646, recall: 0.8063\n",
      "2018-12-27T14:18:04.903812, step: 1020, loss: 0.19636943936347961, acc: 0.9453, auc: 0.988, precision: 0.9455, recall: 0.9452\n",
      "2018-12-27T14:18:05.038946, step: 1021, loss: 0.2607994079589844, acc: 0.8906, auc: 0.9788, precision: 0.9028, recall: 0.9\n",
      "2018-12-27T14:18:05.165554, step: 1022, loss: 0.3768002688884735, acc: 0.8359, auc: 0.9365, precision: 0.84, recall: 0.8359\n",
      "2018-12-27T14:18:05.288898, step: 1023, loss: 0.3851471245288849, acc: 0.7891, auc: 0.9321, precision: 0.8231, recall: 0.7916\n",
      "2018-12-27T14:18:05.430566, step: 1024, loss: 0.27363577485084534, acc: 0.9062, auc: 0.9629, precision: 0.9053, recall: 0.9069\n",
      "2018-12-27T14:18:05.572973, step: 1025, loss: 0.22889280319213867, acc: 0.875, auc: 0.9846, precision: 0.8917, recall: 0.8686\n",
      "2018-12-27T14:18:05.705655, step: 1026, loss: 0.30969902873039246, acc: 0.8594, auc: 0.9642, precision: 0.8664, recall: 0.8549\n",
      "2018-12-27T14:18:05.831216, step: 1027, loss: 0.3251097500324249, acc: 0.7891, auc: 0.9781, precision: 0.8323, recall: 0.8057\n",
      "2018-12-27T14:18:05.961040, step: 1028, loss: 0.32478514313697815, acc: 0.8828, auc: 0.9556, precision: 0.8876, recall: 0.8868\n",
      "2018-12-27T14:18:06.088426, step: 1029, loss: 0.38364869356155396, acc: 0.7969, auc: 0.9181, precision: 0.813, recall: 0.813\n",
      "2018-12-27T14:18:06.225956, step: 1030, loss: 0.22860176861286163, acc: 0.9141, auc: 0.9743, precision: 0.9157, recall: 0.9152\n",
      "2018-12-27T14:18:06.358308, step: 1031, loss: 0.27747300267219543, acc: 0.8984, auc: 0.9556, precision: 0.902, recall: 0.8956\n",
      "2018-12-27T14:18:06.489928, step: 1032, loss: 0.22894573211669922, acc: 0.8906, auc: 0.973, precision: 0.8906, recall: 0.9004\n",
      "2018-12-27T14:18:06.632023, step: 1033, loss: 0.3700450658798218, acc: 0.8516, auc: 0.9265, precision: 0.8482, recall: 0.8497\n",
      "2018-12-27T14:18:06.766930, step: 1034, loss: 0.30846014618873596, acc: 0.8281, auc: 0.9727, precision: 0.8721, recall: 0.8281\n",
      "2018-12-27T14:18:06.899803, step: 1035, loss: 0.32649677991867065, acc: 0.875, auc: 0.9548, precision: 0.8798, recall: 0.8768\n",
      "2018-12-27T14:18:07.024028, step: 1036, loss: 0.31551551818847656, acc: 0.8125, auc: 0.9557, precision: 0.8416, recall: 0.8055\n",
      "2018-12-27T14:18:07.146519, step: 1037, loss: 0.32695960998535156, acc: 0.875, auc: 0.9467, precision: 0.8747, recall: 0.8747\n",
      "2018-12-27T14:18:07.279557, step: 1038, loss: 0.29109135270118713, acc: 0.8047, auc: 0.9843, precision: 0.8547, recall: 0.8134\n",
      "2018-12-27T14:18:07.419630, step: 1039, loss: 0.2939035892486572, acc: 0.9062, auc: 0.9907, precision: 0.9189, recall: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:18:07.553793, step: 1040, loss: 0.29884758591651917, acc: 0.7969, auc: 0.9718, precision: 0.8502, recall: 0.7843\n",
      "2018-12-27T14:18:07.678339, step: 1041, loss: 0.23489993810653687, acc: 0.9297, auc: 0.9785, precision: 0.9304, recall: 0.93\n",
      "2018-12-27T14:18:07.802695, step: 1042, loss: 0.24497944116592407, acc: 0.875, auc: 0.9757, precision: 0.8804, recall: 0.8849\n",
      "2018-12-27T14:18:07.934687, step: 1043, loss: 0.3063560724258423, acc: 0.8672, auc: 0.974, precision: 0.8751, recall: 0.8721\n",
      "2018-12-27T14:18:08.082660, step: 1044, loss: 0.4225309193134308, acc: 0.7734, auc: 0.946, precision: 0.8253, recall: 0.7886\n",
      "2018-12-27T14:18:08.206809, step: 1045, loss: 0.292369544506073, acc: 0.8906, auc: 0.9668, precision: 0.8906, recall: 0.891\n",
      "2018-12-27T14:18:08.329883, step: 1046, loss: 0.23529210686683655, acc: 0.8906, auc: 0.9814, precision: 0.9008, recall: 0.8961\n",
      "2018-12-27T14:18:08.452866, step: 1047, loss: 0.26105231046676636, acc: 0.9375, auc: 0.969, precision: 0.9385, recall: 0.9359\n",
      "2018-12-27T14:18:08.585477, step: 1048, loss: 0.22368010878562927, acc: 0.9141, auc: 0.9773, precision: 0.9202, recall: 0.9123\n",
      "2018-12-27T14:18:08.704270, step: 1049, loss: 0.29355886578559875, acc: 0.8672, auc: 0.9512, precision: 0.8658, recall: 0.8682\n",
      "2018-12-27T14:18:08.828045, step: 1050, loss: 0.23369382321834564, acc: 0.8906, auc: 0.9712, precision: 0.8931, recall: 0.892\n",
      "2018-12-27T14:18:08.981770, step: 1051, loss: 0.18584521114826202, acc: 0.9453, auc: 0.9801, precision: 0.9448, recall: 0.9456\n",
      "2018-12-27T14:18:09.106255, step: 1052, loss: 0.22645193338394165, acc: 0.8828, auc: 0.9851, precision: 0.8983, recall: 0.8844\n",
      "2018-12-27T14:18:09.235494, step: 1053, loss: 0.19443050026893616, acc: 0.9609, auc: 0.9917, precision: 0.9569, recall: 0.9667\n",
      "2018-12-27T14:18:09.378859, step: 1054, loss: 0.38534432649612427, acc: 0.7656, auc: 0.9692, precision: 0.837, recall: 0.7727\n",
      "2018-12-27T14:18:09.526056, step: 1055, loss: 0.4049815535545349, acc: 0.8359, auc: 0.9658, precision: 0.8748, recall: 0.8204\n",
      "2018-12-27T14:18:09.678231, step: 1056, loss: 0.3511072099208832, acc: 0.8203, auc: 0.9516, precision: 0.8777, recall: 0.7982\n",
      "2018-12-27T14:18:09.833809, step: 1057, loss: 0.2911796569824219, acc: 0.9141, auc: 0.9639, precision: 0.9142, recall: 0.9141\n",
      "2018-12-27T14:18:09.984861, step: 1058, loss: 0.24448144435882568, acc: 0.8828, auc: 0.9777, precision: 0.9, recall: 0.8897\n",
      "2018-12-27T14:18:10.123131, step: 1059, loss: 0.2691381573677063, acc: 0.9219, auc: 0.9773, precision: 0.9228, recall: 0.9228\n",
      "2018-12-27T14:18:10.255947, step: 1060, loss: 0.26813510060310364, acc: 0.8672, auc: 0.9784, precision: 0.8882, recall: 0.8768\n",
      "2018-12-27T14:18:10.383879, step: 1061, loss: 0.23398719727993011, acc: 0.9141, auc: 0.975, precision: 0.9135, recall: 0.9142\n",
      "2018-12-27T14:18:10.507939, step: 1062, loss: 0.24992698431015015, acc: 0.9062, auc: 0.9678, precision: 0.907, recall: 0.9094\n",
      "2018-12-27T14:18:10.639160, step: 1063, loss: 0.25618842244148254, acc: 0.8828, auc: 0.9691, precision: 0.8828, recall: 0.8905\n",
      "2018-12-27T14:18:10.771594, step: 1064, loss: 0.30097949504852295, acc: 0.8984, auc: 0.9758, precision: 0.8984, recall: 0.9033\n",
      "2018-12-27T14:18:10.910633, step: 1065, loss: 0.4369608759880066, acc: 0.7812, auc: 0.9536, precision: 0.8462, recall: 0.7846\n",
      "2018-12-27T14:18:11.036187, step: 1066, loss: 0.3636224865913391, acc: 0.8906, auc: 0.9648, precision: 0.8979, recall: 0.8941\n",
      "2018-12-27T14:18:11.160967, step: 1067, loss: 0.34389734268188477, acc: 0.8359, auc: 0.9536, precision: 0.8707, recall: 0.8311\n",
      "2018-12-27T14:18:11.286995, step: 1068, loss: 0.31188297271728516, acc: 0.9062, auc: 0.9634, precision: 0.9078, recall: 0.9062\n",
      "2018-12-27T14:18:11.412596, step: 1069, loss: 0.2769678235054016, acc: 0.8516, auc: 0.9644, precision: 0.892, recall: 0.839\n",
      "2018-12-27T14:18:11.539746, step: 1070, loss: 0.32702261209487915, acc: 0.8438, auc: 0.9369, precision: 0.844, recall: 0.8427\n",
      "2018-12-27T14:18:11.663573, step: 1071, loss: 0.21508201956748962, acc: 0.8906, auc: 0.9799, precision: 0.9091, recall: 0.8826\n",
      "2018-12-27T14:18:11.787602, step: 1072, loss: 0.260858952999115, acc: 0.8828, auc: 0.9607, precision: 0.8819, recall: 0.8827\n",
      "2018-12-27T14:18:11.909693, step: 1073, loss: 0.21025815606117249, acc: 0.9062, auc: 0.9809, precision: 0.9086, recall: 0.9147\n",
      "2018-12-27T14:18:12.045518, step: 1074, loss: 0.25696879625320435, acc: 0.9141, auc: 0.9828, precision: 0.9157, recall: 0.9178\n",
      "2018-12-27T14:18:12.171254, step: 1075, loss: 0.2863274812698364, acc: 0.8281, auc: 0.9801, precision: 0.8682, recall: 0.8176\n",
      "2018-12-27T14:18:12.300076, step: 1076, loss: 0.3191325068473816, acc: 0.875, auc: 0.9399, precision: 0.8739, recall: 0.8739\n",
      "2018-12-27T14:18:12.425026, step: 1077, loss: 0.21209339797496796, acc: 0.8984, auc: 0.978, precision: 0.9012, recall: 0.8978\n",
      "2018-12-27T14:18:12.569206, step: 1078, loss: 0.25611090660095215, acc: 0.9062, auc: 0.9626, precision: 0.9081, recall: 0.9113\n",
      "2018-12-27T14:18:12.710259, step: 1079, loss: 0.2726975083351135, acc: 0.9219, auc: 0.9594, precision: 0.9216, recall: 0.9224\n",
      "2018-12-27T14:18:12.851158, step: 1080, loss: 0.2829277515411377, acc: 0.8438, auc: 0.9753, precision: 0.8824, recall: 0.8413\n",
      "2018-12-27T14:18:12.991033, step: 1081, loss: 0.307835191488266, acc: 0.8984, auc: 0.9649, precision: 0.9059, recall: 0.8912\n",
      "2018-12-27T14:18:13.128046, step: 1082, loss: 0.29521262645721436, acc: 0.8438, auc: 0.9573, precision: 0.8602, recall: 0.8454\n",
      "2018-12-27T14:18:13.272201, step: 1083, loss: 0.273188054561615, acc: 0.9219, auc: 0.9735, precision: 0.9214, recall: 0.9239\n",
      "2018-12-27T14:18:13.397528, step: 1084, loss: 0.4069131314754486, acc: 0.7656, auc: 0.9422, precision: 0.8515, recall: 0.7368\n",
      "2018-12-27T14:18:13.533359, step: 1085, loss: 0.36115437746047974, acc: 0.8672, auc: 0.9382, precision: 0.8668, recall: 0.8681\n",
      "start training model\n",
      "2018-12-27T14:18:13.875293, step: 1086, loss: 0.14004634320735931, acc: 0.9375, auc: 0.9966, precision: 0.9481, recall: 0.9322\n",
      "2018-12-27T14:18:14.004902, step: 1087, loss: 0.1113031804561615, acc: 0.9766, auc: 0.9995, precision: 0.976, recall: 0.977\n",
      "2018-12-27T14:18:14.122550, step: 1088, loss: 0.11321426928043365, acc: 0.9688, auc: 0.9983, precision: 0.9702, recall: 0.967\n",
      "2018-12-27T14:18:14.253213, step: 1089, loss: 0.07929757237434387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:14.379505, step: 1090, loss: 0.09831798076629639, acc: 0.9688, auc: 0.9988, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:18:14.510461, step: 1091, loss: 0.09243865311145782, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:18:14.639586, step: 1092, loss: 0.06267984211444855, acc: 0.9766, auc: 1.0, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:18:14.772473, step: 1093, loss: 0.07135698944330215, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:18:14.900579, step: 1094, loss: 0.11708689481019974, acc: 0.9531, auc: 0.9935, precision: 0.952, recall: 0.952\n",
      "2018-12-27T14:18:15.022598, step: 1095, loss: 0.14793020486831665, acc: 0.9375, auc: 0.9892, precision: 0.9399, recall: 0.9365\n",
      "2018-12-27T14:18:15.146597, step: 1096, loss: 0.08653322607278824, acc: 0.9844, auc: 0.9936, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:15.271963, step: 1097, loss: 0.07733407616615295, acc: 0.9688, auc: 0.9988, precision: 0.9682, recall: 0.9696\n",
      "2018-12-27T14:18:15.399420, step: 1098, loss: 0.1117081344127655, acc: 0.9688, auc: 0.9973, precision: 0.9667, recall: 0.9722\n",
      "2018-12-27T14:18:15.535087, step: 1099, loss: 0.2027808129787445, acc: 0.8828, auc: 0.9998, precision: 0.9085, recall: 0.877\n",
      "2018-12-27T14:18:15.673929, step: 1100, loss: 0.3402298092842102, acc: 0.8516, auc: 0.9998, precision: 0.8827, recall: 0.8561\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:18:20.551614, step: 1100, loss: 0.9836994644842649, acc: 0.5734026315789474, auc: 0.7367763157894738, precision: 0.6900763157894736, recall: 0.5781552631578947\n",
      "2018-12-27T14:18:20.688079, step: 1101, loss: 0.25624150037765503, acc: 0.7969, auc: 0.9978, precision: 0.8617, recall: 0.7833\n",
      "2018-12-27T14:18:20.815186, step: 1102, loss: 0.08844467997550964, acc: 0.9922, auc: 0.9985, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:18:20.941822, step: 1103, loss: 0.10342543572187424, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:18:21.066821, step: 1104, loss: 0.10861761122941971, acc: 0.9453, auc: 0.9995, precision: 0.9557, recall: 0.9375\n",
      "2018-12-27T14:18:21.190976, step: 1105, loss: 0.07146990299224854, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:18:21.314236, step: 1106, loss: 0.08722728490829468, acc: 0.9688, auc: 0.9983, precision: 0.9701, recall: 0.9673\n",
      "2018-12-27T14:18:21.437260, step: 1107, loss: 0.06282032281160355, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:21.561845, step: 1108, loss: 0.06948233395814896, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:18:21.700659, step: 1109, loss: 0.0950303003191948, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:21.826112, step: 1110, loss: 0.09076504409313202, acc: 0.9609, auc: 0.9971, precision: 0.9625, recall: 0.9602\n",
      "2018-12-27T14:18:21.952861, step: 1111, loss: 0.08582916855812073, acc: 0.9531, auc: 1.0, precision: 0.9492, recall: 0.96\n",
      "2018-12-27T14:18:22.076745, step: 1112, loss: 0.12508779764175415, acc: 0.9609, auc: 0.9995, precision: 0.9632, recall: 0.9615\n",
      "2018-12-27T14:18:22.199281, step: 1113, loss: 0.1312185674905777, acc: 0.8984, auc: 1.0, precision: 0.9071, recall: 0.9085\n",
      "2018-12-27T14:18:22.324182, step: 1114, loss: 0.11210456490516663, acc: 0.9688, auc: 0.9985, precision: 0.9677, recall: 0.9714\n",
      "2018-12-27T14:18:22.462440, step: 1115, loss: 0.09012051671743393, acc: 0.9609, auc: 0.9992, precision: 0.9576, recall: 0.9662\n",
      "2018-12-27T14:18:22.587299, step: 1116, loss: 0.13112175464630127, acc: 0.9531, auc: 0.998, precision: 0.9524, recall: 0.9577\n",
      "2018-12-27T14:18:22.715735, step: 1117, loss: 0.1504185050725937, acc: 0.8984, auc: 0.9948, precision: 0.9058, recall: 0.9097\n",
      "2018-12-27T14:18:22.849150, step: 1118, loss: 0.1135002002120018, acc: 0.9609, auc: 0.9995, precision: 0.9667, recall: 0.9569\n",
      "2018-12-27T14:18:22.991188, step: 1119, loss: 0.2652972638607025, acc: 0.9297, auc: 0.9568, precision: 0.9384, recall: 0.9297\n",
      "2018-12-27T14:18:23.131455, step: 1120, loss: 0.22123309969902039, acc: 0.9062, auc: 0.9699, precision: 0.906, recall: 0.9068\n",
      "2018-12-27T14:18:23.262102, step: 1121, loss: 0.15114086866378784, acc: 0.9297, auc: 0.9944, precision: 0.9378, recall: 0.925\n",
      "2018-12-27T14:18:23.399951, step: 1122, loss: 0.1563400775194168, acc: 0.9453, auc: 0.9941, precision: 0.948, recall: 0.9453\n",
      "2018-12-27T14:18:23.525325, step: 1123, loss: 0.1883808970451355, acc: 0.8984, auc: 0.9983, precision: 0.9122, recall: 0.903\n",
      "2018-12-27T14:18:23.648240, step: 1124, loss: 0.17637810111045837, acc: 0.9531, auc: 0.9976, precision: 0.9589, recall: 0.9508\n",
      "2018-12-27T14:18:23.766488, step: 1125, loss: 0.1058981642127037, acc: 0.9375, auc: 0.998, precision: 0.9429, recall: 0.9394\n",
      "2018-12-27T14:18:23.897686, step: 1126, loss: 0.10548888891935349, acc: 0.9688, auc: 0.9958, precision: 0.9701, recall: 0.9673\n",
      "2018-12-27T14:18:24.031122, step: 1127, loss: 0.11455097794532776, acc: 0.9688, auc: 0.9937, precision: 0.975, recall: 0.9615\n",
      "2018-12-27T14:18:24.170910, step: 1128, loss: 0.07543949782848358, acc: 0.9922, auc: 0.9995, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:18:24.297708, step: 1129, loss: 0.12568354606628418, acc: 0.9453, auc: 0.9947, precision: 0.9518, recall: 0.9377\n",
      "2018-12-27T14:18:24.434946, step: 1130, loss: 0.05858943983912468, acc: 0.9922, auc: 0.9993, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:18:24.579553, step: 1131, loss: 0.10734006017446518, acc: 0.9766, auc: 0.9907, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:18:24.719896, step: 1132, loss: 0.1008208841085434, acc: 0.9531, auc: 0.9953, precision: 0.957, recall: 0.9498\n",
      "2018-12-27T14:18:24.863275, step: 1133, loss: 0.08467552065849304, acc: 0.9766, auc: 0.996, precision: 0.9727, recall: 0.9803\n",
      "2018-12-27T14:18:25.008799, step: 1134, loss: 0.07824823260307312, acc: 0.9844, auc: 0.9995, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:18:25.179273, step: 1135, loss: 0.1844860166311264, acc: 0.875, auc: 1.0, precision: 0.8933, recall: 0.8841\n",
      "2018-12-27T14:18:25.302618, step: 1136, loss: 0.3874765634536743, acc: 0.8594, auc: 0.9949, precision: 0.8916, recall: 0.8571\n",
      "2018-12-27T14:18:25.425785, step: 1137, loss: 0.22177955508232117, acc: 0.8359, auc: 0.9912, precision: 0.8793, recall: 0.8306\n",
      "2018-12-27T14:18:25.550274, step: 1138, loss: 0.30768778920173645, acc: 0.875, auc: 0.971, precision: 0.8871, recall: 0.8681\n",
      "2018-12-27T14:18:25.682737, step: 1139, loss: 0.16043764352798462, acc: 0.9062, auc: 0.9985, precision: 0.9178, recall: 0.9104\n",
      "2018-12-27T14:18:25.818613, step: 1140, loss: 0.1542626917362213, acc: 0.9609, auc: 0.9902, precision: 0.9631, recall: 0.9593\n",
      "2018-12-27T14:18:25.951341, step: 1141, loss: 0.11167847365140915, acc: 0.9688, auc: 0.9936, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:18:26.085521, step: 1142, loss: 0.0736924335360527, acc: 0.9609, auc: 0.9998, precision: 0.9648, recall: 0.9597\n",
      "2018-12-27T14:18:26.222737, step: 1143, loss: 0.10355514287948608, acc: 0.9766, auc: 0.9993, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:18:26.360265, step: 1144, loss: 0.22110138833522797, acc: 0.8828, auc: 0.9856, precision: 0.9013, recall: 0.8881\n",
      "2018-12-27T14:18:26.498123, step: 1145, loss: 0.17272116243839264, acc: 0.9297, auc: 0.9978, precision: 0.9408, recall: 0.9262\n",
      "2018-12-27T14:18:26.627684, step: 1146, loss: 0.11383562535047531, acc: 0.9297, auc: 0.9973, precision: 0.9355, recall: 0.9288\n",
      "2018-12-27T14:18:26.758507, step: 1147, loss: 0.07447590678930283, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:26.899065, step: 1148, loss: 0.08365820348262787, acc: 0.9688, auc: 0.9998, precision: 0.9706, recall: 0.9688\n",
      "2018-12-27T14:18:27.007102, step: 1149, loss: 0.09646627306938171, acc: 0.9688, auc: 0.9971, precision: 0.9686, recall: 0.9686\n",
      "2018-12-27T14:18:27.127528, step: 1150, loss: 0.06855016946792603, acc: 0.9688, auc: 0.9995, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:18:27.251894, step: 1151, loss: 0.06465082615613937, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:27.383512, step: 1152, loss: 0.10932973027229309, acc: 0.9688, auc: 0.994, precision: 0.968, recall: 0.968\n",
      "2018-12-27T14:18:27.507463, step: 1153, loss: 0.08491364866495132, acc: 0.9844, auc: 0.9998, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:18:27.631680, step: 1154, loss: 0.0784558355808258, acc: 0.9531, auc: 0.9977, precision: 0.9585, recall: 0.9445\n",
      "2018-12-27T14:18:27.765139, step: 1155, loss: 0.07196222990751266, acc: 0.9844, auc: 0.9993, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:18:27.910266, step: 1156, loss: 0.06654045730829239, acc: 0.9844, auc: 0.9993, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:18:28.032094, step: 1157, loss: 0.10881748795509338, acc: 0.9766, auc: 0.9948, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:18:28.161215, step: 1158, loss: 0.12909278273582458, acc: 0.9297, auc: 0.999, precision: 0.9308, recall: 0.9375\n",
      "2018-12-27T14:18:28.292602, step: 1159, loss: 0.3176150918006897, acc: 0.9141, auc: 0.9875, precision: 0.9113, recall: 0.9286\n",
      "2018-12-27T14:18:28.416251, step: 1160, loss: 0.17958253622055054, acc: 0.8906, auc: 0.9993, precision: 0.9176, recall: 0.8772\n",
      "2018-12-27T14:18:28.549982, step: 1161, loss: 0.09849691390991211, acc: 0.9766, auc: 0.9971, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:18:28.682884, step: 1162, loss: 0.16724126040935516, acc: 0.9297, auc: 0.9837, precision: 0.9291, recall: 0.9328\n",
      "2018-12-27T14:18:28.821550, step: 1163, loss: 0.0955478623509407, acc: 0.9766, auc: 0.998, precision: 0.977, recall: 0.9761\n",
      "2018-12-27T14:18:28.944451, step: 1164, loss: 0.10257115960121155, acc: 0.9375, auc: 0.9978, precision: 0.95, recall: 0.9286\n",
      "2018-12-27T14:18:29.090435, step: 1165, loss: 0.1120072454214096, acc: 0.9766, auc: 0.9941, precision: 0.9742, recall: 0.9772\n",
      "2018-12-27T14:18:29.229541, step: 1166, loss: 0.1602557748556137, acc: 0.9141, auc: 0.9897, precision: 0.9232, recall: 0.9129\n",
      "2018-12-27T14:18:29.366452, step: 1167, loss: 0.16305235028266907, acc: 0.9609, auc: 0.9858, precision: 0.9623, recall: 0.9606\n",
      "2018-12-27T14:18:29.494262, step: 1168, loss: 0.10514620691537857, acc: 0.9453, auc: 0.9949, precision: 0.9498, recall: 0.9426\n",
      "2018-12-27T14:18:29.630353, step: 1169, loss: 0.08808428049087524, acc: 0.9922, auc: 0.9995, precision: 0.9921, recall: 0.9924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:18:29.753399, step: 1170, loss: 0.12974849343299866, acc: 0.9219, auc: 0.9998, precision: 0.9367, recall: 0.9153\n",
      "2018-12-27T14:18:29.885417, step: 1171, loss: 0.15350842475891113, acc: 0.9531, auc: 0.998, precision: 0.9516, recall: 0.9583\n",
      "2018-12-27T14:18:30.018303, step: 1172, loss: 0.12959173321723938, acc: 0.9453, auc: 0.9958, precision: 0.9444, recall: 0.9514\n",
      "2018-12-27T14:18:30.161658, step: 1173, loss: 0.09513982385396957, acc: 0.9844, auc: 0.9985, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:18:30.290355, step: 1174, loss: 0.08918414264917374, acc: 0.9531, auc: 0.9988, precision: 0.9538, recall: 0.9565\n",
      "2018-12-27T14:18:30.413681, step: 1175, loss: 0.1479164958000183, acc: 0.9609, auc: 0.9924, precision: 0.9602, recall: 0.9625\n",
      "2018-12-27T14:18:30.545440, step: 1176, loss: 0.17227914929389954, acc: 0.9062, auc: 0.9929, precision: 0.9211, recall: 0.9062\n",
      "2018-12-27T14:18:30.676185, step: 1177, loss: 0.17028744518756866, acc: 0.9453, auc: 0.9938, precision: 0.9462, recall: 0.95\n",
      "2018-12-27T14:18:30.798164, step: 1178, loss: 0.14640343189239502, acc: 0.9453, auc: 0.9927, precision: 0.9527, recall: 0.9426\n",
      "2018-12-27T14:18:30.921654, step: 1179, loss: 0.1541566103696823, acc: 0.9688, auc: 0.9875, precision: 0.9686, recall: 0.9686\n",
      "2018-12-27T14:18:31.048871, step: 1180, loss: 0.12167293578386307, acc: 0.9609, auc: 0.9951, precision: 0.9625, recall: 0.9602\n",
      "2018-12-27T14:18:31.184144, step: 1181, loss: 0.08799569308757782, acc: 0.9688, auc: 0.9968, precision: 0.9688, recall: 0.9688\n",
      "2018-12-27T14:18:31.311532, step: 1182, loss: 0.11665172129869461, acc: 0.9453, auc: 0.9934, precision: 0.9494, recall: 0.9434\n",
      "2018-12-27T14:18:31.435171, step: 1183, loss: 0.13848285377025604, acc: 0.9297, auc: 0.9939, precision: 0.9242, recall: 0.9316\n",
      "2018-12-27T14:18:31.567549, step: 1184, loss: 0.18142832815647125, acc: 0.9219, auc: 0.991, precision: 0.9324, recall: 0.9219\n",
      "2018-12-27T14:18:31.703882, step: 1185, loss: 0.15358451008796692, acc: 0.9531, auc: 0.9963, precision: 0.9605, recall: 0.9483\n",
      "2018-12-27T14:18:31.833211, step: 1186, loss: 0.11787550151348114, acc: 0.9531, auc: 0.9931, precision: 0.9536, recall: 0.9545\n",
      "2018-12-27T14:18:31.972308, step: 1187, loss: 0.1501299887895584, acc: 0.9297, auc: 0.9902, precision: 0.9309, recall: 0.9293\n",
      "2018-12-27T14:18:32.117153, step: 1188, loss: 0.13911354541778564, acc: 0.9062, auc: 0.9928, precision: 0.9221, recall: 0.8948\n",
      "2018-12-27T14:18:32.257545, step: 1189, loss: 0.1206093430519104, acc: 0.9609, auc: 0.9985, precision: 0.9684, recall: 0.9537\n",
      "2018-12-27T14:18:32.387839, step: 1190, loss: 0.17534534633159637, acc: 0.9219, auc: 0.9863, precision: 0.9251, recall: 0.9226\n",
      "2018-12-27T14:18:32.513766, step: 1191, loss: 0.09057776629924774, acc: 0.9609, auc: 0.998, precision: 0.9609, recall: 0.9638\n",
      "2018-12-27T14:18:32.637015, step: 1192, loss: 0.11704666167497635, acc: 0.9609, auc: 0.999, precision: 0.9554, recall: 0.9675\n",
      "2018-12-27T14:18:32.775033, step: 1193, loss: 0.14900359511375427, acc: 0.9219, auc: 0.9988, precision: 0.9231, recall: 0.9315\n",
      "2018-12-27T14:18:32.913663, step: 1194, loss: 0.11950983107089996, acc: 0.9453, auc: 0.9978, precision: 0.948, recall: 0.9453\n",
      "2018-12-27T14:18:33.041881, step: 1195, loss: 0.10201653838157654, acc: 0.9453, auc: 0.9968, precision: 0.9507, recall: 0.9453\n",
      "2018-12-27T14:18:33.171100, step: 1196, loss: 0.1557317078113556, acc: 0.9453, auc: 0.9926, precision: 0.9453, recall: 0.948\n",
      "2018-12-27T14:18:33.296919, step: 1197, loss: 0.3298395574092865, acc: 0.8125, auc: 0.9792, precision: 0.8588, recall: 0.8209\n",
      "2018-12-27T14:18:33.426625, step: 1198, loss: 0.2157590240240097, acc: 0.9453, auc: 0.9887, precision: 0.9494, recall: 0.9434\n",
      "2018-12-27T14:18:33.564714, step: 1199, loss: 0.1406121402978897, acc: 0.9297, auc: 0.9931, precision: 0.9382, recall: 0.9239\n",
      "2018-12-27T14:18:33.695295, step: 1200, loss: 0.11890004575252533, acc: 0.9766, auc: 0.9929, precision: 0.9767, recall: 0.9766\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:18:38.565709, step: 1200, loss: 0.8348817244956368, acc: 0.6718684210526313, auc: 0.7332657894736844, precision: 0.6773894736842103, recall: 0.6722263157894738\n",
      "2018-12-27T14:18:38.702522, step: 1201, loss: 0.09629583358764648, acc: 0.9609, auc: 0.9975, precision: 0.9598, recall: 0.9628\n",
      "2018-12-27T14:18:38.836281, step: 1202, loss: 0.06386054307222366, acc: 0.9844, auc: 0.999, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:18:38.964612, step: 1203, loss: 0.08722659200429916, acc: 0.9766, auc: 0.9976, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:18:39.088091, step: 1204, loss: 0.17673571407794952, acc: 0.9375, auc: 0.9855, precision: 0.937, recall: 0.9396\n",
      "2018-12-27T14:18:39.220213, step: 1205, loss: 0.10220394283533096, acc: 0.9453, auc: 0.999, precision: 0.9435, recall: 0.9521\n",
      "2018-12-27T14:18:39.344815, step: 1206, loss: 0.11791780591011047, acc: 0.9844, auc: 0.9971, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:18:39.464882, step: 1207, loss: 0.16801519691944122, acc: 0.8984, auc: 0.9939, precision: 0.9123, recall: 0.8957\n",
      "2018-12-27T14:18:39.589252, step: 1208, loss: 0.20912501215934753, acc: 0.9297, auc: 0.9939, precision: 0.9366, recall: 0.9318\n",
      "2018-12-27T14:18:39.719356, step: 1209, loss: 0.13216283917427063, acc: 0.9141, auc: 0.9961, precision: 0.9276, recall: 0.9127\n",
      "2018-12-27T14:18:39.858133, step: 1210, loss: 0.13988392055034637, acc: 0.9219, auc: 0.988, precision: 0.9248, recall: 0.9189\n",
      "2018-12-27T14:18:39.981511, step: 1211, loss: 0.09378832578659058, acc: 0.9609, auc: 0.997, precision: 0.9593, recall: 0.9631\n",
      "2018-12-27T14:18:40.104512, step: 1212, loss: 0.1225663423538208, acc: 0.9609, auc: 0.9945, precision: 0.9584, recall: 0.9635\n",
      "2018-12-27T14:18:40.231227, step: 1213, loss: 0.09911186993122101, acc: 0.9375, auc: 0.9961, precision: 0.9375, recall: 0.9392\n",
      "2018-12-27T14:18:40.366030, step: 1214, loss: 0.13447017967700958, acc: 0.9453, auc: 0.9891, precision: 0.9446, recall: 0.9418\n",
      "2018-12-27T14:18:40.492605, step: 1215, loss: 0.10339665412902832, acc: 0.9453, auc: 0.9966, precision: 0.9507, recall: 0.9453\n",
      "2018-12-27T14:18:40.629560, step: 1216, loss: 0.052646804600954056, acc: 0.9766, auc: 0.9995, precision: 0.9756, recall: 0.9771\n",
      "2018-12-27T14:18:40.754725, step: 1217, loss: 0.11481237411499023, acc: 0.9688, auc: 0.9929, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:18:40.879210, step: 1218, loss: 0.10603825747966766, acc: 0.9375, auc: 0.9949, precision: 0.9405, recall: 0.9353\n",
      "2018-12-27T14:18:41.012820, step: 1219, loss: 0.0779063031077385, acc: 0.9688, auc: 0.9985, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:18:41.138232, step: 1220, loss: 0.08597066253423691, acc: 0.9531, auc: 0.9985, precision: 0.9559, recall: 0.9545\n",
      "2018-12-27T14:18:41.270374, step: 1221, loss: 0.15018127858638763, acc: 0.9609, auc: 0.9995, precision: 0.959, recall: 0.9653\n",
      "2018-12-27T14:18:41.396159, step: 1222, loss: 0.27068406343460083, acc: 0.8828, auc: 0.9934, precision: 0.9051, recall: 0.8828\n",
      "2018-12-27T14:18:41.519111, step: 1223, loss: 0.2480825036764145, acc: 0.8984, auc: 0.999, precision: 0.9122, recall: 0.903\n",
      "2018-12-27T14:18:41.641387, step: 1224, loss: 0.16454088687896729, acc: 0.9062, auc: 0.9963, precision: 0.9104, recall: 0.9178\n",
      "2018-12-27T14:18:41.765798, step: 1225, loss: 0.13151517510414124, acc: 0.9688, auc: 0.995, precision: 0.9666, recall: 0.9704\n",
      "2018-12-27T14:18:41.902565, step: 1226, loss: 0.13503655791282654, acc: 0.9219, auc: 0.9946, precision: 0.9246, recall: 0.9263\n",
      "2018-12-27T14:18:42.027166, step: 1227, loss: 0.11422916501760483, acc: 0.9688, auc: 0.9966, precision: 0.9688, recall: 0.9692\n",
      "2018-12-27T14:18:42.174779, step: 1228, loss: 0.11254669725894928, acc: 0.9297, auc: 0.9956, precision: 0.9367, recall: 0.927\n",
      "2018-12-27T14:18:42.309528, step: 1229, loss: 0.07578611373901367, acc: 0.9766, auc: 0.9985, precision: 0.977, recall: 0.9761\n",
      "2018-12-27T14:18:42.433044, step: 1230, loss: 0.13001619279384613, acc: 0.9219, auc: 0.9921, precision: 0.9219, recall: 0.9256\n",
      "2018-12-27T14:18:42.569598, step: 1231, loss: 0.18615996837615967, acc: 0.9297, auc: 0.9875, precision: 0.9328, recall: 0.9291\n",
      "2018-12-27T14:18:42.718685, step: 1232, loss: 0.19278670847415924, acc: 0.8984, auc: 0.9887, precision: 0.9046, recall: 0.9057\n",
      "2018-12-27T14:18:42.851552, step: 1233, loss: 0.24044395983219147, acc: 0.9297, auc: 0.9735, precision: 0.9372, recall: 0.926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:18:42.982790, step: 1234, loss: 0.16069626808166504, acc: 0.9297, auc: 0.9898, precision: 0.9288, recall: 0.9355\n",
      "2018-12-27T14:18:43.123517, step: 1235, loss: 0.20259076356887817, acc: 0.9219, auc: 0.9922, precision: 0.9245, recall: 0.9233\n",
      "2018-12-27T14:18:43.252112, step: 1236, loss: 0.180564284324646, acc: 0.8984, auc: 0.9978, precision: 0.9044, recall: 0.911\n",
      "2018-12-27T14:18:43.381425, step: 1237, loss: 0.12199750542640686, acc: 0.9688, auc: 0.9895, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:18:43.505932, step: 1238, loss: 0.08149851113557816, acc: 0.9453, auc: 0.9987, precision: 0.9417, recall: 0.9533\n",
      "2018-12-27T14:18:43.629086, step: 1239, loss: 0.0987354964017868, acc: 0.9609, auc: 0.9971, precision: 0.9607, recall: 0.9612\n",
      "2018-12-27T14:18:43.769713, step: 1240, loss: 0.07751232385635376, acc: 0.9688, auc: 0.9985, precision: 0.9676, recall: 0.97\n",
      "start training model\n",
      "2018-12-27T14:18:44.055947, step: 1241, loss: 0.06576905399560928, acc: 0.9688, auc: 0.9993, precision: 0.9692, recall: 0.9701\n",
      "2018-12-27T14:18:44.187743, step: 1242, loss: 0.0468125082552433, acc: 0.9766, auc: 1.0, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:18:44.325792, step: 1243, loss: 0.028131743893027306, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:44.472732, step: 1244, loss: 0.033548347651958466, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:18:44.607934, step: 1245, loss: 0.02231588587164879, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:44.733515, step: 1246, loss: 0.030882617458701134, acc: 0.9922, auc: 1.0, precision: 0.9934, recall: 0.9906\n",
      "2018-12-27T14:18:44.858215, step: 1247, loss: 0.018120571970939636, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:44.984331, step: 1248, loss: 0.03854171559214592, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:18:45.112863, step: 1249, loss: 0.03005043789744377, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:18:45.237583, step: 1250, loss: 0.050192274153232574, acc: 0.9922, auc: 0.9993, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:18:45.365387, step: 1251, loss: 0.02760961651802063, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:18:45.503449, step: 1252, loss: 0.015728769823908806, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:45.639784, step: 1253, loss: 0.028698131442070007, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:18:45.761760, step: 1254, loss: 0.06542946398258209, acc: 0.9766, auc: 0.9985, precision: 0.976, recall: 0.977\n",
      "2018-12-27T14:18:45.903771, step: 1255, loss: 0.06132054701447487, acc: 0.9922, auc: 0.9998, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:18:46.026299, step: 1256, loss: 0.09220080077648163, acc: 0.9453, auc: 1.0, precision: 0.9485, recall: 0.9478\n",
      "2018-12-27T14:18:46.168115, step: 1257, loss: 0.09792183339595795, acc: 0.9844, auc: 0.9983, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:18:46.305926, step: 1258, loss: 0.0590975321829319, acc: 0.9766, auc: 0.9993, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:18:46.433309, step: 1259, loss: 0.04353755712509155, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:18:46.567983, step: 1260, loss: 0.01982157677412033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:46.706729, step: 1261, loss: 0.03706427291035652, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:18:46.853093, step: 1262, loss: 0.07562815397977829, acc: 0.9766, auc: 0.997, precision: 0.9771, recall: 0.9756\n",
      "2018-12-27T14:18:46.977195, step: 1263, loss: 0.08465243130922318, acc: 0.9609, auc: 0.9954, precision: 0.9611, recall: 0.9608\n",
      "2018-12-27T14:18:47.100355, step: 1264, loss: 0.05329785868525505, acc: 0.9766, auc: 0.999, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:18:47.241583, step: 1265, loss: 0.07457587122917175, acc: 0.9844, auc: 0.9968, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:47.372724, step: 1266, loss: 0.03526192158460617, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:18:47.500642, step: 1267, loss: 0.054407838732004166, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:18:47.627470, step: 1268, loss: 0.12971344590187073, acc: 0.9375, auc: 1.0, precision: 0.9518, recall: 0.9245\n",
      "2018-12-27T14:18:47.757776, step: 1269, loss: 0.07719965279102325, acc: 0.9844, auc: 0.9998, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:18:47.892334, step: 1270, loss: 0.0363459512591362, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:18:48.019993, step: 1271, loss: 0.04157007485628128, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:18:48.145728, step: 1272, loss: 0.07733407616615295, acc: 0.9453, auc: 0.9958, precision: 0.945, recall: 0.9456\n",
      "2018-12-27T14:18:48.290215, step: 1273, loss: 0.031763654202222824, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:48.441856, step: 1274, loss: 0.06049523130059242, acc: 0.9688, auc: 0.999, precision: 0.9688, recall: 0.9706\n",
      "2018-12-27T14:18:48.567515, step: 1275, loss: 0.07705508917570114, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:18:48.691551, step: 1276, loss: 0.1113915890455246, acc: 0.9375, auc: 1.0, precision: 0.9459, recall: 0.9355\n",
      "2018-12-27T14:18:48.813886, step: 1277, loss: 0.053498879075050354, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:18:48.946171, step: 1278, loss: 0.04315802454948425, acc: 0.9766, auc: 1.0, precision: 0.9762, recall: 0.9779\n",
      "2018-12-27T14:18:49.080350, step: 1279, loss: 0.03164558485150337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:49.203686, step: 1280, loss: 0.06525147706270218, acc: 0.9844, auc: 0.9956, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:18:49.326730, step: 1281, loss: 0.05237840116024017, acc: 0.9688, auc: 0.9995, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:18:49.463494, step: 1282, loss: 0.04742170125246048, acc: 0.9922, auc: 0.9995, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:18:49.602650, step: 1283, loss: 0.03987089917063713, acc: 0.9922, auc: 0.9998, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:18:49.743718, step: 1284, loss: 0.05056970566511154, acc: 0.9688, auc: 0.9993, precision: 0.9706, recall: 0.9659\n",
      "2018-12-27T14:18:49.892291, step: 1285, loss: 0.039004214107990265, acc: 0.9844, auc: 0.9995, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:18:50.028307, step: 1286, loss: 0.029735993593931198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:50.167988, step: 1287, loss: 0.028157003223896027, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:50.303358, step: 1288, loss: 0.05691500008106232, acc: 0.9844, auc: 0.9985, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:18:50.443840, step: 1289, loss: 0.025470618158578873, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:18:50.579630, step: 1290, loss: 0.03989812731742859, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:18:50.732254, step: 1291, loss: 0.04066137969493866, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:18:50.865828, step: 1292, loss: 0.13164851069450378, acc: 0.9609, auc: 0.9998, precision: 0.9603, recall: 0.9643\n",
      "2018-12-27T14:18:51.002526, step: 1293, loss: 0.3628336191177368, acc: 0.8828, auc: 0.9888, precision: 0.9012, recall: 0.8795\n",
      "2018-12-27T14:18:51.128966, step: 1294, loss: 0.030858434736728668, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:18:51.253975, step: 1295, loss: 0.0461675263941288, acc: 0.9922, auc: 0.999, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:18:51.399924, step: 1296, loss: 0.07504088431596756, acc: 0.9531, auc: 0.9998, precision: 0.9524, recall: 0.9577\n",
      "2018-12-27T14:18:51.538442, step: 1297, loss: 0.103732168674469, acc: 0.9766, auc: 1.0, precision: 0.975, recall: 0.9789\n",
      "2018-12-27T14:18:51.669074, step: 1298, loss: 0.13021992146968842, acc: 0.9297, auc: 0.9993, precision: 0.943, recall: 0.9224\n",
      "2018-12-27T14:18:51.812828, step: 1299, loss: 0.11599299311637878, acc: 0.9844, auc: 0.9985, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:18:51.950612, step: 1300, loss: 0.10246918350458145, acc: 0.9453, auc: 0.999, precision: 0.9521, recall: 0.9435\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:18:56.635985, step: 1300, loss: 1.0996216959074925, acc: 0.6531684210526315, auc: 0.7233815789473684, precision: 0.6544236842105263, recall: 0.6532894736842106\n",
      "2018-12-27T14:18:56.756869, step: 1301, loss: 0.04259106144309044, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:18:56.886835, step: 1302, loss: 0.04219050705432892, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:18:57.020580, step: 1303, loss: 0.036173220723867416, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:18:57.163709, step: 1304, loss: 0.04789676517248154, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:18:57.303131, step: 1305, loss: 0.023081399500370026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:57.444026, step: 1306, loss: 0.02471758984029293, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:57.573056, step: 1307, loss: 0.02247076854109764, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:57.703577, step: 1308, loss: 0.04028945043683052, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:18:57.831022, step: 1309, loss: 0.05668577179312706, acc: 0.9688, auc: 0.998, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:18:57.975755, step: 1310, loss: 0.0793788954615593, acc: 0.9609, auc: 0.998, precision: 0.9613, recall: 0.9599\n",
      "2018-12-27T14:18:58.112703, step: 1311, loss: 0.06360252946615219, acc: 0.9688, auc: 0.9971, precision: 0.9701, recall: 0.9673\n",
      "2018-12-27T14:18:58.255504, step: 1312, loss: 0.04144749790430069, acc: 0.9766, auc: 1.0, precision: 0.9766, recall: 0.9776\n",
      "2018-12-27T14:18:58.387974, step: 1313, loss: 0.06462249159812927, acc: 0.9844, auc: 0.9993, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:18:58.533774, step: 1314, loss: 0.04480218514800072, acc: 0.9844, auc: 1.0, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:18:58.667957, step: 1315, loss: 0.04787256568670273, acc: 0.9766, auc: 0.9993, precision: 0.9771, recall: 0.9756\n",
      "2018-12-27T14:18:58.818552, step: 1316, loss: 0.044805221259593964, acc: 0.9766, auc: 0.9995, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:18:58.948334, step: 1317, loss: 0.04590345919132233, acc: 0.9766, auc: 0.9993, precision: 0.9763, recall: 0.9769\n",
      "2018-12-27T14:18:59.084519, step: 1318, loss: 0.033862918615341187, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:18:59.231082, step: 1319, loss: 0.07316351681947708, acc: 0.9531, auc: 1.0, precision: 0.9538, recall: 0.9565\n",
      "2018-12-27T14:18:59.364717, step: 1320, loss: 0.2584030330181122, acc: 0.8984, auc: 0.9953, precision: 0.911, recall: 0.9044\n",
      "2018-12-27T14:18:59.493017, step: 1321, loss: 0.22706331312656403, acc: 0.8594, auc: 0.9973, precision: 0.8902, recall: 0.8594\n",
      "2018-12-27T14:18:59.628988, step: 1322, loss: 0.17758400738239288, acc: 0.9375, auc: 0.9912, precision: 0.9408, recall: 0.9382\n",
      "2018-12-27T14:18:59.754315, step: 1323, loss: 0.11781657487154007, acc: 0.9297, auc: 0.998, precision: 0.9328, recall: 0.9357\n",
      "2018-12-27T14:18:59.898490, step: 1324, loss: 0.07158681750297546, acc: 0.9844, auc: 0.999, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:19:00.024938, step: 1325, loss: 0.051850028336048126, acc: 0.9844, auc: 0.9998, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:19:00.151156, step: 1326, loss: 0.025720281526446342, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:19:00.274893, step: 1327, loss: 0.09986130148172379, acc: 0.9688, auc: 0.9944, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:19:00.412339, step: 1328, loss: 0.09901633113622665, acc: 0.9609, auc: 0.9958, precision: 0.9613, recall: 0.9616\n",
      "2018-12-27T14:19:00.550544, step: 1329, loss: 0.09648595750331879, acc: 0.9688, auc: 0.9939, precision: 0.9688, recall: 0.9688\n",
      "2018-12-27T14:19:00.675832, step: 1330, loss: 0.03829817473888397, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:00.800044, step: 1331, loss: 0.024399995803833008, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:00.926760, step: 1332, loss: 0.060267385095357895, acc: 0.9688, auc: 0.998, precision: 0.9698, recall: 0.9679\n",
      "2018-12-27T14:19:01.063294, step: 1333, loss: 0.03552621230483055, acc: 0.9922, auc: 0.9998, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:19:01.192264, step: 1334, loss: 0.039317481219768524, acc: 0.9766, auc: 0.9993, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:19:01.317305, step: 1335, loss: 0.027864011004567146, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:01.449052, step: 1336, loss: 0.028542393818497658, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:01.574436, step: 1337, loss: 0.04081647843122482, acc: 0.9844, auc: 0.9998, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:19:01.710761, step: 1338, loss: 0.05667804181575775, acc: 0.9766, auc: 0.9985, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:19:01.845255, step: 1339, loss: 0.04773368686437607, acc: 0.9766, auc: 0.999, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:19:01.970699, step: 1340, loss: 0.042064398527145386, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:19:02.106719, step: 1341, loss: 0.07785247266292572, acc: 0.9609, auc: 0.9973, precision: 0.9611, recall: 0.9608\n",
      "2018-12-27T14:19:02.239659, step: 1342, loss: 0.18012735247612, acc: 0.8984, auc: 0.9934, precision: 0.9123, recall: 0.8957\n",
      "2018-12-27T14:19:02.377546, step: 1343, loss: 0.36432796716690063, acc: 0.8672, auc: 0.9988, precision: 0.8938, recall: 0.8692\n",
      "2018-12-27T14:19:02.516277, step: 1344, loss: 0.27912548184394836, acc: 0.8672, auc: 0.9988, precision: 0.8938, recall: 0.8692\n",
      "2018-12-27T14:19:02.656288, step: 1345, loss: 0.308932363986969, acc: 0.8984, auc: 0.9707, precision: 0.8986, recall: 0.8983\n",
      "2018-12-27T14:19:02.795963, step: 1346, loss: 0.1754244714975357, acc: 0.8984, auc: 0.9874, precision: 0.9064, recall: 0.8857\n",
      "2018-12-27T14:19:02.912229, step: 1347, loss: 0.06974204629659653, acc: 0.9766, auc: 0.9983, precision: 0.9769, recall: 0.9763\n",
      "2018-12-27T14:19:03.039409, step: 1348, loss: 0.04241722822189331, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:19:03.168241, step: 1349, loss: 0.0653204619884491, acc: 0.9688, auc: 0.9987, precision: 0.9663, recall: 0.9705\n",
      "2018-12-27T14:19:03.327049, step: 1350, loss: 0.04312692582607269, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:03.473176, step: 1351, loss: 0.05580281838774681, acc: 0.9766, auc: 0.9983, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:19:03.606923, step: 1352, loss: 0.06381381303071976, acc: 0.9766, auc: 0.9976, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:19:03.734048, step: 1353, loss: 0.04217875748872757, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:03.865313, step: 1354, loss: 0.07955848425626755, acc: 0.9688, auc: 0.9957, precision: 0.9674, recall: 0.9674\n",
      "2018-12-27T14:19:03.998316, step: 1355, loss: 0.04533999785780907, acc: 0.9844, auc: 0.9993, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:19:04.141954, step: 1356, loss: 0.04448161646723747, acc: 0.9766, auc: 0.9995, precision: 0.9758, recall: 0.9771\n",
      "2018-12-27T14:19:04.301381, step: 1357, loss: 0.0694957748055458, acc: 0.9766, auc: 0.9975, precision: 0.9674, recall: 0.9824\n",
      "2018-12-27T14:19:04.436048, step: 1358, loss: 0.06956657022237778, acc: 0.9766, auc: 1.0, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:19:04.563104, step: 1359, loss: 0.1398789882659912, acc: 0.9375, auc: 0.9918, precision: 0.9368, recall: 0.9419\n",
      "2018-12-27T14:19:04.700458, step: 1360, loss: 0.08360391110181808, acc: 0.9609, auc: 0.9968, precision: 0.9625, recall: 0.9602\n",
      "2018-12-27T14:19:04.836458, step: 1361, loss: 0.03348886966705322, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:19:04.974788, step: 1362, loss: 0.04289916157722473, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:05.111998, step: 1363, loss: 0.019636593759059906, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:19:05.242234, step: 1364, loss: 0.050534456968307495, acc: 0.9922, auc: 0.9998, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:19:05.380456, step: 1365, loss: 0.04515986889600754, acc: 0.9844, auc: 0.9993, precision: 0.9853, recall: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:19:05.514434, step: 1366, loss: 0.04063861817121506, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:19:05.651980, step: 1367, loss: 0.1126214787364006, acc: 0.9453, auc: 0.9937, precision: 0.9523, recall: 0.9357\n",
      "2018-12-27T14:19:05.780394, step: 1368, loss: 0.07638584822416306, acc: 0.9844, auc: 0.9968, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:05.902679, step: 1369, loss: 0.08520674705505371, acc: 0.9531, auc: 0.9995, precision: 0.9565, recall: 0.9538\n",
      "2018-12-27T14:19:06.041194, step: 1370, loss: 0.2533285319805145, acc: 0.9219, auc: 0.9934, precision: 0.9315, recall: 0.9231\n",
      "2018-12-27T14:19:06.188857, step: 1371, loss: 0.19582302868366241, acc: 0.9062, auc: 0.9961, precision: 0.925, recall: 0.9\n",
      "2018-12-27T14:19:06.318629, step: 1372, loss: 0.034966129809617996, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:06.446298, step: 1373, loss: 0.03798942640423775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:06.568572, step: 1374, loss: 0.08405741304159164, acc: 0.9609, auc: 0.9971, precision: 0.9603, recall: 0.9613\n",
      "2018-12-27T14:19:06.700135, step: 1375, loss: 0.03150301054120064, acc: 0.9844, auc: 1.0, precision: 0.9828, recall: 0.9861\n",
      "2018-12-27T14:19:06.830743, step: 1376, loss: 0.04998547583818436, acc: 0.9766, auc: 0.9985, precision: 0.9752, recall: 0.9772\n",
      "2018-12-27T14:19:06.956510, step: 1377, loss: 0.09282718598842621, acc: 0.9531, auc: 0.9975, precision: 0.9542, recall: 0.952\n",
      "2018-12-27T14:19:07.092725, step: 1378, loss: 0.058513712137937546, acc: 0.9609, auc: 1.0, precision: 0.9597, recall: 0.9648\n",
      "2018-12-27T14:19:07.226680, step: 1379, loss: 0.044161535799503326, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:19:07.354073, step: 1380, loss: 0.09976615756750107, acc: 0.9609, auc: 0.9955, precision: 0.9576, recall: 0.9662\n",
      "2018-12-27T14:19:07.478831, step: 1381, loss: 0.0816044881939888, acc: 0.9766, auc: 0.9998, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:19:07.615816, step: 1382, loss: 0.05012691766023636, acc: 0.9766, auc: 0.9985, precision: 0.9763, recall: 0.9769\n",
      "2018-12-27T14:19:07.755764, step: 1383, loss: 0.05936131253838539, acc: 0.9688, auc: 0.9975, precision: 0.9702, recall: 0.967\n",
      "2018-12-27T14:19:07.885826, step: 1384, loss: 0.04778074473142624, acc: 0.9922, auc: 0.9995, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:19:08.011689, step: 1385, loss: 0.05815206840634346, acc: 0.9766, auc: 0.999, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:19:08.148157, step: 1386, loss: 0.06877537071704865, acc: 0.9766, auc: 0.9973, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:19:08.290033, step: 1387, loss: 0.059891313314437866, acc: 0.9688, auc: 0.999, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:19:08.445787, step: 1388, loss: 0.06489831209182739, acc: 0.9766, auc: 0.9983, precision: 0.9756, recall: 0.9771\n",
      "2018-12-27T14:19:08.586312, step: 1389, loss: 0.09593851864337921, acc: 0.9531, auc: 0.9946, precision: 0.9539, recall: 0.9526\n",
      "2018-12-27T14:19:08.711428, step: 1390, loss: 0.13166862726211548, acc: 0.9219, auc: 0.9941, precision: 0.9246, recall: 0.9263\n",
      "2018-12-27T14:19:08.835762, step: 1391, loss: 0.11590655893087387, acc: 0.9453, auc: 0.9995, precision: 0.9478, recall: 0.9485\n",
      "2018-12-27T14:19:08.963768, step: 1392, loss: 0.08680468797683716, acc: 0.9453, auc: 1.0, precision: 0.9514, recall: 0.9444\n",
      "2018-12-27T14:19:09.090374, step: 1393, loss: 0.08898366242647171, acc: 0.9688, auc: 0.9966, precision: 0.9694, recall: 0.9685\n",
      "2018-12-27T14:19:09.235514, step: 1394, loss: 0.03296347334980965, acc: 0.9844, auc: 0.9998, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:19:09.375067, step: 1395, loss: 0.08917263150215149, acc: 0.9766, auc: 0.9968, precision: 0.9779, recall: 0.9762\n",
      "start training model\n",
      "2018-12-27T14:19:09.660274, step: 1396, loss: 0.0473015196621418, acc: 0.9766, auc: 0.9993, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:19:09.791936, step: 1397, loss: 0.02117784693837166, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:09.923351, step: 1398, loss: 0.01185928750783205, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:10.048321, step: 1399, loss: 0.01184802781790495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:10.181296, step: 1400, loss: 0.03185126930475235, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:19:14.987383, step: 1400, loss: 1.1028853498007123, acc: 0.6743447368421053, auc: 0.7405947368421052, precision: 0.6757552631578949, recall: 0.6762131578947369\n",
      "2018-12-27T14:19:15.128570, step: 1401, loss: 0.015390533022582531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:15.267698, step: 1402, loss: 0.01993286795914173, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:15.420404, step: 1403, loss: 0.011817063204944134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:15.556615, step: 1404, loss: 0.047775350511074066, acc: 0.9922, auc: 0.9988, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:19:15.690163, step: 1405, loss: 0.017386168241500854, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:15.828481, step: 1406, loss: 0.015408596023917198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:15.961355, step: 1407, loss: 0.015741892158985138, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:16.112507, step: 1408, loss: 0.02418377622961998, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:16.251225, step: 1409, loss: 0.016419291496276855, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:19:16.397250, step: 1410, loss: 0.032289087772369385, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:19:16.536693, step: 1411, loss: 0.0208229199051857, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:16.673503, step: 1412, loss: 0.05203845724463463, acc: 0.9922, auc: 0.9992, precision: 0.9907, recall: 0.9933\n",
      "2018-12-27T14:19:16.809005, step: 1413, loss: 0.05581621825695038, acc: 0.9922, auc: 0.9977, precision: 0.9933, recall: 0.9907\n",
      "2018-12-27T14:19:16.945776, step: 1414, loss: 0.017260026186704636, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:17.080606, step: 1415, loss: 0.02252749539911747, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:19:17.216507, step: 1416, loss: 0.04323802515864372, acc: 0.9844, auc: 0.9982, precision: 0.9839, recall: 0.9839\n",
      "2018-12-27T14:19:17.340705, step: 1417, loss: 0.009905972518026829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:17.465928, step: 1418, loss: 0.020714109763503075, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:19:17.592165, step: 1419, loss: 0.03473836928606033, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:19:17.714636, step: 1420, loss: 0.10077192634344101, acc: 0.9609, auc: 0.9998, precision: 0.9662, recall: 0.9576\n",
      "2018-12-27T14:19:17.842443, step: 1421, loss: 0.36070576310157776, acc: 0.8672, auc: 0.997, precision: 0.8867, recall: 0.8786\n",
      "2018-12-27T14:19:17.963629, step: 1422, loss: 0.4540361166000366, acc: 0.8672, auc: 0.984, precision: 0.8867, recall: 0.8786\n",
      "2018-12-27T14:19:18.097466, step: 1423, loss: 0.2008492648601532, acc: 0.8828, auc: 0.9966, precision: 0.9051, recall: 0.8828\n",
      "2018-12-27T14:19:18.219409, step: 1424, loss: 0.07179611176252365, acc: 0.9766, auc: 1.0, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:19:18.336438, step: 1425, loss: 0.0891653299331665, acc: 0.9453, auc: 0.9978, precision: 0.9485, recall: 0.9478\n",
      "2018-12-27T14:19:18.450368, step: 1426, loss: 0.08165223896503448, acc: 0.9844, auc: 0.998, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:19:18.562661, step: 1427, loss: 0.02740168198943138, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:19:18.685727, step: 1428, loss: 0.006902269087731838, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:18.811467, step: 1429, loss: 0.02141735889017582, acc: 0.9844, auc: 1.0, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:19:18.951451, step: 1430, loss: 0.05148959532380104, acc: 0.9766, auc: 0.9988, precision: 0.977, recall: 0.976\n",
      "2018-12-27T14:19:19.075735, step: 1431, loss: 0.021016800776124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:19:19.212977, step: 1432, loss: 0.02133847214281559, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:19.349905, step: 1433, loss: 0.02836320549249649, acc: 0.9922, auc: 0.9995, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:19.485979, step: 1434, loss: 0.015672912821173668, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:19.628451, step: 1435, loss: 0.02837279625236988, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:19:19.770056, step: 1436, loss: 0.03917660191655159, acc: 0.9922, auc: 0.9995, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:19:19.897918, step: 1437, loss: 0.008105192333459854, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:20.038269, step: 1438, loss: 0.03332645818591118, acc: 0.9844, auc: 0.9997, precision: 0.9868, recall: 0.9815\n",
      "2018-12-27T14:19:20.161375, step: 1439, loss: 0.012882666662335396, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:20.289285, step: 1440, loss: 0.03859405964612961, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:19:20.423682, step: 1441, loss: 0.06378807127475739, acc: 0.9844, auc: 0.9971, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:20.556384, step: 1442, loss: 0.014649633318185806, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:20.703657, step: 1443, loss: 0.0144517682492733, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:19:20.833826, step: 1444, loss: 0.022983571514487267, acc: 0.9922, auc: 0.9998, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:19:20.946089, step: 1445, loss: 0.04228408634662628, acc: 0.9844, auc: 0.999, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:21.056553, step: 1446, loss: 0.01609136536717415, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:21.187539, step: 1447, loss: 0.01955040730535984, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:21.316554, step: 1448, loss: 0.04263858497142792, acc: 0.9766, auc: 1.0, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:19:21.445739, step: 1449, loss: 0.07759813964366913, acc: 0.9766, auc: 1.0, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:19:21.571725, step: 1450, loss: 0.09942124038934708, acc: 0.9688, auc: 0.9995, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:19:21.703649, step: 1451, loss: 0.08229517936706543, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:19:21.834413, step: 1452, loss: 0.07076915353536606, acc: 0.9531, auc: 0.9998, precision: 0.9565, recall: 0.9538\n",
      "2018-12-27T14:19:21.972625, step: 1453, loss: 0.09269731491804123, acc: 0.9766, auc: 0.9995, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:19:22.120366, step: 1454, loss: 0.3366048336029053, acc: 0.875, auc: 0.9946, precision: 0.8933, recall: 0.8841\n",
      "2018-12-27T14:19:22.248123, step: 1455, loss: 0.1021009162068367, acc: 0.9766, auc: 0.9988, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:19:22.385690, step: 1456, loss: 0.03667811304330826, acc: 0.9922, auc: 0.9998, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:19:22.513311, step: 1457, loss: 0.051687195897102356, acc: 0.9766, auc: 0.9985, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:19:22.637551, step: 1458, loss: 0.04466976970434189, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:19:22.765131, step: 1459, loss: 0.011768907308578491, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:22.911217, step: 1460, loss: 0.02675887569785118, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:19:23.054898, step: 1461, loss: 0.03430020064115524, acc: 0.9766, auc: 0.9993, precision: 0.977, recall: 0.9761\n",
      "2018-12-27T14:19:23.178368, step: 1462, loss: 0.07726934552192688, acc: 0.9688, auc: 0.9968, precision: 0.9688, recall: 0.9692\n",
      "2018-12-27T14:19:23.305963, step: 1463, loss: 0.06114336848258972, acc: 0.9922, auc: 0.9958, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:19:23.449506, step: 1464, loss: 0.033913787454366684, acc: 0.9766, auc: 0.9995, precision: 0.9772, recall: 0.9745\n",
      "2018-12-27T14:19:23.586040, step: 1465, loss: 0.021588822826743126, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:19:23.711608, step: 1466, loss: 0.02344934269785881, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:19:23.835697, step: 1467, loss: 0.01051600743085146, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:23.980164, step: 1468, loss: 0.02529257908463478, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:19:24.103725, step: 1469, loss: 0.061450034379959106, acc: 0.9922, auc: 0.9983, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:24.228912, step: 1470, loss: 0.03431890904903412, acc: 0.9922, auc: 0.9992, precision: 0.9938, recall: 0.9896\n",
      "2018-12-27T14:19:24.356439, step: 1471, loss: 0.06782643496990204, acc: 0.9688, auc: 0.999, precision: 0.974, recall: 0.9636\n",
      "2018-12-27T14:19:24.487080, step: 1472, loss: 0.06931885331869125, acc: 0.9766, auc: 0.9988, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:19:24.614101, step: 1473, loss: 0.06188531965017319, acc: 0.9766, auc: 0.9993, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:19:24.760801, step: 1474, loss: 0.055397290736436844, acc: 0.9688, auc: 0.9993, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:19:24.887691, step: 1475, loss: 0.08933605998754501, acc: 0.9766, auc: 0.999, precision: 0.9789, recall: 0.975\n",
      "2018-12-27T14:19:25.017259, step: 1476, loss: 0.04236813634634018, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:25.153656, step: 1477, loss: 0.05564386770129204, acc: 0.9688, auc: 0.9992, precision: 0.9649, recall: 0.9733\n",
      "2018-12-27T14:19:25.289256, step: 1478, loss: 0.04951898753643036, acc: 0.9766, auc: 0.9998, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:19:25.433142, step: 1479, loss: 0.09449124336242676, acc: 0.9609, auc: 0.9966, precision: 0.9631, recall: 0.9593\n",
      "2018-12-27T14:19:25.565988, step: 1480, loss: 0.06035633012652397, acc: 0.9844, auc: 0.9971, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:25.702706, step: 1481, loss: 0.03269663080573082, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:19:25.827809, step: 1482, loss: 0.028871828690171242, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:25.951833, step: 1483, loss: 0.010987084358930588, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:26.076328, step: 1484, loss: 0.05996600538492203, acc: 0.9844, auc: 0.998, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:19:26.200217, step: 1485, loss: 0.05113035812973976, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:26.325617, step: 1486, loss: 0.02888641506433487, acc: 0.9844, auc: 1.0, precision: 0.987, recall: 0.9811\n",
      "2018-12-27T14:19:26.449298, step: 1487, loss: 0.03271637111902237, acc: 0.9844, auc: 0.999, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:19:26.584610, step: 1488, loss: 0.023468181490898132, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:19:26.765477, step: 1489, loss: 0.0839492678642273, acc: 0.9609, auc: 0.9985, precision: 0.9598, recall: 0.9628\n",
      "2018-12-27T14:19:26.900465, step: 1490, loss: 0.16366764903068542, acc: 0.9141, auc: 0.9993, precision: 0.9337, recall: 0.9018\n",
      "2018-12-27T14:19:27.037258, step: 1491, loss: 0.1587841808795929, acc: 0.9453, auc: 0.999, precision: 0.9485, recall: 0.9478\n",
      "2018-12-27T14:19:27.172257, step: 1492, loss: 0.11220663785934448, acc: 0.9219, auc: 1.0, precision: 0.9383, recall: 0.9123\n",
      "2018-12-27T14:19:27.313010, step: 1493, loss: 0.12521794438362122, acc: 0.9453, auc: 1.0, precision: 0.9485, recall: 0.9478\n",
      "2018-12-27T14:19:27.442293, step: 1494, loss: 0.07571683824062347, acc: 0.9609, auc: 0.9998, precision: 0.959, recall: 0.9653\n",
      "2018-12-27T14:19:27.572133, step: 1495, loss: 0.057469647377729416, acc: 0.9766, auc: 0.9978, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:19:27.699847, step: 1496, loss: 0.0380752794444561, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:19:27.822294, step: 1497, loss: 0.04519972577691078, acc: 0.9922, auc: 0.9992, precision: 0.9907, recall: 0.9933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:19:27.953542, step: 1498, loss: 0.05945741385221481, acc: 0.9844, auc: 0.999, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:19:28.086820, step: 1499, loss: 0.12135649472475052, acc: 0.9609, auc: 0.9978, precision: 0.9597, recall: 0.9648\n",
      "2018-12-27T14:19:28.214933, step: 1500, loss: 0.136856347322464, acc: 0.9219, auc: 0.9988, precision: 0.9254, recall: 0.9296\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:19:33.061664, step: 1500, loss: 1.3786652935178656, acc: 0.650492105263158, auc: 0.715692105263158, precision: 0.6589289473684211, recall: 0.6485184210526315\n",
      "2018-12-27T14:19:33.191024, step: 1501, loss: 0.14038988947868347, acc: 0.9531, auc: 0.9937, precision: 0.9549, recall: 0.9531\n",
      "2018-12-27T14:19:33.323690, step: 1502, loss: 0.0269133523106575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:33.451729, step: 1503, loss: 0.044482260942459106, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:33.587564, step: 1504, loss: 0.01410471647977829, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:33.724204, step: 1505, loss: 0.024107560515403748, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:19:33.850740, step: 1506, loss: 0.02813594602048397, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:33.979946, step: 1507, loss: 0.07144545763731003, acc: 0.9688, auc: 0.9946, precision: 0.9686, recall: 0.9686\n",
      "2018-12-27T14:19:34.122113, step: 1508, loss: 0.03298525884747505, acc: 0.9844, auc: 0.9998, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:19:34.247714, step: 1509, loss: 0.058740437030792236, acc: 0.9766, auc: 0.9988, precision: 0.9762, recall: 0.9779\n",
      "2018-12-27T14:19:34.372512, step: 1510, loss: 0.04982846602797508, acc: 0.9531, auc: 1.0, precision: 0.9577, recall: 0.9524\n",
      "2018-12-27T14:19:34.498186, step: 1511, loss: 0.0942695289850235, acc: 0.9609, auc: 0.9968, precision: 0.9613, recall: 0.9601\n",
      "2018-12-27T14:19:34.628572, step: 1512, loss: 0.014670955948531628, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:34.758300, step: 1513, loss: 0.023118438199162483, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:34.887302, step: 1514, loss: 0.011489076539874077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:35.026241, step: 1515, loss: 0.03434727340936661, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:19:35.152730, step: 1516, loss: 0.01615879125893116, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:19:35.278670, step: 1517, loss: 0.034604091197252274, acc: 0.9844, auc: 1.0, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:19:35.423278, step: 1518, loss: 0.07480806112289429, acc: 0.9688, auc: 0.9998, precision: 0.9733, recall: 0.9649\n",
      "2018-12-27T14:19:35.546866, step: 1519, loss: 0.18221458792686462, acc: 0.9375, auc: 0.9998, precision: 0.9394, recall: 0.9429\n",
      "2018-12-27T14:19:35.673181, step: 1520, loss: 0.14885686337947845, acc: 0.9062, auc: 1.0, precision: 0.9259, recall: 0.8983\n",
      "2018-12-27T14:19:35.813636, step: 1521, loss: 0.08974568545818329, acc: 0.9688, auc: 0.9973, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:19:35.950167, step: 1522, loss: 0.06152181699872017, acc: 0.9766, auc: 0.9978, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:19:36.087816, step: 1523, loss: 0.04260724410414696, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:19:36.215023, step: 1524, loss: 0.04932025074958801, acc: 0.9609, auc: 1.0, precision: 0.9648, recall: 0.9597\n",
      "2018-12-27T14:19:36.341234, step: 1525, loss: 0.043567731976509094, acc: 0.9922, auc: 0.9995, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:19:36.465321, step: 1526, loss: 0.04810803383588791, acc: 0.9766, auc: 0.9987, precision: 0.9772, recall: 0.9747\n",
      "2018-12-27T14:19:36.596215, step: 1527, loss: 0.05766134709119797, acc: 0.9844, auc: 0.9988, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:36.725849, step: 1528, loss: 0.05014921352267265, acc: 0.9609, auc: 0.9993, precision: 0.9603, recall: 0.9643\n",
      "2018-12-27T14:19:36.875491, step: 1529, loss: 0.08260036259889603, acc: 0.9688, auc: 0.9953, precision: 0.9684, recall: 0.9684\n",
      "2018-12-27T14:19:36.993621, step: 1530, loss: 0.08109298348426819, acc: 0.9609, auc: 0.9953, precision: 0.9602, recall: 0.9562\n",
      "2018-12-27T14:19:37.104482, step: 1531, loss: 0.03633910417556763, acc: 0.9844, auc: 0.9995, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:19:37.235307, step: 1532, loss: 0.04483126848936081, acc: 0.9766, auc: 0.9993, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:19:37.368054, step: 1533, loss: 0.027187176048755646, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:19:37.496933, step: 1534, loss: 0.07826893031597137, acc: 0.9844, auc: 0.9975, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:19:37.643893, step: 1535, loss: 0.07260271906852722, acc: 0.9766, auc: 0.9992, precision: 0.9722, recall: 0.9805\n",
      "2018-12-27T14:19:37.779248, step: 1536, loss: 0.1099347397685051, acc: 0.9609, auc: 0.9983, precision: 0.9616, recall: 0.9613\n",
      "2018-12-27T14:19:37.901371, step: 1537, loss: 0.06577849388122559, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:19:38.031005, step: 1538, loss: 0.030181489884853363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:38.160083, step: 1539, loss: 0.015566342510282993, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:38.311016, step: 1540, loss: 0.025076761841773987, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:38.434763, step: 1541, loss: 0.03973982483148575, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:19:38.570589, step: 1542, loss: 0.04253796860575676, acc: 0.9688, auc: 1.0, precision: 0.9655, recall: 0.973\n",
      "2018-12-27T14:19:38.705414, step: 1543, loss: 0.0751381516456604, acc: 0.9688, auc: 0.9995, precision: 0.971, recall: 0.9683\n",
      "2018-12-27T14:19:38.843200, step: 1544, loss: 0.10432863235473633, acc: 0.9453, auc: 0.999, precision: 0.9453, recall: 0.9507\n",
      "2018-12-27T14:19:38.975114, step: 1545, loss: 0.1768212914466858, acc: 0.9531, auc: 0.9958, precision: 0.9545, recall: 0.9536\n",
      "2018-12-27T14:19:39.098255, step: 1546, loss: 0.2643982768058777, acc: 0.875, auc: 0.9881, precision: 0.8856, recall: 0.8856\n",
      "2018-12-27T14:19:39.230644, step: 1547, loss: 0.2164124995470047, acc: 0.9297, auc: 0.9966, precision: 0.9423, recall: 0.9237\n",
      "2018-12-27T14:19:39.364469, step: 1548, loss: 0.09934046864509583, acc: 0.9297, auc: 0.9983, precision: 0.9408, recall: 0.9262\n",
      "2018-12-27T14:19:39.494117, step: 1549, loss: 0.036631643772125244, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:39.628749, step: 1550, loss: 0.06661749631166458, acc: 0.9688, auc: 0.9988, precision: 0.9692, recall: 0.9688\n",
      "start training model\n",
      "2018-12-27T14:19:39.898001, step: 1551, loss: 0.051494404673576355, acc: 0.9844, auc: 0.9998, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:19:40.027914, step: 1552, loss: 0.014139235019683838, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:40.153591, step: 1553, loss: 0.018145661801099777, acc: 0.9844, auc: 1.0, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:19:40.292301, step: 1554, loss: 0.016246085986495018, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:40.422233, step: 1555, loss: 0.027842216193675995, acc: 0.9844, auc: 1.0, precision: 0.9828, recall: 0.9861\n",
      "2018-12-27T14:19:40.566842, step: 1556, loss: 0.034382306039333344, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:19:40.697788, step: 1557, loss: 0.02377730794250965, acc: 0.9844, auc: 1.0, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:19:40.827329, step: 1558, loss: 0.022305311635136604, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:40.968285, step: 1559, loss: 0.028705593198537827, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:19:41.093655, step: 1560, loss: 0.02376185730099678, acc: 0.9922, auc: 1.0, precision: 0.9934, recall: 0.9906\n",
      "2018-12-27T14:19:41.224448, step: 1561, loss: 0.026546048000454903, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:41.352346, step: 1562, loss: 0.011051221750676632, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:19:41.484338, step: 1563, loss: 0.02092106081545353, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:41.621271, step: 1564, loss: 0.023485004901885986, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:41.758572, step: 1565, loss: 0.0066353934817016125, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:41.888223, step: 1566, loss: 0.006482655182480812, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:42.017662, step: 1567, loss: 0.02537144534289837, acc: 0.9844, auc: 0.9998, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:19:42.142276, step: 1568, loss: 0.01835251972079277, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:19:42.277796, step: 1569, loss: 0.011729398742318153, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:42.403109, step: 1570, loss: 0.005206828471273184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:42.542295, step: 1571, loss: 0.04359897971153259, acc: 0.9844, auc: 0.9983, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:42.668630, step: 1572, loss: 0.0353352315723896, acc: 0.9766, auc: 1.0, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:19:42.790135, step: 1573, loss: 0.03387007862329483, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:19:42.915569, step: 1574, loss: 0.03797921538352966, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:43.025655, step: 1575, loss: 0.020843593403697014, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:19:43.159357, step: 1576, loss: 0.04919493570923805, acc: 0.9844, auc: 0.999, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:19:43.285733, step: 1577, loss: 0.019785312935709953, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:43.424016, step: 1578, loss: 0.020432230085134506, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:43.558043, step: 1579, loss: 0.04252304136753082, acc: 0.9844, auc: 0.9988, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:19:43.701849, step: 1580, loss: 0.07783666998147964, acc: 0.9844, auc: 0.9995, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:19:43.830704, step: 1581, loss: 0.11356387287378311, acc: 0.9297, auc: 1.0, precision: 0.9384, recall: 0.9297\n",
      "2018-12-27T14:19:43.974336, step: 1582, loss: 0.22251617908477783, acc: 0.9531, auc: 0.9933, precision: 0.961, recall: 0.9474\n",
      "2018-12-27T14:19:44.113690, step: 1583, loss: 0.15436166524887085, acc: 0.9531, auc: 0.9965, precision: 0.9516, recall: 0.9583\n",
      "2018-12-27T14:19:44.251485, step: 1584, loss: 0.16053453087806702, acc: 0.9297, auc: 0.9876, precision: 0.9296, recall: 0.928\n",
      "2018-12-27T14:19:44.381295, step: 1585, loss: 0.10517745465040207, acc: 0.9766, auc: 0.9926, precision: 0.976, recall: 0.977\n",
      "2018-12-27T14:19:44.516298, step: 1586, loss: 0.047823645174503326, acc: 0.9688, auc: 0.9995, precision: 0.9688, recall: 0.9706\n",
      "2018-12-27T14:19:44.651770, step: 1587, loss: 0.013542002066969872, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:44.785228, step: 1588, loss: 0.018546145409345627, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:44.909743, step: 1589, loss: 0.01835380122065544, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:45.038861, step: 1590, loss: 0.008822817355394363, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:45.169917, step: 1591, loss: 0.06714634597301483, acc: 0.9766, auc: 0.9997, precision: 0.9732, recall: 0.98\n",
      "2018-12-27T14:19:45.295309, step: 1592, loss: 0.0428154394030571, acc: 0.9766, auc: 1.0, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:19:45.426209, step: 1593, loss: 0.023576557636260986, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:19:45.561706, step: 1594, loss: 0.024746917188167572, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:19:45.706914, step: 1595, loss: 0.011257725767791271, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:45.842935, step: 1596, loss: 0.03961435332894325, acc: 0.9922, auc: 0.999, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:45.990201, step: 1597, loss: 0.03938593715429306, acc: 0.9922, auc: 0.9998, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:46.134197, step: 1598, loss: 0.027934128418564796, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:46.268041, step: 1599, loss: 0.022625761106610298, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:19:46.399799, step: 1600, loss: 0.021029550582170486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:19:51.220869, step: 1600, loss: 1.3281073805532957, acc: 0.6681763157894737, auc: 0.7384842105263159, precision: 0.673134210526316, recall: 0.6695236842105263\n",
      "2018-12-27T14:19:51.343971, step: 1601, loss: 0.014937371015548706, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:51.487097, step: 1602, loss: 0.01156471949070692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:51.611119, step: 1603, loss: 0.01827484741806984, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:19:51.749037, step: 1604, loss: 0.056315165013074875, acc: 0.9922, auc: 0.9993, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:19:51.879102, step: 1605, loss: 0.06694667041301727, acc: 0.9609, auc: 0.9993, precision: 0.9658, recall: 0.9583\n",
      "2018-12-27T14:19:52.001972, step: 1606, loss: 0.04753243178129196, acc: 0.9766, auc: 0.9985, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:19:52.142849, step: 1607, loss: 0.011387577280402184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:52.278678, step: 1608, loss: 0.035382091999053955, acc: 0.9766, auc: 0.9993, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:19:52.422908, step: 1609, loss: 0.02832353115081787, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:19:52.542290, step: 1610, loss: 0.044006023555994034, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:19:52.674029, step: 1611, loss: 0.05657818540930748, acc: 0.9688, auc: 1.0, precision: 0.9706, recall: 0.9688\n",
      "2018-12-27T14:19:52.826613, step: 1612, loss: 0.05603208392858505, acc: 0.9688, auc: 0.9993, precision: 0.9698, recall: 0.9679\n",
      "2018-12-27T14:19:52.978374, step: 1613, loss: 0.028482254594564438, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:19:53.116934, step: 1614, loss: 0.01859270967543125, acc: 0.9844, auc: 1.0, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:19:53.243166, step: 1615, loss: 0.03255624696612358, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:19:53.371049, step: 1616, loss: 0.04122868925333023, acc: 0.9766, auc: 0.9998, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:19:53.498148, step: 1617, loss: 0.02926492691040039, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:53.643895, step: 1618, loss: 0.07454272359609604, acc: 0.9688, auc: 0.998, precision: 0.9706, recall: 0.9659\n",
      "2018-12-27T14:19:53.784286, step: 1619, loss: 0.03686007484793663, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:19:53.907779, step: 1620, loss: 0.06183905899524689, acc: 0.9844, auc: 0.9992, precision: 0.9818, recall: 0.9867\n",
      "2018-12-27T14:19:54.031470, step: 1621, loss: 0.0823766440153122, acc: 0.9766, auc: 0.9987, precision: 0.9747, recall: 0.9772\n",
      "2018-12-27T14:19:54.154174, step: 1622, loss: 0.07748828083276749, acc: 0.9688, auc: 0.999, precision: 0.9661, recall: 0.9726\n",
      "2018-12-27T14:19:54.277657, step: 1623, loss: 0.01944073662161827, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:54.414074, step: 1624, loss: 0.027273211628198624, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:19:54.559066, step: 1625, loss: 0.025869734585285187, acc: 0.9922, auc: 0.9998, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:19:54.693523, step: 1626, loss: 0.012149063870310783, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:19:54.824500, step: 1627, loss: 0.01787535846233368, acc: 0.9922, auc: 0.9998, precision: 0.9915, recall: 0.9929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:19:54.950769, step: 1628, loss: 0.011317653581500053, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:19:55.101708, step: 1629, loss: 0.004876288585364819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:55.230327, step: 1630, loss: 0.026877541095018387, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:19:55.377349, step: 1631, loss: 0.03224155679345131, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:19:55.488911, step: 1632, loss: 0.05969296023249626, acc: 0.9609, auc: 1.0, precision: 0.9638, recall: 0.9609\n",
      "2018-12-27T14:19:55.609209, step: 1633, loss: 0.1664244532585144, acc: 0.9453, auc: 0.999, precision: 0.9562, recall: 0.9364\n",
      "2018-12-27T14:19:55.740006, step: 1634, loss: 0.11292633414268494, acc: 0.9297, auc: 0.999, precision: 0.94, recall: 0.9274\n",
      "2018-12-27T14:19:55.862891, step: 1635, loss: 0.06544004380702972, acc: 0.9766, auc: 1.0, precision: 0.9766, recall: 0.9776\n",
      "2018-12-27T14:19:55.992696, step: 1636, loss: 0.14306247234344482, acc: 0.9453, auc: 0.9919, precision: 0.9441, recall: 0.947\n",
      "2018-12-27T14:19:56.135437, step: 1637, loss: 0.025136584416031837, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:19:56.263555, step: 1638, loss: 0.06610757857561111, acc: 0.9766, auc: 0.9977, precision: 0.9739, recall: 0.9772\n",
      "2018-12-27T14:19:56.389426, step: 1639, loss: 0.02530853822827339, acc: 0.9688, auc: 1.0, precision: 0.9718, recall: 0.9672\n",
      "2018-12-27T14:19:56.525890, step: 1640, loss: 0.01627018302679062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:56.656360, step: 1641, loss: 0.05551751330494881, acc: 0.9922, auc: 0.997, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:19:56.781812, step: 1642, loss: 0.031011059880256653, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:56.915696, step: 1643, loss: 0.016961224377155304, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:57.049395, step: 1644, loss: 0.054144490510225296, acc: 0.9844, auc: 0.9985, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:57.174112, step: 1645, loss: 0.022634726017713547, acc: 0.9844, auc: 1.0, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:19:57.311396, step: 1646, loss: 0.03932200372219086, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:19:57.456565, step: 1647, loss: 0.029742464423179626, acc: 0.9922, auc: 0.9995, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:19:57.589348, step: 1648, loss: 0.01951957307755947, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:19:57.744084, step: 1649, loss: 0.048530757427215576, acc: 0.9922, auc: 0.9995, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:57.886277, step: 1650, loss: 0.0728355422616005, acc: 0.9688, auc: 0.9971, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:19:58.024567, step: 1651, loss: 0.03864455968141556, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:19:58.161144, step: 1652, loss: 0.05802805721759796, acc: 0.9844, auc: 0.998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:19:58.296448, step: 1653, loss: 0.055066999047994614, acc: 0.9766, auc: 0.9993, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:19:58.424008, step: 1654, loss: 0.11795148998498917, acc: 0.9688, auc: 0.9941, precision: 0.9726, recall: 0.9661\n",
      "2018-12-27T14:19:58.547185, step: 1655, loss: 0.23671233654022217, acc: 0.8906, auc: 0.9778, precision: 0.8996, recall: 0.8918\n",
      "2018-12-27T14:19:58.676763, step: 1656, loss: 0.44236260652542114, acc: 0.8906, auc: 0.988, precision: 0.9114, recall: 0.8889\n",
      "2018-12-27T14:19:58.811844, step: 1657, loss: 0.20028036832809448, acc: 0.8828, auc: 0.9985, precision: 0.8929, recall: 0.8973\n",
      "2018-12-27T14:19:58.943031, step: 1658, loss: 0.07912874966859818, acc: 0.9688, auc: 0.9995, precision: 0.9722, recall: 0.9667\n",
      "2018-12-27T14:19:59.066873, step: 1659, loss: 0.05526716634631157, acc: 0.9453, auc: 1.0, precision: 0.9533, recall: 0.9417\n",
      "2018-12-27T14:19:59.193725, step: 1660, loss: 0.027038291096687317, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:19:59.326006, step: 1661, loss: 0.0245499387383461, acc: 0.9922, auc: 1.0, precision: 0.9936, recall: 0.9902\n",
      "2018-12-27T14:19:59.444208, step: 1662, loss: 0.016693616285920143, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:19:59.568828, step: 1663, loss: 0.03805612027645111, acc: 0.9766, auc: 0.9995, precision: 0.98, recall: 0.9732\n",
      "2018-12-27T14:19:59.705088, step: 1664, loss: 0.06253441423177719, acc: 0.9688, auc: 0.9985, precision: 0.97, recall: 0.9676\n",
      "2018-12-27T14:19:59.844623, step: 1665, loss: 0.07241267710924149, acc: 0.9531, auc: 1.0, precision: 0.9464, recall: 0.9615\n",
      "2018-12-27T14:19:59.986544, step: 1666, loss: 0.05559971183538437, acc: 0.9766, auc: 1.0, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:20:00.128908, step: 1667, loss: 0.06872877478599548, acc: 0.9688, auc: 0.9978, precision: 0.9705, recall: 0.9663\n",
      "2018-12-27T14:20:00.251666, step: 1668, loss: 0.01426171138882637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:00.417756, step: 1669, loss: 0.011888640932738781, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:00.555274, step: 1670, loss: 0.04275691509246826, acc: 0.9922, auc: 0.9978, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:20:00.681549, step: 1671, loss: 0.01536182127892971, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:00.808344, step: 1672, loss: 0.007888441905379295, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:00.947226, step: 1673, loss: 0.05427182465791702, acc: 0.9766, auc: 0.9995, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:20:01.074853, step: 1674, loss: 0.04901969060301781, acc: 0.9609, auc: 1.0, precision: 0.9658, recall: 0.9583\n",
      "2018-12-27T14:20:01.207880, step: 1675, loss: 0.05730371177196503, acc: 0.9766, auc: 0.999, precision: 0.9763, recall: 0.9769\n",
      "2018-12-27T14:20:01.331229, step: 1676, loss: 0.023538388311862946, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:20:01.463422, step: 1677, loss: 0.017782431095838547, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:01.587044, step: 1678, loss: 0.03221724182367325, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:01.722734, step: 1679, loss: 0.04926177114248276, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:01.849831, step: 1680, loss: 0.03482384979724884, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:20:01.986825, step: 1681, loss: 0.06109681725502014, acc: 0.9766, auc: 0.998, precision: 0.976, recall: 0.977\n",
      "2018-12-27T14:20:02.123692, step: 1682, loss: 0.048262760043144226, acc: 0.9766, auc: 0.9988, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:20:02.265561, step: 1683, loss: 0.048393309116363525, acc: 0.9766, auc: 0.9995, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:20:02.429786, step: 1684, loss: 0.06920842826366425, acc: 0.9531, auc: 1.0, precision: 0.9615, recall: 0.9464\n",
      "2018-12-27T14:20:02.555189, step: 1685, loss: 0.07131597399711609, acc: 0.9766, auc: 0.9995, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:20:02.682938, step: 1686, loss: 0.03467034548521042, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:20:02.808007, step: 1687, loss: 0.0336136519908905, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:02.945550, step: 1688, loss: 0.06782181560993195, acc: 0.9688, auc: 0.9978, precision: 0.9688, recall: 0.9688\n",
      "2018-12-27T14:20:03.085573, step: 1689, loss: 0.08068877458572388, acc: 0.9688, auc: 0.997, precision: 0.975, recall: 0.9615\n",
      "2018-12-27T14:20:03.211258, step: 1690, loss: 0.03438762202858925, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:20:03.336270, step: 1691, loss: 0.0332193449139595, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:20:03.474225, step: 1692, loss: 0.058983445167541504, acc: 0.9766, auc: 0.9976, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:20:03.610020, step: 1693, loss: 0.05553806573152542, acc: 0.9844, auc: 0.998, precision: 0.9843, recall: 0.9843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:20:03.736423, step: 1694, loss: 0.056373052299022675, acc: 0.9531, auc: 0.9985, precision: 0.9536, recall: 0.9545\n",
      "2018-12-27T14:20:03.860319, step: 1695, loss: 0.08292929828166962, acc: 0.9766, auc: 1.0, precision: 0.98, recall: 0.9732\n",
      "2018-12-27T14:20:03.986447, step: 1696, loss: 0.1829206943511963, acc: 0.9219, auc: 1.0, precision: 0.9206, recall: 0.9333\n",
      "2018-12-27T14:20:04.127146, step: 1697, loss: 0.1846914291381836, acc: 0.9297, auc: 0.9983, precision: 0.9416, recall: 0.925\n",
      "2018-12-27T14:20:04.251196, step: 1698, loss: 0.08606093376874924, acc: 0.9375, auc: 0.9998, precision: 0.9429, recall: 0.9394\n",
      "2018-12-27T14:20:04.390335, step: 1699, loss: 0.12353802472352982, acc: 0.9688, auc: 0.994, precision: 0.9667, recall: 0.9722\n",
      "2018-12-27T14:20:04.514841, step: 1700, loss: 0.14483368396759033, acc: 0.9219, auc: 0.9971, precision: 0.9306, recall: 0.9242\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:20:09.382355, step: 1700, loss: 1.3875985961211355, acc: 0.6611815789473685, auc: 0.7251631578947367, precision: 0.6684605263157893, recall: 0.660428947368421\n",
      "2018-12-27T14:20:09.540165, step: 1701, loss: 0.07776839286088943, acc: 0.9766, auc: 1.0, precision: 0.9754, recall: 0.9786\n",
      "2018-12-27T14:20:09.677499, step: 1702, loss: 0.03887837380170822, acc: 0.9766, auc: 0.9995, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:20:09.806066, step: 1703, loss: 0.02496369555592537, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:09.954691, step: 1704, loss: 0.0659511610865593, acc: 0.9844, auc: 0.997, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:20:10.080976, step: 1705, loss: 0.07644645869731903, acc: 0.9297, auc: 0.998, precision: 0.9336, recall: 0.9313\n",
      "start training model\n",
      "2018-12-27T14:20:10.367581, step: 1706, loss: 0.01840474084019661, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:10.501961, step: 1707, loss: 0.00479239271953702, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:10.630298, step: 1708, loss: 0.014094162732362747, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:10.754469, step: 1709, loss: 0.027232691645622253, acc: 0.9922, auc: 0.9997, precision: 0.9906, recall: 0.9934\n",
      "2018-12-27T14:20:10.881292, step: 1710, loss: 0.03901129961013794, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:20:11.015763, step: 1711, loss: 0.032398633658885956, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:20:11.173144, step: 1712, loss: 0.04739471524953842, acc: 0.9844, auc: 0.9988, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:20:11.320478, step: 1713, loss: 0.09009449928998947, acc: 0.9688, auc: 0.9983, precision: 0.9679, recall: 0.9698\n",
      "2018-12-27T14:20:11.454703, step: 1714, loss: 0.03378333896398544, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:11.587253, step: 1715, loss: 0.006934579461812973, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:11.710758, step: 1716, loss: 0.007737213745713234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:11.844920, step: 1717, loss: 0.012873468920588493, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:11.976143, step: 1718, loss: 0.016176866367459297, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:12.122547, step: 1719, loss: 0.013151325285434723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:12.278393, step: 1720, loss: 0.018388211727142334, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:12.425438, step: 1721, loss: 0.010701783932745457, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:20:12.577929, step: 1722, loss: 0.007051474414765835, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:12.719222, step: 1723, loss: 0.013268135488033295, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:12.844527, step: 1724, loss: 0.01960127428174019, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:12.971686, step: 1725, loss: 0.007708905264735222, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:13.099850, step: 1726, loss: 0.012752179987728596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:13.247756, step: 1727, loss: 0.0077355592511594296, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:13.378793, step: 1728, loss: 0.011861194856464863, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:13.508705, step: 1729, loss: 0.022696055471897125, acc: 0.9844, auc: 1.0, precision: 0.9868, recall: 0.9815\n",
      "2018-12-27T14:20:13.651841, step: 1730, loss: 0.10785672813653946, acc: 0.9609, auc: 1.0, precision: 0.9621, recall: 0.9627\n",
      "2018-12-27T14:20:13.794322, step: 1731, loss: 0.29743921756744385, acc: 0.8594, auc: 1.0, precision: 0.8831, recall: 0.8696\n",
      "2018-12-27T14:20:13.936013, step: 1732, loss: 0.11728204786777496, acc: 0.9688, auc: 0.9995, precision: 0.9683, recall: 0.971\n",
      "2018-12-27T14:20:14.064828, step: 1733, loss: 0.0274877417832613, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:20:14.211690, step: 1734, loss: 0.0649513527750969, acc: 0.9688, auc: 0.9995, precision: 0.9718, recall: 0.9672\n",
      "2018-12-27T14:20:14.352580, step: 1735, loss: 0.12564212083816528, acc: 0.9375, auc: 0.9978, precision: 0.9365, recall: 0.9452\n",
      "2018-12-27T14:20:14.490633, step: 1736, loss: 0.03887765482068062, acc: 0.9922, auc: 0.9992, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:20:14.631206, step: 1737, loss: 0.027385491877794266, acc: 0.9922, auc: 0.999, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:20:14.779879, step: 1738, loss: 0.02951187454164028, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:20:14.919827, step: 1739, loss: 0.05477134510874748, acc: 0.9922, auc: 0.999, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:15.069180, step: 1740, loss: 0.0462479367852211, acc: 0.9922, auc: 0.9985, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:20:15.215024, step: 1741, loss: 0.04270945116877556, acc: 0.9844, auc: 1.0, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:20:15.360841, step: 1742, loss: 0.045358359813690186, acc: 0.9844, auc: 0.9995, precision: 0.9882, recall: 0.9778\n",
      "2018-12-27T14:20:15.499398, step: 1743, loss: 0.02313106507062912, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:15.625283, step: 1744, loss: 0.007019948214292526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:15.765725, step: 1745, loss: 0.04012567177414894, acc: 0.9844, auc: 0.999, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:20:15.899460, step: 1746, loss: 0.009802118875086308, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:16.025680, step: 1747, loss: 0.028134271502494812, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:20:16.165039, step: 1748, loss: 0.04752500355243683, acc: 0.9844, auc: 0.9985, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:20:16.301779, step: 1749, loss: 0.018180307000875473, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:16.446798, step: 1750, loss: 0.006760960444808006, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:16.586948, step: 1751, loss: 0.005826751701533794, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:16.712221, step: 1752, loss: 0.023154903203248978, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:20:16.853838, step: 1753, loss: 0.03973647579550743, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:20:16.977934, step: 1754, loss: 0.03444453701376915, acc: 0.9766, auc: 1.0, precision: 0.9741, recall: 0.9795\n",
      "2018-12-27T14:20:17.107110, step: 1755, loss: 0.09527327865362167, acc: 0.9688, auc: 1.0, precision: 0.9722, recall: 0.9667\n",
      "2018-12-27T14:20:17.228086, step: 1756, loss: 0.07455667853355408, acc: 0.9531, auc: 1.0, precision: 0.9559, recall: 0.9545\n",
      "2018-12-27T14:20:17.346149, step: 1757, loss: 0.04425831884145737, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:17.460736, step: 1758, loss: 0.05574570596218109, acc: 0.9609, auc: 0.999, precision: 0.9635, recall: 0.9584\n",
      "2018-12-27T14:20:17.570705, step: 1759, loss: 0.04644683375954628, acc: 0.9766, auc: 0.9993, precision: 0.9761, recall: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:20:17.691129, step: 1760, loss: 0.022845018655061722, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:17.838990, step: 1761, loss: 0.00708343368023634, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:17.975956, step: 1762, loss: 0.06471087038516998, acc: 0.9609, auc: 0.9973, precision: 0.9613, recall: 0.9605\n",
      "2018-12-27T14:20:18.114296, step: 1763, loss: 0.03514727205038071, acc: 0.9688, auc: 0.9993, precision: 0.9692, recall: 0.9688\n",
      "2018-12-27T14:20:18.241270, step: 1764, loss: 0.007449714466929436, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:18.379445, step: 1765, loss: 0.028585102409124374, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:20:18.519786, step: 1766, loss: 0.030854221433401108, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:20:18.664202, step: 1767, loss: 0.06804247945547104, acc: 0.9766, auc: 0.999, precision: 0.9795, recall: 0.9741\n",
      "2018-12-27T14:20:18.825720, step: 1768, loss: 0.04943368583917618, acc: 0.9688, auc: 0.9998, precision: 0.9718, recall: 0.9672\n",
      "2018-12-27T14:20:18.962403, step: 1769, loss: 0.032217174768447876, acc: 0.9922, auc: 0.9995, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:19.102391, step: 1770, loss: 0.038687437772750854, acc: 0.9766, auc: 0.9993, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:20:19.234294, step: 1771, loss: 0.02007615938782692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:19.373505, step: 1772, loss: 0.03368843346834183, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:19.504877, step: 1773, loss: 0.09510128200054169, acc: 0.9531, auc: 0.9958, precision: 0.9531, recall: 0.9571\n",
      "2018-12-27T14:20:19.650860, step: 1774, loss: 0.019754474982619286, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:20:19.797916, step: 1775, loss: 0.009501412510871887, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:20:19.935677, step: 1776, loss: 0.008665090426802635, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:20.071590, step: 1777, loss: 0.057933468371629715, acc: 0.9844, auc: 0.9998, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:20:20.209141, step: 1778, loss: 0.18156278133392334, acc: 0.9453, auc: 0.9998, precision: 0.9539, recall: 0.9407\n",
      "2018-12-27T14:20:20.340831, step: 1779, loss: 0.15596890449523926, acc: 0.9609, auc: 0.999, precision: 0.9679, recall: 0.9545\n",
      "2018-12-27T14:20:20.466815, step: 1780, loss: 0.11945508420467377, acc: 0.9375, auc: 0.9988, precision: 0.9481, recall: 0.9322\n",
      "2018-12-27T14:20:20.592594, step: 1781, loss: 0.06877782195806503, acc: 0.9844, auc: 0.9971, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:20:20.718002, step: 1782, loss: 0.02331385388970375, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:20:20.839685, step: 1783, loss: 0.03992269188165665, acc: 0.9844, auc: 0.9997, precision: 0.9839, recall: 0.9839\n",
      "2018-12-27T14:20:20.967195, step: 1784, loss: 0.042216312140226364, acc: 0.9766, auc: 0.9995, precision: 0.9732, recall: 0.98\n",
      "2018-12-27T14:20:21.092473, step: 1785, loss: 0.027977338060736656, acc: 0.9844, auc: 1.0, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:20:21.233414, step: 1786, loss: 0.01416848972439766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:21.377154, step: 1787, loss: 0.02484244480729103, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:20:21.510922, step: 1788, loss: 0.05424249917268753, acc: 0.9766, auc: 0.9993, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:20:21.648831, step: 1789, loss: 0.0641389787197113, acc: 0.9688, auc: 0.998, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:20:21.789150, step: 1790, loss: 0.04092022031545639, acc: 0.9844, auc: 0.9993, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:21.921386, step: 1791, loss: 0.022335197776556015, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:22.048902, step: 1792, loss: 0.011945756152272224, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:22.178390, step: 1793, loss: 0.006688165478408337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:22.314726, step: 1794, loss: 0.051649779081344604, acc: 0.9844, auc: 0.9985, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:20:22.455219, step: 1795, loss: 0.06400489062070847, acc: 0.9609, auc: 0.9985, precision: 0.9633, recall: 0.9589\n",
      "2018-12-27T14:20:22.586159, step: 1796, loss: 0.011587359942495823, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:22.718171, step: 1797, loss: 0.027628019452095032, acc: 0.9922, auc: 0.9995, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:20:22.842763, step: 1798, loss: 0.02592485584318638, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:20:22.973441, step: 1799, loss: 0.05188862234354019, acc: 0.9766, auc: 0.9998, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:20:23.118595, step: 1800, loss: 0.09024348109960556, acc: 0.9609, auc: 1.0, precision: 0.9632, recall: 0.9615\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:20:27.935047, step: 1800, loss: 1.7381967619845742, acc: 0.6404210526315789, auc: 0.7430789473684211, precision: 0.6794605263157893, recall: 0.643478947368421\n",
      "2018-12-27T14:20:28.048460, step: 1801, loss: 0.14871743321418762, acc: 0.9375, auc: 0.9993, precision: 0.9467, recall: 0.9344\n",
      "2018-12-27T14:20:28.183276, step: 1802, loss: 0.23606206476688385, acc: 0.9297, auc: 0.9909, precision: 0.9328, recall: 0.9357\n",
      "2018-12-27T14:20:28.311259, step: 1803, loss: 0.3170032799243927, acc: 0.9219, auc: 0.9944, precision: 0.9306, recall: 0.9242\n",
      "2018-12-27T14:20:28.435625, step: 1804, loss: 0.035955220460891724, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:20:28.565659, step: 1805, loss: 0.012203678488731384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:28.695052, step: 1806, loss: 0.036213114857673645, acc: 0.9766, auc: 1.0, precision: 0.9762, recall: 0.9779\n",
      "2018-12-27T14:20:28.821253, step: 1807, loss: 0.021566692739725113, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:28.955597, step: 1808, loss: 0.027469052001833916, acc: 0.9766, auc: 1.0, precision: 0.9737, recall: 0.9797\n",
      "2018-12-27T14:20:29.080097, step: 1809, loss: 0.026560990139842033, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:29.217509, step: 1810, loss: 0.022374192252755165, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:20:29.349614, step: 1811, loss: 0.02934674732387066, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:20:29.475769, step: 1812, loss: 0.016624141484498978, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:29.601009, step: 1813, loss: 0.02484280988574028, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:29.741811, step: 1814, loss: 0.014706953428685665, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:29.903190, step: 1815, loss: 0.02302008494734764, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:20:30.078958, step: 1816, loss: 0.027407651767134666, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:30.217703, step: 1817, loss: 0.01832881011068821, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:30.364732, step: 1818, loss: 0.012469424866139889, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:30.502854, step: 1819, loss: 0.027592333033680916, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:20:30.645846, step: 1820, loss: 0.022944264113903046, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:30.783163, step: 1821, loss: 0.06995570659637451, acc: 0.9688, auc: 0.999, precision: 0.9683, recall: 0.971\n",
      "2018-12-27T14:20:30.928240, step: 1822, loss: 0.060383740812540054, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:20:31.069402, step: 1823, loss: 0.019090838730335236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:31.204868, step: 1824, loss: 0.031503595411777496, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:20:31.340884, step: 1825, loss: 0.02324039675295353, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:20:31.478833, step: 1826, loss: 0.05691475048661232, acc: 0.9844, auc: 0.9983, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:20:31.614916, step: 1827, loss: 0.01060642022639513, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:31.757287, step: 1828, loss: 0.018367839977145195, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:31.876112, step: 1829, loss: 0.024873312562704086, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:32.016485, step: 1830, loss: 0.02516280487179756, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:32.170322, step: 1831, loss: 0.07749525457620621, acc: 0.9766, auc: 0.9971, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:20:32.294907, step: 1832, loss: 0.12634459137916565, acc: 0.9609, auc: 0.9917, precision: 0.9613, recall: 0.9605\n",
      "2018-12-27T14:20:32.437984, step: 1833, loss: 0.04172766953706741, acc: 0.9688, auc: 0.999, precision: 0.9686, recall: 0.9686\n",
      "2018-12-27T14:20:32.572216, step: 1834, loss: 0.03823172301054001, acc: 0.9844, auc: 0.999, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:20:32.699752, step: 1835, loss: 0.03326943889260292, acc: 0.9922, auc: 0.999, precision: 0.9933, recall: 0.9907\n",
      "2018-12-27T14:20:32.837107, step: 1836, loss: 0.07345199584960938, acc: 0.9766, auc: 1.0, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:20:32.963405, step: 1837, loss: 0.24733802676200867, acc: 0.9141, auc: 0.999, precision: 0.9304, recall: 0.9083\n",
      "2018-12-27T14:20:33.088414, step: 1838, loss: 0.11476889252662659, acc: 0.9609, auc: 0.9968, precision: 0.9606, recall: 0.9623\n",
      "2018-12-27T14:20:33.222690, step: 1839, loss: 0.09514150023460388, acc: 0.9531, auc: 0.9983, precision: 0.9545, recall: 0.9559\n",
      "2018-12-27T14:20:33.350256, step: 1840, loss: 0.10686475038528442, acc: 0.9531, auc: 0.9978, precision: 0.9565, recall: 0.9538\n",
      "2018-12-27T14:20:33.478093, step: 1841, loss: 0.06274629384279251, acc: 0.9531, auc: 0.9993, precision: 0.9538, recall: 0.9565\n",
      "2018-12-27T14:20:33.606633, step: 1842, loss: 0.051040247082710266, acc: 0.9922, auc: 0.9963, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:33.722061, step: 1843, loss: 0.07266723364591599, acc: 0.9766, auc: 0.9966, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:20:33.865063, step: 1844, loss: 0.025688758119940758, acc: 0.9844, auc: 1.0, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:20:34.003111, step: 1845, loss: 0.06072353944182396, acc: 0.9609, auc: 0.9975, precision: 0.9598, recall: 0.9628\n",
      "2018-12-27T14:20:34.138506, step: 1846, loss: 0.03519143909215927, acc: 0.9766, auc: 0.9993, precision: 0.9769, recall: 0.9763\n",
      "2018-12-27T14:20:34.301687, step: 1847, loss: 0.027124902233481407, acc: 0.9922, auc: 0.9997, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:20:34.434331, step: 1848, loss: 0.005946583114564419, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:34.592757, step: 1849, loss: 0.015101883560419083, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:34.743098, step: 1850, loss: 0.042285412549972534, acc: 0.9922, auc: 0.9998, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:20:34.890424, step: 1851, loss: 0.027286246418952942, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:35.027072, step: 1852, loss: 0.046597257256507874, acc: 0.9688, auc: 0.999, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:20:35.154181, step: 1853, loss: 0.029494469985365868, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:20:35.296204, step: 1854, loss: 0.04982278496026993, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:20:35.423332, step: 1855, loss: 0.053843334317207336, acc: 0.9922, auc: 0.9985, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:20:35.550495, step: 1856, loss: 0.06267334520816803, acc: 0.9688, auc: 0.9993, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:20:35.677240, step: 1857, loss: 0.047990623861551285, acc: 0.9766, auc: 1.0, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:20:35.800261, step: 1858, loss: 0.05603821575641632, acc: 0.9688, auc: 1.0, precision: 0.9661, recall: 0.9726\n",
      "2018-12-27T14:20:35.940331, step: 1859, loss: 0.0875023826956749, acc: 0.9766, auc: 0.997, precision: 0.9772, recall: 0.975\n",
      "2018-12-27T14:20:36.079266, step: 1860, loss: 0.07857119292020798, acc: 0.9453, auc: 0.998, precision: 0.9512, recall: 0.9395\n",
      "start training model\n",
      "2018-12-27T14:20:36.413605, step: 1861, loss: 0.03496353700757027, acc: 0.9922, auc: 0.9998, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:20:36.547810, step: 1862, loss: 0.02096807211637497, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:20:36.703666, step: 1863, loss: 0.031701404601335526, acc: 0.9766, auc: 1.0, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:20:36.845274, step: 1864, loss: 0.025815976783633232, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:36.989968, step: 1865, loss: 0.022930489853024483, acc: 0.9844, auc: 1.0, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:20:37.135902, step: 1866, loss: 0.0028012534603476524, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:37.282221, step: 1867, loss: 0.012919720262289047, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:37.423698, step: 1868, loss: 0.020315855741500854, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:37.556303, step: 1869, loss: 0.01282491348683834, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:37.691479, step: 1870, loss: 0.039053868502378464, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:20:37.818222, step: 1871, loss: 0.16492129862308502, acc: 0.9453, auc: 0.9982, precision: 0.9583, recall: 0.9314\n",
      "2018-12-27T14:20:37.955717, step: 1872, loss: 0.37430229783058167, acc: 0.9297, auc: 0.9733, precision: 0.9308, recall: 0.9314\n",
      "2018-12-27T14:20:38.083664, step: 1873, loss: 0.03933018818497658, acc: 0.9688, auc: 1.0, precision: 0.9706, recall: 0.9688\n",
      "2018-12-27T14:20:38.223047, step: 1874, loss: 0.05612238869071007, acc: 0.9922, auc: 0.999, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:38.350513, step: 1875, loss: 0.03666114807128906, acc: 0.9844, auc: 0.9998, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:20:38.475978, step: 1876, loss: 0.024637851864099503, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:38.603600, step: 1877, loss: 0.011279724538326263, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:38.738628, step: 1878, loss: 0.01727997697889805, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:38.867307, step: 1879, loss: 0.01780862733721733, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:38.994606, step: 1880, loss: 0.030606405809521675, acc: 0.9922, auc: 0.9992, precision: 0.9934, recall: 0.9906\n",
      "2018-12-27T14:20:39.131731, step: 1881, loss: 0.028688915073871613, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:39.259904, step: 1882, loss: 0.05190875381231308, acc: 0.9844, auc: 1.0, precision: 0.9828, recall: 0.9861\n",
      "2018-12-27T14:20:39.414475, step: 1883, loss: 0.0432044118642807, acc: 0.9844, auc: 1.0, precision: 0.9792, recall: 0.9878\n",
      "2018-12-27T14:20:39.556608, step: 1884, loss: 0.04675494506955147, acc: 0.9766, auc: 0.9995, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:20:39.690721, step: 1885, loss: 0.037786345928907394, acc: 0.9844, auc: 0.9995, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:39.819058, step: 1886, loss: 0.0220657791942358, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:39.935159, step: 1887, loss: 0.0196722112596035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:40.050444, step: 1888, loss: 0.01627931371331215, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:20:40.167049, step: 1889, loss: 0.014996728859841824, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:40.281809, step: 1890, loss: 0.003463434288278222, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:40.398966, step: 1891, loss: 0.009381627663969994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:40.515276, step: 1892, loss: 0.007726005278527737, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:20:40.629068, step: 1893, loss: 0.009488295763731003, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:40.752899, step: 1894, loss: 0.04062909632921219, acc: 0.9844, auc: 1.0, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:20:40.869483, step: 1895, loss: 0.03864069655537605, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:20:40.986929, step: 1896, loss: 0.025599990040063858, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:41.123009, step: 1897, loss: 0.02974480763077736, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:20:41.251232, step: 1898, loss: 0.015952596440911293, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:41.386687, step: 1899, loss: 0.04378793388605118, acc: 0.9609, auc: 0.9993, precision: 0.9621, recall: 0.9627\n",
      "2018-12-27T14:20:41.523685, step: 1900, loss: 0.1831609010696411, acc: 0.9609, auc: 0.9953, precision: 0.9609, recall: 0.9638\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:20:46.261472, step: 1900, loss: 1.926023991484391, acc: 0.6223315789473682, auc: 0.7177394736842109, precision: 0.667294736842105, recall: 0.6244157894736846\n",
      "2018-12-27T14:20:46.383737, step: 1901, loss: 0.11183202266693115, acc: 0.9453, auc: 0.9993, precision: 0.9533, recall: 0.9417\n",
      "2018-12-27T14:20:46.512653, step: 1902, loss: 0.01824222318828106, acc: 0.9922, auc: 0.9998, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:46.641057, step: 1903, loss: 0.009567678906023502, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:46.784188, step: 1904, loss: 0.012041193433105946, acc: 0.9922, auc: 1.0, precision: 0.9896, recall: 0.9938\n",
      "2018-12-27T14:20:46.917202, step: 1905, loss: 0.018775610253214836, acc: 0.9766, auc: 1.0, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:20:47.043385, step: 1906, loss: 0.021693984046578407, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:47.169943, step: 1907, loss: 0.008190235123038292, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:47.300499, step: 1908, loss: 0.0027835324872285128, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:47.438945, step: 1909, loss: 0.04724226891994476, acc: 0.9766, auc: 0.9995, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:20:47.562406, step: 1910, loss: 0.078806571662426, acc: 0.9688, auc: 0.999, precision: 0.9722, recall: 0.9667\n",
      "2018-12-27T14:20:47.694445, step: 1911, loss: 0.02412048913538456, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:47.823632, step: 1912, loss: 0.019682440906763077, acc: 0.9922, auc: 1.0, precision: 0.9907, recall: 0.9933\n",
      "2018-12-27T14:20:47.960233, step: 1913, loss: 0.016377871856093407, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:20:48.088603, step: 1914, loss: 0.009445971809327602, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:48.218932, step: 1915, loss: 0.05211235582828522, acc: 0.9844, auc: 0.9992, precision: 0.9875, recall: 0.98\n",
      "2018-12-27T14:20:48.345354, step: 1916, loss: 0.06361601501703262, acc: 0.9844, auc: 1.0, precision: 0.9818, recall: 0.9867\n",
      "2018-12-27T14:20:48.468244, step: 1917, loss: 0.08151201903820038, acc: 0.9531, auc: 1.0, precision: 0.9483, recall: 0.9605\n",
      "2018-12-27T14:20:48.609316, step: 1918, loss: 0.09992804378271103, acc: 0.9844, auc: 0.999, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:48.737135, step: 1919, loss: 0.0159161277115345, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:48.858847, step: 1920, loss: 0.017740624025464058, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:48.983664, step: 1921, loss: 0.022032761946320534, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:20:49.105087, step: 1922, loss: 0.061710890382528305, acc: 0.9844, auc: 0.9995, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:20:49.231678, step: 1923, loss: 0.039188116788864136, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:20:49.359157, step: 1924, loss: 0.032587118446826935, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:20:49.499992, step: 1925, loss: 0.040982846170663834, acc: 0.9766, auc: 0.9992, precision: 0.9772, recall: 0.9742\n",
      "2018-12-27T14:20:49.621911, step: 1926, loss: 0.0073458910919725895, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:49.747879, step: 1927, loss: 0.03914972022175789, acc: 0.9922, auc: 0.9995, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:20:49.872098, step: 1928, loss: 0.03149215504527092, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:20:50.014161, step: 1929, loss: 0.02421572059392929, acc: 0.9922, auc: 0.9998, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:20:50.152035, step: 1930, loss: 0.05874305218458176, acc: 0.9766, auc: 0.998, precision: 0.9764, recall: 0.9768\n",
      "2018-12-27T14:20:50.280776, step: 1931, loss: 0.091671422123909, acc: 0.9688, auc: 0.9968, precision: 0.9685, recall: 0.9694\n",
      "2018-12-27T14:20:50.406707, step: 1932, loss: 0.07689046859741211, acc: 0.9531, auc: 1.0, precision: 0.9545, recall: 0.9559\n",
      "2018-12-27T14:20:50.542354, step: 1933, loss: 0.16129626333713531, acc: 0.9453, auc: 0.9965, precision: 0.9453, recall: 0.9507\n",
      "2018-12-27T14:20:50.666099, step: 1934, loss: 0.10999680310487747, acc: 0.9297, auc: 0.9985, precision: 0.943, recall: 0.9224\n",
      "2018-12-27T14:20:50.794713, step: 1935, loss: 0.053159572184085846, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:20:50.920946, step: 1936, loss: 0.03532356023788452, acc: 0.9688, auc: 1.0, precision: 0.9733, recall: 0.9649\n",
      "2018-12-27T14:20:51.055764, step: 1937, loss: 0.03503251075744629, acc: 0.9766, auc: 0.999, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:20:51.192225, step: 1938, loss: 0.04258280619978905, acc: 0.9766, auc: 0.9995, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:20:51.329025, step: 1939, loss: 0.007025893311947584, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:51.454674, step: 1940, loss: 0.0736854076385498, acc: 0.9844, auc: 0.9978, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:20:51.590080, step: 1941, loss: 0.014627314172685146, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:20:51.717599, step: 1942, loss: 0.03426985442638397, acc: 0.9922, auc: 0.9995, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:51.841559, step: 1943, loss: 0.03735145181417465, acc: 0.9688, auc: 0.9995, precision: 0.971, recall: 0.9683\n",
      "2018-12-27T14:20:51.970782, step: 1944, loss: 0.07070799916982651, acc: 0.9688, auc: 0.9995, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:20:52.101282, step: 1945, loss: 0.038960326462984085, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:20:52.229456, step: 1946, loss: 0.02636389061808586, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:20:52.369919, step: 1947, loss: 0.01116762775927782, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:52.497402, step: 1948, loss: 0.04357646778225899, acc: 0.9844, auc: 0.9988, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:20:52.633453, step: 1949, loss: 0.023902446031570435, acc: 0.9844, auc: 0.9997, precision: 0.9868, recall: 0.9815\n",
      "2018-12-27T14:20:52.756038, step: 1950, loss: 0.013994860462844372, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:52.881949, step: 1951, loss: 0.016625823453068733, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:20:53.019439, step: 1952, loss: 0.025012699887156487, acc: 0.9844, auc: 0.9998, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:20:53.146422, step: 1953, loss: 0.008879500441253185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:53.279570, step: 1954, loss: 0.006849577650427818, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:53.408938, step: 1955, loss: 0.0025056893937289715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:53.534344, step: 1956, loss: 0.028505362570285797, acc: 0.9922, auc: 0.9998, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:20:53.657286, step: 1957, loss: 0.07523222267627716, acc: 0.9609, auc: 0.9995, precision: 0.9583, recall: 0.9658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:20:53.782517, step: 1958, loss: 0.09024018049240112, acc: 0.9844, auc: 0.998, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:20:53.907225, step: 1959, loss: 0.05888387933373451, acc: 0.9766, auc: 1.0, precision: 0.9737, recall: 0.9797\n",
      "2018-12-27T14:20:54.035000, step: 1960, loss: 0.05124002322554588, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:20:54.171631, step: 1961, loss: 0.026440361514687538, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:20:54.297690, step: 1962, loss: 0.01840786449611187, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:20:54.434819, step: 1963, loss: 0.026283303275704384, acc: 0.9922, auc: 0.9998, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:20:54.574869, step: 1964, loss: 0.03408404812216759, acc: 0.9844, auc: 0.9995, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:20:54.708105, step: 1965, loss: 0.15535511076450348, acc: 0.9531, auc: 0.9945, precision: 0.9491, recall: 0.9572\n",
      "2018-12-27T14:20:54.840426, step: 1966, loss: 0.09317248314619064, acc: 0.9609, auc: 0.9995, precision: 0.9643, recall: 0.9603\n",
      "2018-12-27T14:20:54.968510, step: 1967, loss: 0.03250572830438614, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:20:55.092779, step: 1968, loss: 0.10218879580497742, acc: 0.9844, auc: 0.9985, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:20:55.228993, step: 1969, loss: 0.14501047134399414, acc: 0.9297, auc: 0.9988, precision: 0.9384, recall: 0.9297\n",
      "2018-12-27T14:20:55.352265, step: 1970, loss: 0.09765685349702835, acc: 0.9844, auc: 0.9971, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:20:55.494525, step: 1971, loss: 0.06364864856004715, acc: 0.9766, auc: 0.9975, precision: 0.9746, recall: 0.9792\n",
      "2018-12-27T14:20:55.618447, step: 1972, loss: 0.0850231796503067, acc: 0.9688, auc: 0.9967, precision: 0.9663, recall: 0.9705\n",
      "2018-12-27T14:20:55.754180, step: 1973, loss: 0.08554017543792725, acc: 0.9688, auc: 0.9963, precision: 0.9698, recall: 0.9679\n",
      "2018-12-27T14:20:55.888495, step: 1974, loss: 0.028001220896840096, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:20:56.022018, step: 1975, loss: 0.036550745368003845, acc: 0.9844, auc: 0.9995, precision: 0.9815, recall: 0.9868\n",
      "2018-12-27T14:20:56.149014, step: 1976, loss: 0.0319499745965004, acc: 0.9844, auc: 0.9995, precision: 0.9873, recall: 0.9804\n",
      "2018-12-27T14:20:56.286558, step: 1977, loss: 0.016904477030038834, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:20:56.415402, step: 1978, loss: 0.017856568098068237, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:20:56.554192, step: 1979, loss: 0.031041165813803673, acc: 0.9922, auc: 0.9995, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:20:56.699209, step: 1980, loss: 0.044728707522153854, acc: 0.9844, auc: 0.9988, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:20:56.826244, step: 1981, loss: 0.035808950662612915, acc: 0.9922, auc: 0.9995, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:20:56.962830, step: 1982, loss: 0.027346454560756683, acc: 0.9922, auc: 0.9995, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:20:57.098090, step: 1983, loss: 0.034477945417165756, acc: 0.9844, auc: 1.0, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:20:57.236213, step: 1984, loss: 0.03490776568651199, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:57.356025, step: 1985, loss: 0.037533778697252274, acc: 0.9844, auc: 0.9998, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:20:57.494742, step: 1986, loss: 0.014763972721993923, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:57.631180, step: 1987, loss: 0.023088615387678146, acc: 0.9922, auc: 0.9995, precision: 0.9904, recall: 0.9935\n",
      "2018-12-27T14:20:57.768201, step: 1988, loss: 0.021111804991960526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:57.894689, step: 1989, loss: 0.03628326579928398, acc: 0.9688, auc: 1.0, precision: 0.9677, recall: 0.9714\n",
      "2018-12-27T14:20:58.020716, step: 1990, loss: 0.04724486917257309, acc: 0.9766, auc: 0.9998, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:20:58.160073, step: 1991, loss: 0.045129939913749695, acc: 0.9766, auc: 0.9998, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:20:58.289774, step: 1992, loss: 0.0606185682117939, acc: 0.9766, auc: 0.9995, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:20:58.416104, step: 1993, loss: 0.12276207655668259, acc: 0.9375, auc: 0.999, precision: 0.942, recall: 0.9403\n",
      "2018-12-27T14:20:58.551848, step: 1994, loss: 0.08354958891868591, acc: 0.9609, auc: 1.0, precision: 0.9658, recall: 0.9583\n",
      "2018-12-27T14:20:58.679182, step: 1995, loss: 0.03996557742357254, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:20:58.811694, step: 1996, loss: 0.0708174929022789, acc: 0.9844, auc: 0.9949, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:20:58.949357, step: 1997, loss: 0.06481263786554337, acc: 0.9688, auc: 0.9995, precision: 0.9701, recall: 0.9692\n",
      "2018-12-27T14:20:59.084254, step: 1998, loss: 0.08300407230854034, acc: 0.9609, auc: 0.9985, precision: 0.9609, recall: 0.962\n",
      "2018-12-27T14:20:59.211244, step: 1999, loss: 0.015250231139361858, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:20:59.337127, step: 2000, loss: 0.05758003890514374, acc: 0.9844, auc: 0.9973, precision: 0.9859, recall: 0.9831\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:21:04.068466, step: 2000, loss: 1.4970812860288119, acc: 0.6694052631578946, auc: 0.725042105263158, precision: 0.6700973684210525, recall: 0.6693394736842105\n",
      "2018-12-27T14:21:04.187985, step: 2001, loss: 0.04380878433585167, acc: 0.9844, auc: 0.999, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:21:04.311953, step: 2002, loss: 0.019941505044698715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:04.434472, step: 2003, loss: 0.018118340522050858, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:21:04.564036, step: 2004, loss: 0.026489226147532463, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:21:04.703127, step: 2005, loss: 0.023992758244276047, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:04.840969, step: 2006, loss: 0.04859492927789688, acc: 0.9766, auc: 0.999, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:21:04.976560, step: 2007, loss: 0.024411309510469437, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:21:05.100413, step: 2008, loss: 0.029778029769659042, acc: 0.9922, auc: 0.9995, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:21:05.237731, step: 2009, loss: 0.00908249244093895, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:21:05.363689, step: 2010, loss: 0.018816489726305008, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:05.508156, step: 2011, loss: 0.02550671249628067, acc: 0.9844, auc: 1.0, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:21:05.642199, step: 2012, loss: 0.05939650535583496, acc: 0.9844, auc: 0.9983, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:21:05.773398, step: 2013, loss: 0.04206749051809311, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:21:05.906952, step: 2014, loss: 0.08826223760843277, acc: 0.9766, auc: 0.9995, precision: 0.9762, recall: 0.9779\n",
      "2018-12-27T14:21:06.034136, step: 2015, loss: 0.04847954958677292, acc: 0.9766, auc: 1.0, precision: 0.98, recall: 0.9732\n",
      "start training model\n",
      "2018-12-27T14:21:06.318031, step: 2016, loss: 0.009533189237117767, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:06.448140, step: 2017, loss: 0.01698126643896103, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:21:06.578145, step: 2018, loss: 0.014730799943208694, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:21:06.703008, step: 2019, loss: 0.011612117290496826, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:21:06.836679, step: 2020, loss: 0.002120398683473468, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:06.980859, step: 2021, loss: 0.004323145374655724, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:07.116515, step: 2022, loss: 0.01095929928123951, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:07.245837, step: 2023, loss: 0.009027628228068352, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:21:07.383255, step: 2024, loss: 0.025813071057200432, acc: 0.9922, auc: 0.9998, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:21:07.511413, step: 2025, loss: 0.01252913847565651, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:07.635927, step: 2026, loss: 0.0028559276834130287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:07.768830, step: 2027, loss: 0.009402241557836533, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:07.897358, step: 2028, loss: 0.0037749281618744135, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:08.025161, step: 2029, loss: 0.002795188222080469, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:08.164634, step: 2030, loss: 0.004872405901551247, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:08.294916, step: 2031, loss: 0.007633526809513569, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:08.427324, step: 2032, loss: 0.018588995561003685, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:08.552790, step: 2033, loss: 0.1291823536157608, acc: 0.9531, auc: 0.9993, precision: 0.9589, recall: 0.9508\n",
      "2018-12-27T14:21:08.674940, step: 2034, loss: 0.1326371133327484, acc: 0.9531, auc: 1.0, precision: 0.9595, recall: 0.95\n",
      "2018-12-27T14:21:08.813047, step: 2035, loss: 0.06353581696748734, acc: 0.9766, auc: 0.998, precision: 0.9795, recall: 0.9741\n",
      "2018-12-27T14:21:08.959583, step: 2036, loss: 0.04213445633649826, acc: 0.9688, auc: 0.9998, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:21:09.091908, step: 2037, loss: 0.03014874830842018, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:09.233464, step: 2038, loss: 0.027254721149802208, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:21:09.378752, step: 2039, loss: 0.07097946107387543, acc: 0.9766, auc: 0.9995, precision: 0.9766, recall: 0.9776\n",
      "2018-12-27T14:21:09.509029, step: 2040, loss: 0.128059983253479, acc: 0.9375, auc: 0.9936, precision: 0.9384, recall: 0.9384\n",
      "2018-12-27T14:21:09.636149, step: 2041, loss: 0.06724453717470169, acc: 0.9844, auc: 0.998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:21:09.781047, step: 2042, loss: 0.04903777688741684, acc: 0.9766, auc: 0.998, precision: 0.9747, recall: 0.9772\n",
      "2018-12-27T14:21:09.913467, step: 2043, loss: 0.04238408803939819, acc: 0.9844, auc: 1.0, precision: 0.9861, recall: 0.9828\n",
      "2018-12-27T14:21:10.038074, step: 2044, loss: 0.03172623738646507, acc: 0.9766, auc: 1.0, precision: 0.9762, recall: 0.9779\n",
      "2018-12-27T14:21:10.165225, step: 2045, loss: 0.03716506063938141, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:21:10.290664, step: 2046, loss: 0.0096171535551548, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:10.420672, step: 2047, loss: 0.003628529142588377, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:10.549722, step: 2048, loss: 0.022251971065998077, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:10.677655, step: 2049, loss: 0.040356751531362534, acc: 0.9688, auc: 1.0, precision: 0.9692, recall: 0.9701\n",
      "2018-12-27T14:21:10.815730, step: 2050, loss: 0.07217100262641907, acc: 0.9766, auc: 0.9983, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:21:10.948554, step: 2051, loss: 0.028699379414319992, acc: 0.9844, auc: 1.0, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:21:11.068716, step: 2052, loss: 0.027942270040512085, acc: 0.9844, auc: 0.9998, precision: 0.9825, recall: 0.9863\n",
      "2018-12-27T14:21:11.208919, step: 2053, loss: 0.01816248521208763, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:11.347870, step: 2054, loss: 0.010899833403527737, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:11.485711, step: 2055, loss: 0.013029161840677261, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:21:11.620741, step: 2056, loss: 0.013746274635195732, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:11.747393, step: 2057, loss: 0.03399774804711342, acc: 0.9922, auc: 0.999, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:11.875940, step: 2058, loss: 0.054232485592365265, acc: 0.9844, auc: 0.999, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:21:11.997276, step: 2059, loss: 0.07078597694635391, acc: 0.9922, auc: 0.999, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:12.135504, step: 2060, loss: 0.19611656665802002, acc: 0.9375, auc: 0.9943, precision: 0.9506, recall: 0.9273\n",
      "2018-12-27T14:21:12.268360, step: 2061, loss: 0.04531760513782501, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:21:12.413128, step: 2062, loss: 0.027003547176718712, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:12.538283, step: 2063, loss: 0.0072806342504918575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:12.664648, step: 2064, loss: 0.024834247305989265, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:21:12.799616, step: 2065, loss: 0.00956242997199297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:12.933249, step: 2066, loss: 0.05265297740697861, acc: 0.9922, auc: 0.9985, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:21:13.055233, step: 2067, loss: 0.017848405987024307, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:21:13.191286, step: 2068, loss: 0.005173802375793457, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:13.327082, step: 2069, loss: 0.019024189561605453, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:13.463757, step: 2070, loss: 0.027135752141475677, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:13.593026, step: 2071, loss: 0.007657346315681934, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:13.729633, step: 2072, loss: 0.007358195260167122, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:13.860014, step: 2073, loss: 0.012880101799964905, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:13.986058, step: 2074, loss: 0.026170875877141953, acc: 0.9922, auc: 0.9998, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:21:14.120208, step: 2075, loss: 0.05359126627445221, acc: 0.9766, auc: 0.9971, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:21:14.254908, step: 2076, loss: 0.029915381222963333, acc: 0.9922, auc: 0.9995, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:21:14.395280, step: 2077, loss: 0.025854751467704773, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:14.535905, step: 2078, loss: 0.02554478868842125, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:14.669823, step: 2079, loss: 0.011157195083796978, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:14.811810, step: 2080, loss: 0.02154889702796936, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:14.948394, step: 2081, loss: 0.03533347323536873, acc: 0.9922, auc: 0.9995, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:21:15.085728, step: 2082, loss: 0.03523121029138565, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:21:15.221329, step: 2083, loss: 0.0658707320690155, acc: 0.9688, auc: 0.9975, precision: 0.9683, recall: 0.9683\n",
      "2018-12-27T14:21:15.355514, step: 2084, loss: 0.03644707798957825, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:21:15.477806, step: 2085, loss: 0.05769721418619156, acc: 0.9609, auc: 1.0, precision: 0.9691, recall: 0.9519\n",
      "2018-12-27T14:21:15.607344, step: 2086, loss: 0.1066228523850441, acc: 0.9688, auc: 0.9976, precision: 0.9694, recall: 0.9685\n",
      "2018-12-27T14:21:15.732637, step: 2087, loss: 0.01546025276184082, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:15.860366, step: 2088, loss: 0.023874877020716667, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:21:15.994050, step: 2089, loss: 0.0254049189388752, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:21:16.128482, step: 2090, loss: 0.0027959905564785004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:16.251234, step: 2091, loss: 0.05094992369413376, acc: 0.9844, auc: 0.9992, precision: 0.9818, recall: 0.9867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:21:16.378720, step: 2092, loss: 0.08788610994815826, acc: 0.9766, auc: 1.0, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:21:16.505868, step: 2093, loss: 0.11123454570770264, acc: 0.9609, auc: 0.9988, precision: 0.9648, recall: 0.9597\n",
      "2018-12-27T14:21:16.644624, step: 2094, loss: 0.05425208806991577, acc: 0.9766, auc: 0.9995, precision: 0.975, recall: 0.9789\n",
      "2018-12-27T14:21:16.762437, step: 2095, loss: 0.030855346471071243, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:16.899930, step: 2096, loss: 0.05087532848119736, acc: 0.9844, auc: 0.9988, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:21:17.029147, step: 2097, loss: 0.015391413122415543, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:21:17.171317, step: 2098, loss: 0.039105042815208435, acc: 0.9922, auc: 0.9998, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:21:17.301206, step: 2099, loss: 0.012778900563716888, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:17.438179, step: 2100, loss: 0.09226331114768982, acc: 0.9531, auc: 0.9983, precision: 0.9563, recall: 0.951\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:21:22.245932, step: 2100, loss: 1.5867708199902584, acc: 0.6624105263157894, auc: 0.7247947368421053, precision: 0.6635657894736843, recall: 0.6623631578947368\n",
      "2018-12-27T14:21:22.379870, step: 2101, loss: 0.016716962680220604, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:22.518358, step: 2102, loss: 0.018626481294631958, acc: 0.9922, auc: 1.0, precision: 0.9906, recall: 0.9934\n",
      "2018-12-27T14:21:22.653338, step: 2103, loss: 0.025827394798398018, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:21:22.791739, step: 2104, loss: 0.0330515131354332, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:21:22.924164, step: 2105, loss: 0.03571266680955887, acc: 0.9922, auc: 0.9998, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:21:23.066706, step: 2106, loss: 0.039498042315244675, acc: 0.9922, auc: 0.9995, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:21:23.209282, step: 2107, loss: 0.05432413890957832, acc: 0.9688, auc: 0.999, precision: 0.9726, recall: 0.9661\n",
      "2018-12-27T14:21:23.341227, step: 2108, loss: 0.10612055659294128, acc: 0.9688, auc: 0.9961, precision: 0.9685, recall: 0.9694\n",
      "2018-12-27T14:21:23.474249, step: 2109, loss: 0.038716137409210205, acc: 0.9766, auc: 0.999, precision: 0.9764, recall: 0.9768\n",
      "2018-12-27T14:21:23.602054, step: 2110, loss: 0.04165657237172127, acc: 0.9844, auc: 0.999, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:21:23.734149, step: 2111, loss: 0.04524107277393341, acc: 0.9844, auc: 0.9993, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:21:23.860838, step: 2112, loss: 0.06556793302297592, acc: 0.9688, auc: 0.999, precision: 0.9688, recall: 0.9706\n",
      "2018-12-27T14:21:23.989490, step: 2113, loss: 0.0915948748588562, acc: 0.9609, auc: 0.9995, precision: 0.9621, recall: 0.9627\n",
      "2018-12-27T14:21:24.119481, step: 2114, loss: 0.10334258526563644, acc: 0.9688, auc: 0.9998, precision: 0.9667, recall: 0.9722\n",
      "2018-12-27T14:21:24.243002, step: 2115, loss: 0.02796938084065914, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:24.370951, step: 2116, loss: 0.0700647160410881, acc: 0.9766, auc: 0.9974, precision: 0.9717, recall: 0.9808\n",
      "2018-12-27T14:21:24.506348, step: 2117, loss: 0.035156697034835815, acc: 0.9844, auc: 0.9995, precision: 0.9818, recall: 0.9867\n",
      "2018-12-27T14:21:24.648690, step: 2118, loss: 0.024140959605574608, acc: 0.9922, auc: 0.9998, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:24.792198, step: 2119, loss: 0.022314228117465973, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:24.924060, step: 2120, loss: 0.008973448537290096, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:25.062320, step: 2121, loss: 0.0137824397534132, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:21:25.197719, step: 2122, loss: 0.004985299427062273, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:25.335267, step: 2123, loss: 0.02994859218597412, acc: 0.9922, auc: 0.9998, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:21:25.468553, step: 2124, loss: 0.027964584529399872, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:25.605272, step: 2125, loss: 0.053203389048576355, acc: 0.9922, auc: 0.9978, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:21:25.732855, step: 2126, loss: 0.026650041341781616, acc: 0.9844, auc: 0.9997, precision: 0.987, recall: 0.9811\n",
      "2018-12-27T14:21:25.866151, step: 2127, loss: 0.028074152767658234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:26.001174, step: 2128, loss: 0.03707539290189743, acc: 0.9688, auc: 1.0, precision: 0.9672, recall: 0.9718\n",
      "2018-12-27T14:21:26.130484, step: 2129, loss: 0.0832023024559021, acc: 0.9844, auc: 0.998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:26.259192, step: 2130, loss: 0.014964580535888672, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:26.382481, step: 2131, loss: 0.013541504740715027, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:21:26.508705, step: 2132, loss: 0.036236394196748734, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:21:26.645073, step: 2133, loss: 0.06371188163757324, acc: 0.9688, auc: 0.9976, precision: 0.9696, recall: 0.9682\n",
      "2018-12-27T14:21:26.781392, step: 2134, loss: 0.029509618878364563, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:21:26.922662, step: 2135, loss: 0.02117476984858513, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:27.063850, step: 2136, loss: 0.01938968524336815, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:21:27.197998, step: 2137, loss: 0.01248806994408369, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:27.331187, step: 2138, loss: 0.013808615505695343, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:27.457883, step: 2139, loss: 0.04875648021697998, acc: 0.9844, auc: 0.9988, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:21:27.585612, step: 2140, loss: 0.06683880090713501, acc: 0.9688, auc: 0.9988, precision: 0.9694, recall: 0.9685\n",
      "2018-12-27T14:21:27.721881, step: 2141, loss: 0.014956851489841938, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:27.851036, step: 2142, loss: 0.04466899856925011, acc: 0.9844, auc: 0.999, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:27.980360, step: 2143, loss: 0.026873428374528885, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:21:28.106497, step: 2144, loss: 0.07516519725322723, acc: 0.9531, auc: 1.0, precision: 0.9595, recall: 0.95\n",
      "2018-12-27T14:21:28.238228, step: 2145, loss: 0.23104701936244965, acc: 0.9609, auc: 0.9971, precision: 0.9658, recall: 0.9583\n",
      "2018-12-27T14:21:28.377336, step: 2146, loss: 0.15540041029453278, acc: 0.9375, auc: 0.9954, precision: 0.9425, recall: 0.936\n",
      "2018-12-27T14:21:28.521579, step: 2147, loss: 0.060999784618616104, acc: 0.9688, auc: 0.9993, precision: 0.9694, recall: 0.9685\n",
      "2018-12-27T14:21:28.655920, step: 2148, loss: 0.029279526323080063, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:21:28.794288, step: 2149, loss: 0.03540301322937012, acc: 0.9844, auc: 0.9993, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:21:28.923899, step: 2150, loss: 0.022911792621016502, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:21:29.061796, step: 2151, loss: 0.019025038927793503, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:21:29.194579, step: 2152, loss: 0.01043029222637415, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:29.323664, step: 2153, loss: 0.04547545313835144, acc: 0.9844, auc: 0.9995, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:21:29.456696, step: 2154, loss: 0.023620054125785828, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:21:29.583363, step: 2155, loss: 0.06184053421020508, acc: 0.9688, auc: 0.999, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:21:29.719054, step: 2156, loss: 0.015685828402638435, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:21:29.853946, step: 2157, loss: 0.015636498108506203, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:21:29.987610, step: 2158, loss: 0.0086705656722188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:30.125158, step: 2159, loss: 0.06280208379030228, acc: 0.9766, auc: 0.9985, precision: 0.9769, recall: 0.9763\n",
      "2018-12-27T14:21:30.273517, step: 2160, loss: 0.029143299907445908, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:21:30.407191, step: 2161, loss: 0.1412433385848999, acc: 0.8984, auc: 1.0, precision: 0.9145, recall: 0.9\n",
      "2018-12-27T14:21:30.538747, step: 2162, loss: 0.23264318704605103, acc: 0.9141, auc: 1.0, precision: 0.9267, recall: 0.9141\n",
      "2018-12-27T14:21:30.667698, step: 2163, loss: 0.15233182907104492, acc: 0.9219, auc: 0.9983, precision: 0.9351, recall: 0.918\n",
      "2018-12-27T14:21:30.791580, step: 2164, loss: 0.031634002923965454, acc: 0.9844, auc: 1.0, precision: 0.9877, recall: 0.9796\n",
      "2018-12-27T14:21:30.925047, step: 2165, loss: 0.043896373361349106, acc: 0.9688, auc: 0.9989, precision: 0.9619, recall: 0.9708\n",
      "2018-12-27T14:21:31.048762, step: 2166, loss: 0.04441579803824425, acc: 0.9922, auc: 0.9993, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:21:31.180227, step: 2167, loss: 0.05450472980737686, acc: 0.9766, auc: 0.9983, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:21:31.318973, step: 2168, loss: 0.03609254211187363, acc: 0.9766, auc: 0.999, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:21:31.457760, step: 2169, loss: 0.0393773578107357, acc: 0.9766, auc: 0.9993, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:21:31.585226, step: 2170, loss: 0.07136347889900208, acc: 0.9688, auc: 0.998, precision: 0.9697, recall: 0.9697\n",
      "start training model\n",
      "2018-12-27T14:21:31.940405, step: 2171, loss: 0.0046645645052194595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:32.074818, step: 2172, loss: 0.0034758190158754587, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:32.215375, step: 2173, loss: 0.02461395040154457, acc: 0.9922, auc: 0.9998, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:21:32.357493, step: 2174, loss: 0.013132455758750439, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:32.483894, step: 2175, loss: 0.02791876345872879, acc: 0.9766, auc: 0.9998, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:21:32.614080, step: 2176, loss: 0.009182548150420189, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:32.749862, step: 2177, loss: 0.0011771460995078087, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:32.890259, step: 2178, loss: 0.00553872948512435, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:33.021594, step: 2179, loss: 0.008322847075760365, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:33.138204, step: 2180, loss: 0.004681548103690147, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:33.267893, step: 2181, loss: 0.010953905992209911, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:33.394587, step: 2182, loss: 0.0038709843065589666, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:33.529253, step: 2183, loss: 0.00821877270936966, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:21:33.659074, step: 2184, loss: 0.038235247135162354, acc: 0.9922, auc: 0.9993, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:21:33.783736, step: 2185, loss: 0.007077085319906473, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:21:33.917837, step: 2186, loss: 0.0022534842137247324, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.052477, step: 2187, loss: 0.006323947571218014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.187199, step: 2188, loss: 0.00634782062843442, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.325896, step: 2189, loss: 0.0048844413831830025, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.466604, step: 2190, loss: 0.004850334487855434, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.591561, step: 2191, loss: 0.0008965322049334645, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.721701, step: 2192, loss: 0.004211208783090115, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:34.853690, step: 2193, loss: 0.004886748269200325, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:21:34.988951, step: 2194, loss: 0.007583721540868282, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:35.118887, step: 2195, loss: 0.013886529952287674, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:35.250513, step: 2196, loss: 0.010472931899130344, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:21:35.372158, step: 2197, loss: 0.02019726298749447, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:35.500087, step: 2198, loss: 0.05960173159837723, acc: 0.9453, auc: 1.0, precision: 0.947, recall: 0.9493\n",
      "2018-12-27T14:21:35.621172, step: 2199, loss: 0.40255579352378845, acc: 0.8906, auc: 1.0, precision: 0.9079, recall: 0.8939\n",
      "2018-12-27T14:21:35.743490, step: 2200, loss: 0.24201831221580505, acc: 0.9141, auc: 0.9988, precision: 0.9247, recall: 0.9167\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:21:40.602563, step: 2200, loss: 1.5759940649333752, acc: 0.6426789473684211, auc: 0.6967684210526315, precision: 0.6427815789473684, recall: 0.6419157894736842\n",
      "2018-12-27T14:21:40.732517, step: 2201, loss: 0.031153075397014618, acc: 0.9922, auc: 0.9995, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:21:40.862863, step: 2202, loss: 0.06648747622966766, acc: 0.9844, auc: 0.9983, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:21:41.005804, step: 2203, loss: 0.024947037920355797, acc: 0.9766, auc: 1.0, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:21:41.141410, step: 2204, loss: 0.00967046245932579, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:41.270128, step: 2205, loss: 0.015285009518265724, acc: 0.9922, auc: 1.0, precision: 0.9935, recall: 0.9904\n",
      "2018-12-27T14:21:41.411749, step: 2206, loss: 0.015897877514362335, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:41.549277, step: 2207, loss: 0.015800632536411285, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:21:41.686114, step: 2208, loss: 0.006391603033989668, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:41.824845, step: 2209, loss: 0.020755084231495857, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:41.957363, step: 2210, loss: 0.005915017332881689, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:42.089205, step: 2211, loss: 0.043179549276828766, acc: 0.9766, auc: 0.9998, precision: 0.9795, recall: 0.9741\n",
      "2018-12-27T14:21:42.218600, step: 2212, loss: 0.013967047445476055, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:42.345877, step: 2213, loss: 0.026598919183015823, acc: 0.9766, auc: 0.9995, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:21:42.474080, step: 2214, loss: 0.005156384781002998, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:42.616924, step: 2215, loss: 0.041328731924295425, acc: 0.9922, auc: 0.9993, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:21:42.745453, step: 2216, loss: 0.026464471593499184, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:42.886106, step: 2217, loss: 0.032858189195394516, acc: 0.9766, auc: 0.9998, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:21:43.018401, step: 2218, loss: 0.016166508197784424, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:43.156456, step: 2219, loss: 0.021765656769275665, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:21:43.289178, step: 2220, loss: 0.04382377117872238, acc: 0.9766, auc: 0.9988, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:21:43.429021, step: 2221, loss: 0.026924941688776016, acc: 0.9922, auc: 0.9997, precision: 0.9935, recall: 0.9904\n",
      "2018-12-27T14:21:43.569234, step: 2222, loss: 0.023772772401571274, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:21:43.714214, step: 2223, loss: 0.013196147978305817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:21:43.840991, step: 2224, loss: 0.011906956322491169, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:21:43.978837, step: 2225, loss: 0.018200917169451714, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:44.111208, step: 2226, loss: 0.004180961288511753, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:44.254270, step: 2227, loss: 0.008951440453529358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:44.391073, step: 2228, loss: 0.020580224692821503, acc: 0.9922, auc: 0.9998, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:21:44.523453, step: 2229, loss: 0.021523140370845795, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:21:44.648938, step: 2230, loss: 0.010205250233411789, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:44.778051, step: 2231, loss: 0.016745714470744133, acc: 0.9844, auc: 1.0, precision: 0.9818, recall: 0.9867\n",
      "2018-12-27T14:21:44.915599, step: 2232, loss: 0.01832682080566883, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:45.033602, step: 2233, loss: 0.016408218070864677, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:21:45.167351, step: 2234, loss: 0.016185028478503227, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:45.302719, step: 2235, loss: 0.01818215660750866, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:45.432270, step: 2236, loss: 0.00824921764433384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:45.557893, step: 2237, loss: 0.017085887491703033, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:21:45.694576, step: 2238, loss: 0.024816298857331276, acc: 0.9922, auc: 0.9995, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:21:45.830239, step: 2239, loss: 0.006459149997681379, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:45.964451, step: 2240, loss: 0.022091489285230637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:46.098953, step: 2241, loss: 0.08817287534475327, acc: 0.9609, auc: 1.0, precision: 0.9643, recall: 0.9603\n",
      "2018-12-27T14:21:46.227297, step: 2242, loss: 0.1404055655002594, acc: 0.9688, auc: 0.9995, precision: 0.9683, recall: 0.971\n",
      "2018-12-27T14:21:46.370544, step: 2243, loss: 0.06074784696102142, acc: 0.9766, auc: 1.0, precision: 0.9795, recall: 0.9741\n",
      "2018-12-27T14:21:46.511041, step: 2244, loss: 0.04961209371685982, acc: 0.9609, auc: 0.9985, precision: 0.9605, recall: 0.9613\n",
      "2018-12-27T14:21:46.638667, step: 2245, loss: 0.06924527883529663, acc: 0.9766, auc: 0.9988, precision: 0.977, recall: 0.9761\n",
      "2018-12-27T14:21:46.765693, step: 2246, loss: 0.06254678964614868, acc: 0.9844, auc: 0.9995, precision: 0.987, recall: 0.9811\n",
      "2018-12-27T14:21:46.900293, step: 2247, loss: 0.03580702841281891, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:21:47.032484, step: 2248, loss: 0.014737153425812721, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:47.180175, step: 2249, loss: 0.013085803017020226, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:47.319680, step: 2250, loss: 0.019316980615258217, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:47.447429, step: 2251, loss: 0.017862394452095032, acc: 0.9922, auc: 1.0, precision: 0.99, recall: 0.9937\n",
      "2018-12-27T14:21:47.582448, step: 2252, loss: 0.005911036394536495, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:47.722882, step: 2253, loss: 0.01275668852031231, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:21:47.855145, step: 2254, loss: 0.009062004275619984, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:47.981496, step: 2255, loss: 0.0457303561270237, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:21:48.117046, step: 2256, loss: 0.0665755569934845, acc: 0.9688, auc: 0.9995, precision: 0.9718, recall: 0.9672\n",
      "2018-12-27T14:21:48.251288, step: 2257, loss: 0.031267959624528885, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:21:48.384913, step: 2258, loss: 0.04337812960147858, acc: 0.9844, auc: 0.9995, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:21:48.520416, step: 2259, loss: 0.07844619452953339, acc: 0.9766, auc: 0.9963, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:21:48.653606, step: 2260, loss: 0.032617151737213135, acc: 0.9844, auc: 0.9995, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:48.780793, step: 2261, loss: 0.007610707078129053, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:48.910471, step: 2262, loss: 0.025872550904750824, acc: 0.9844, auc: 0.9998, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:21:49.040532, step: 2263, loss: 0.0022158522624522448, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:49.178693, step: 2264, loss: 0.00430614547803998, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:49.317915, step: 2265, loss: 0.006704243365675211, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:49.456864, step: 2266, loss: 0.042926788330078125, acc: 0.9844, auc: 0.9998, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:21:49.591817, step: 2267, loss: 0.04817517101764679, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:21:49.719306, step: 2268, loss: 0.1375522017478943, acc: 0.9609, auc: 0.9973, precision: 0.9633, recall: 0.9589\n",
      "2018-12-27T14:21:49.847955, step: 2269, loss: 0.12143270671367645, acc: 0.9297, auc: 1.0, precision: 0.9423, recall: 0.9237\n",
      "2018-12-27T14:21:49.987659, step: 2270, loss: 0.1041794866323471, acc: 0.9609, auc: 0.9992, precision: 0.9706, recall: 0.9479\n",
      "2018-12-27T14:21:50.115470, step: 2271, loss: 0.16046404838562012, acc: 0.9453, auc: 0.9922, precision: 0.948, recall: 0.9453\n",
      "2018-12-27T14:21:50.251750, step: 2272, loss: 0.1086377501487732, acc: 0.9609, auc: 0.9973, precision: 0.9606, recall: 0.9623\n",
      "2018-12-27T14:21:50.381448, step: 2273, loss: 0.05571431666612625, acc: 0.9766, auc: 0.999, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:21:50.524902, step: 2274, loss: 0.057654205709695816, acc: 0.9688, auc: 0.9985, precision: 0.9694, recall: 0.9685\n",
      "2018-12-27T14:21:50.656210, step: 2275, loss: 0.04437298700213432, acc: 0.9766, auc: 1.0, precision: 0.9727, recall: 0.9803\n",
      "2018-12-27T14:21:50.794711, step: 2276, loss: 0.0231642946600914, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:21:50.923136, step: 2277, loss: 0.08092984557151794, acc: 0.9688, auc: 0.9973, precision: 0.9692, recall: 0.9688\n",
      "2018-12-27T14:21:51.054909, step: 2278, loss: 0.045918144285678864, acc: 0.9844, auc: 0.9988, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:21:51.193873, step: 2279, loss: 0.02637277916073799, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:21:51.321498, step: 2280, loss: 0.038263555616140366, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:21:51.453867, step: 2281, loss: 0.03941011801362038, acc: 0.9844, auc: 0.999, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:21:51.587920, step: 2282, loss: 0.016224049031734467, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:51.721876, step: 2283, loss: 0.009245998226106167, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:51.854873, step: 2284, loss: 0.017601966857910156, acc: 0.9922, auc: 0.9998, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:21:51.989876, step: 2285, loss: 0.0669773519039154, acc: 0.9766, auc: 0.9988, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:21:52.118445, step: 2286, loss: 0.0035155918449163437, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:52.243935, step: 2287, loss: 0.03696214035153389, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:21:52.381575, step: 2288, loss: 0.013651402667164803, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:52.509764, step: 2289, loss: 0.03323329985141754, acc: 0.9922, auc: 0.9995, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:21:52.650390, step: 2290, loss: 0.01127859391272068, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:52.781570, step: 2291, loss: 0.005682401359081268, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:21:52.904121, step: 2292, loss: 0.0030564137268811464, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:21:53.040604, step: 2293, loss: 0.022892702370882034, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:21:53.169881, step: 2294, loss: 0.025976743549108505, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:53.304959, step: 2295, loss: 0.053418803960084915, acc: 0.9766, auc: 1.0, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:21:53.435688, step: 2296, loss: 0.25820791721343994, acc: 0.9141, auc: 0.998, precision: 0.9179, recall: 0.9236\n",
      "2018-12-27T14:21:53.561123, step: 2297, loss: 0.46887701749801636, acc: 0.875, auc: 0.9897, precision: 0.8869, recall: 0.8842\n",
      "2018-12-27T14:21:53.687845, step: 2298, loss: 0.1578969657421112, acc: 0.9453, auc: 0.9995, precision: 0.95, recall: 0.9462\n",
      "2018-12-27T14:21:53.820277, step: 2299, loss: 0.03291381895542145, acc: 0.9766, auc: 0.9993, precision: 0.9752, recall: 0.9772\n",
      "2018-12-27T14:21:53.938476, step: 2300, loss: 0.0384543240070343, acc: 0.9766, auc: 0.9993, precision: 0.9766, recall: 0.9767\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:21:58.723199, step: 2300, loss: 1.5342632281152826, acc: 0.658507894736842, auc: 0.7225763157894735, precision: 0.6579894736842105, recall: 0.6583684210526315\n",
      "2018-12-27T14:21:58.842174, step: 2301, loss: 0.021576276049017906, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:21:58.967415, step: 2302, loss: 0.069914311170578, acc: 0.9688, auc: 0.997, precision: 0.9673, recall: 0.9701\n",
      "2018-12-27T14:21:59.097406, step: 2303, loss: 0.031330980360507965, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:21:59.227109, step: 2304, loss: 0.04077649116516113, acc: 0.9766, auc: 0.9998, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:21:59.353331, step: 2305, loss: 0.05075191706418991, acc: 0.9844, auc: 0.9976, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:21:59.500878, step: 2306, loss: 0.03162790462374687, acc: 0.9922, auc: 0.9993, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:21:59.629193, step: 2307, loss: 0.08511637896299362, acc: 0.9609, auc: 0.9973, precision: 0.9603, recall: 0.9643\n",
      "2018-12-27T14:21:59.760639, step: 2308, loss: 0.06101265549659729, acc: 0.9844, auc: 0.999, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:21:59.894212, step: 2309, loss: 0.007253500632941723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:00.027123, step: 2310, loss: 0.020009318366646767, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:22:00.166296, step: 2311, loss: 0.045331865549087524, acc: 0.9766, auc: 0.999, precision: 0.9805, recall: 0.9722\n",
      "2018-12-27T14:22:00.299913, step: 2312, loss: 0.06567542999982834, acc: 0.9688, auc: 0.999, precision: 0.9682, recall: 0.9696\n",
      "2018-12-27T14:22:00.432683, step: 2313, loss: 0.0040379599668085575, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:00.561196, step: 2314, loss: 0.022236324846744537, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:00.700350, step: 2315, loss: 0.020150482654571533, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:22:00.831290, step: 2316, loss: 0.033872708678245544, acc: 0.9766, auc: 0.9985, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:22:00.963108, step: 2317, loss: 0.04960303753614426, acc: 0.9844, auc: 0.998, precision: 0.9861, recall: 0.9828\n",
      "2018-12-27T14:22:01.099089, step: 2318, loss: 0.0155051713809371, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:01.244857, step: 2319, loss: 0.01410556398332119, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:01.376755, step: 2320, loss: 0.07468535006046295, acc: 0.9609, auc: 0.9973, precision: 0.9605, recall: 0.9613\n",
      "2018-12-27T14:22:01.510920, step: 2321, loss: 0.013186823576688766, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:01.643554, step: 2322, loss: 0.1118246465921402, acc: 0.9375, auc: 0.9968, precision: 0.9425, recall: 0.936\n",
      "2018-12-27T14:22:01.779041, step: 2323, loss: 0.11541429162025452, acc: 0.9609, auc: 1.0, precision: 0.9609, recall: 0.9638\n",
      "2018-12-27T14:22:01.912449, step: 2324, loss: 0.11335253715515137, acc: 0.9375, auc: 0.9995, precision: 0.9444, recall: 0.9375\n",
      "2018-12-27T14:22:02.053890, step: 2325, loss: 0.010947946459054947, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2018-12-27T14:22:02.396538, step: 2326, loss: 0.023084644228219986, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:02.538929, step: 2327, loss: 0.025620408356189728, acc: 0.9922, auc: 1.0, precision: 0.9937, recall: 0.99\n",
      "2018-12-27T14:22:02.673513, step: 2328, loss: 0.006865834817290306, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:02.800369, step: 2329, loss: 0.002764324191957712, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:02.935798, step: 2330, loss: 0.0035292694810777903, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:03.073032, step: 2331, loss: 0.004267315845936537, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:03.200579, step: 2332, loss: 0.006746686529368162, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:03.338828, step: 2333, loss: 0.025834016501903534, acc: 0.9922, auc: 0.9995, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:03.464275, step: 2334, loss: 0.019817881286144257, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:03.591362, step: 2335, loss: 0.01340631116181612, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:22:03.723159, step: 2336, loss: 0.045165129005908966, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:22:03.862754, step: 2337, loss: 0.0028492240235209465, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:03.998462, step: 2338, loss: 0.013277352787554264, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:04.135386, step: 2339, loss: 0.012714080512523651, acc: 0.9844, auc: 1.0, precision: 0.9815, recall: 0.9868\n",
      "2018-12-27T14:22:04.262414, step: 2340, loss: 0.02385636419057846, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:04.386514, step: 2341, loss: 0.0017712636617943645, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:04.518576, step: 2342, loss: 0.029988089576363564, acc: 0.9844, auc: 1.0, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:22:04.638077, step: 2343, loss: 0.020938124507665634, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:22:04.775254, step: 2344, loss: 0.02735329419374466, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:22:04.917788, step: 2345, loss: 0.004011993296444416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:05.056492, step: 2346, loss: 0.006873468868434429, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:05.184638, step: 2347, loss: 0.012850584462285042, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:05.316100, step: 2348, loss: 0.028011146932840347, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:22:05.446627, step: 2349, loss: 0.021250272169709206, acc: 0.9844, auc: 0.9997, precision: 0.9837, recall: 0.9837\n",
      "2018-12-27T14:22:05.581662, step: 2350, loss: 0.020518943667411804, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:05.712230, step: 2351, loss: 0.05339870974421501, acc: 0.9766, auc: 1.0, precision: 0.975, recall: 0.9789\n",
      "2018-12-27T14:22:05.844875, step: 2352, loss: 0.10153353959321976, acc: 0.9688, auc: 0.998, precision: 0.9708, recall: 0.9641\n",
      "2018-12-27T14:22:05.979407, step: 2353, loss: 0.02446657232940197, acc: 0.9844, auc: 1.0, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:22:06.126387, step: 2354, loss: 0.024830615147948265, acc: 0.9922, auc: 0.9998, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:22:06.263480, step: 2355, loss: 0.005706897471100092, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:06.397197, step: 2356, loss: 0.005218041595071554, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:06.527797, step: 2357, loss: 0.0023082853294909, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:22:06.664614, step: 2358, loss: 0.015909617766737938, acc: 0.9922, auc: 0.9998, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:06.804834, step: 2359, loss: 0.023371748626232147, acc: 0.9922, auc: 0.9995, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:06.950505, step: 2360, loss: 0.00278709689155221, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:07.088117, step: 2361, loss: 0.009418821893632412, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:07.225146, step: 2362, loss: 0.016557710245251656, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:07.366442, step: 2363, loss: 0.014761913567781448, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:07.503212, step: 2364, loss: 0.029449468478560448, acc: 0.9844, auc: 0.9998, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:22:07.637322, step: 2365, loss: 0.03087012469768524, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:07.774901, step: 2366, loss: 0.020979518070816994, acc: 0.9766, auc: 1.0, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:22:07.903522, step: 2367, loss: 0.00767612224444747, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:08.041325, step: 2368, loss: 0.01772509329020977, acc: 0.9922, auc: 0.9997, precision: 0.9907, recall: 0.9933\n",
      "2018-12-27T14:22:08.167991, step: 2369, loss: 0.004915275145322084, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:08.292388, step: 2370, loss: 0.010414794087409973, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:08.418022, step: 2371, loss: 0.018953023478388786, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:08.551474, step: 2372, loss: 0.009352217428386211, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:08.695373, step: 2373, loss: 0.045260582119226456, acc: 0.9688, auc: 0.9988, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:22:08.831807, step: 2374, loss: 0.022098146378993988, acc: 0.9766, auc: 1.0, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:22:08.966850, step: 2375, loss: 0.02488422766327858, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:09.092769, step: 2376, loss: 0.08339199423789978, acc: 0.9844, auc: 0.9988, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:22:09.217484, step: 2377, loss: 0.06574147194623947, acc: 0.9766, auc: 1.0, precision: 0.97, recall: 0.9815\n",
      "2018-12-27T14:22:09.338921, step: 2378, loss: 0.03148982673883438, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:09.471099, step: 2379, loss: 0.008654619567096233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:09.602775, step: 2380, loss: 0.028637651354074478, acc: 0.9922, auc: 0.9998, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:09.731496, step: 2381, loss: 0.013912001624703407, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:09.864680, step: 2382, loss: 0.005711505189538002, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:10.002161, step: 2383, loss: 0.047630250453948975, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:10.142033, step: 2384, loss: 0.024497471749782562, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:10.272632, step: 2385, loss: 0.027963723987340927, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:22:10.393678, step: 2386, loss: 0.05821070820093155, acc: 0.9531, auc: 1.0, precision: 0.9545, recall: 0.9559\n",
      "2018-12-27T14:22:10.525124, step: 2387, loss: 0.11343920230865479, acc: 0.9609, auc: 1.0, precision: 0.9632, recall: 0.9615\n",
      "2018-12-27T14:22:10.652043, step: 2388, loss: 0.04490426555275917, acc: 0.9766, auc: 1.0, precision: 0.9754, recall: 0.9786\n",
      "2018-12-27T14:22:10.789344, step: 2389, loss: 0.06225370988249779, acc: 0.9844, auc: 0.999, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:22:10.914801, step: 2390, loss: 0.08592286705970764, acc: 0.9531, auc: 0.9985, precision: 0.9577, recall: 0.9524\n",
      "2018-12-27T14:22:11.051743, step: 2391, loss: 0.03469962254166603, acc: 0.9766, auc: 0.9995, precision: 0.977, recall: 0.976\n",
      "2018-12-27T14:22:11.175391, step: 2392, loss: 0.002883054781705141, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:11.315233, step: 2393, loss: 0.027589842677116394, acc: 0.9844, auc: 0.9998, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:22:11.452988, step: 2394, loss: 0.005234135780483484, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:11.592080, step: 2395, loss: 0.018698574975132942, acc: 0.9844, auc: 0.9998, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:22:11.717436, step: 2396, loss: 0.03596043214201927, acc: 0.9766, auc: 0.9995, precision: 0.9772, recall: 0.9747\n",
      "2018-12-27T14:22:11.846532, step: 2397, loss: 0.004667205736041069, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:11.983818, step: 2398, loss: 0.008998218923807144, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:12.128730, step: 2399, loss: 0.024304017424583435, acc: 0.9922, auc: 0.9998, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:12.259635, step: 2400, loss: 0.017523519694805145, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:22:17.054856, step: 2400, loss: 1.6981639297384965, acc: 0.6607736842105263, auc: 0.7366894736842106, precision: 0.6676631578947368, recall: 0.6621868421052632\n",
      "2018-12-27T14:22:17.186908, step: 2401, loss: 0.005821769591420889, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:17.321473, step: 2402, loss: 0.013624145649373531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:17.467008, step: 2403, loss: 0.05996812507510185, acc: 0.9844, auc: 0.9961, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:17.602258, step: 2404, loss: 0.003119903849437833, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:17.733622, step: 2405, loss: 0.04929633438587189, acc: 0.9844, auc: 0.9985, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:17.875442, step: 2406, loss: 0.02689727395772934, acc: 0.9844, auc: 0.9998, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:22:18.009224, step: 2407, loss: 0.045739542692899704, acc: 0.9922, auc: 0.9995, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:22:18.153660, step: 2408, loss: 0.01861879974603653, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:22:18.295356, step: 2409, loss: 0.008071169257164001, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:18.427238, step: 2410, loss: 0.020070340484380722, acc: 0.9922, auc: 0.9995, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:18.561945, step: 2411, loss: 0.029671015217900276, acc: 0.9922, auc: 0.9998, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:18.697875, step: 2412, loss: 0.05402260646224022, acc: 0.9844, auc: 1.0, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:22:18.823019, step: 2413, loss: 0.1432066112756729, acc: 0.9297, auc: 0.998, precision: 0.9477, recall: 0.9118\n",
      "2018-12-27T14:22:18.956329, step: 2414, loss: 0.052605435252189636, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:22:19.091915, step: 2415, loss: 0.08787181973457336, acc: 0.9609, auc: 0.9963, precision: 0.9609, recall: 0.9638\n",
      "2018-12-27T14:22:19.223000, step: 2416, loss: 0.05539708957076073, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:22:19.354865, step: 2417, loss: 0.05844677984714508, acc: 0.9688, auc: 1.0, precision: 0.9714, recall: 0.9677\n",
      "2018-12-27T14:22:19.491161, step: 2418, loss: 0.03761123865842819, acc: 0.9766, auc: 0.9995, precision: 0.9758, recall: 0.9771\n",
      "2018-12-27T14:22:19.633258, step: 2419, loss: 0.012977752834558487, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:19.773560, step: 2420, loss: 0.012657249346375465, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:19.906738, step: 2421, loss: 0.05252332612872124, acc: 0.9844, auc: 0.9983, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:20.028410, step: 2422, loss: 0.08876718580722809, acc: 0.9688, auc: 0.996, precision: 0.9681, recall: 0.9681\n",
      "2018-12-27T14:22:20.161901, step: 2423, loss: 0.012932289391756058, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:22:20.294056, step: 2424, loss: 0.002593585755676031, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:20.445047, step: 2425, loss: 0.0035786102525889874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:20.586513, step: 2426, loss: 0.019449084997177124, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:20.723952, step: 2427, loss: 0.07002183049917221, acc: 0.9844, auc: 0.998, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:22:20.861694, step: 2428, loss: 0.021426847204566002, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:20.998406, step: 2429, loss: 0.017543481662869453, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:21.131863, step: 2430, loss: 0.0544709637761116, acc: 0.9766, auc: 0.9997, precision: 0.9712, recall: 0.981\n",
      "2018-12-27T14:22:21.267705, step: 2431, loss: 0.040080100297927856, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:21.405025, step: 2432, loss: 0.002451021922752261, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:21.550775, step: 2433, loss: 0.002985542407259345, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:21.686786, step: 2434, loss: 0.01576000079512596, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:21.824109, step: 2435, loss: 0.012538393959403038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:21.981403, step: 2436, loss: 0.024992085993289948, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:22.116901, step: 2437, loss: 0.10082432627677917, acc: 0.9688, auc: 0.9993, precision: 0.9672, recall: 0.9718\n",
      "2018-12-27T14:22:22.245704, step: 2438, loss: 0.04961094260215759, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:22.372127, step: 2439, loss: 0.05474794656038284, acc: 0.9766, auc: 0.9993, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:22:22.495229, step: 2440, loss: 0.018431805074214935, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:22.624880, step: 2441, loss: 0.009479589760303497, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:22.761494, step: 2442, loss: 0.01984269730746746, acc: 0.9922, auc: 0.9995, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:22.900416, step: 2443, loss: 0.02195451594889164, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:23.033191, step: 2444, loss: 0.006027614697813988, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:23.179147, step: 2445, loss: 0.008619178086519241, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:23.317681, step: 2446, loss: 0.005739945452660322, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:23.452348, step: 2447, loss: 0.030581191182136536, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:23.579299, step: 2448, loss: 0.018540233373641968, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:23.711842, step: 2449, loss: 0.03669567406177521, acc: 0.9766, auc: 0.9997, precision: 0.981, recall: 0.9712\n",
      "2018-12-27T14:22:23.839516, step: 2450, loss: 0.04402776062488556, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:22:23.971315, step: 2451, loss: 0.1164652556180954, acc: 0.9453, auc: 1.0, precision: 0.9521, recall: 0.9435\n",
      "2018-12-27T14:22:24.106543, step: 2452, loss: 0.040906816720962524, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:24.229548, step: 2453, loss: 0.06203345209360123, acc: 0.9766, auc: 0.997, precision: 0.9772, recall: 0.9754\n",
      "2018-12-27T14:22:24.363188, step: 2454, loss: 0.003670785343274474, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:24.491235, step: 2455, loss: 0.020456641912460327, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:24.625886, step: 2456, loss: 0.0370919331908226, acc: 0.9766, auc: 1.0, precision: 0.9754, recall: 0.9786\n",
      "2018-12-27T14:22:24.753604, step: 2457, loss: 0.03857994079589844, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:24.880110, step: 2458, loss: 0.009811916388571262, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:25.013885, step: 2459, loss: 0.1256401538848877, acc: 0.9688, auc: 0.9962, precision: 0.9643, recall: 0.9737\n",
      "2018-12-27T14:22:25.147028, step: 2460, loss: 0.11413872241973877, acc: 0.9609, auc: 1.0, precision: 0.9615, recall: 0.9632\n",
      "2018-12-27T14:22:25.280169, step: 2461, loss: 0.040243931114673615, acc: 0.9766, auc: 0.9995, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:22:25.418708, step: 2462, loss: 0.014419736340641975, acc: 0.9922, auc: 1.0, precision: 0.9894, recall: 0.9939\n",
      "2018-12-27T14:22:25.547853, step: 2463, loss: 0.04005788266658783, acc: 0.9844, auc: 0.999, precision: 0.9841, recall: 0.9841\n",
      "2018-12-27T14:22:25.686944, step: 2464, loss: 0.03590468317270279, acc: 0.9766, auc: 0.9993, precision: 0.9763, recall: 0.9769\n",
      "2018-12-27T14:22:25.820359, step: 2465, loss: 0.02646411396563053, acc: 0.9844, auc: 1.0, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:22:25.943903, step: 2466, loss: 0.03822849318385124, acc: 0.9844, auc: 1.0, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:22:26.080553, step: 2467, loss: 0.011865957640111446, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:22:26.214410, step: 2468, loss: 0.014841213822364807, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:26.340781, step: 2469, loss: 0.012607122771441936, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:26.470069, step: 2470, loss: 0.035345714539289474, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:26.606288, step: 2471, loss: 0.03924529626965523, acc: 0.9922, auc: 0.999, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:22:26.735566, step: 2472, loss: 0.016399074345827103, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:26.883039, step: 2473, loss: 0.09514922648668289, acc: 0.9609, auc: 0.9973, precision: 0.9616, recall: 0.9613\n",
      "2018-12-27T14:22:27.013729, step: 2474, loss: 0.19726814329624176, acc: 0.9375, auc: 0.998, precision: 0.9459, recall: 0.9355\n",
      "2018-12-27T14:22:27.146430, step: 2475, loss: 0.1867104172706604, acc: 0.9375, auc: 0.999, precision: 0.9474, recall: 0.9333\n",
      "2018-12-27T14:22:27.271504, step: 2476, loss: 0.21236851811408997, acc: 0.9141, auc: 0.9968, precision: 0.9225, recall: 0.9191\n",
      "2018-12-27T14:22:27.407636, step: 2477, loss: 0.10263596475124359, acc: 0.9688, auc: 0.9985, precision: 0.9737, recall: 0.9643\n",
      "2018-12-27T14:22:27.537220, step: 2478, loss: 0.016407227143645287, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:27.668556, step: 2479, loss: 0.02157425880432129, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:22:27.793169, step: 2480, loss: 0.021286776289343834, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "start training model\n",
      "2018-12-27T14:22:28.080257, step: 2481, loss: 0.007981177419424057, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:28.209551, step: 2482, loss: 0.0035304336342960596, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:28.341729, step: 2483, loss: 0.018061475828289986, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:22:28.477398, step: 2484, loss: 0.011904848739504814, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:28.606581, step: 2485, loss: 0.012719865888357162, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:22:28.732792, step: 2486, loss: 0.009120302274823189, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:22:28.879029, step: 2487, loss: 0.01728173717856407, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:29.013754, step: 2488, loss: 0.03166262060403824, acc: 0.9766, auc: 0.9995, precision: 0.9733, recall: 0.9771\n",
      "2018-12-27T14:22:29.156890, step: 2489, loss: 0.0116617102175951, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:22:29.296487, step: 2490, loss: 0.031181389465928078, acc: 0.9844, auc: 0.9995, precision: 0.9842, recall: 0.9842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:22:29.424304, step: 2491, loss: 0.04126334190368652, acc: 0.9844, auc: 0.9993, precision: 0.9825, recall: 0.9863\n",
      "2018-12-27T14:22:29.549471, step: 2492, loss: 0.09537537395954132, acc: 0.9609, auc: 0.9968, precision: 0.9616, recall: 0.9613\n",
      "2018-12-27T14:22:29.698999, step: 2493, loss: 0.08055876195430756, acc: 0.9688, auc: 0.9998, precision: 0.9688, recall: 0.9706\n",
      "2018-12-27T14:22:29.825268, step: 2494, loss: 0.027820471674203873, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:22:29.960070, step: 2495, loss: 0.01860855333507061, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:22:30.089433, step: 2496, loss: 0.00247720698826015, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:30.218157, step: 2497, loss: 0.015070346184074879, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:30.344694, step: 2498, loss: 0.03430351987481117, acc: 0.9844, auc: 1.0, precision: 0.9828, recall: 0.9861\n",
      "2018-12-27T14:22:30.478505, step: 2499, loss: 0.006045189686119556, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:30.621615, step: 2500, loss: 0.02518250048160553, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:22:35.383202, step: 2500, loss: 1.6396930374597247, acc: 0.6710473684210525, auc: 0.7348500000000001, precision: 0.6713894736842105, recall: 0.6716342105263158\n",
      "2018-12-27T14:22:35.514082, step: 2501, loss: 0.010060840286314487, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:35.638592, step: 2502, loss: 0.08976529538631439, acc: 0.9766, auc: 0.9971, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:22:35.776677, step: 2503, loss: 0.046265918761491776, acc: 0.9766, auc: 1.0, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:22:35.922670, step: 2504, loss: 0.04354698956012726, acc: 0.9844, auc: 1.0, precision: 0.9877, recall: 0.9796\n",
      "2018-12-27T14:22:36.070220, step: 2505, loss: 0.002541501075029373, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:36.206191, step: 2506, loss: 0.007968146353960037, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:22:36.331386, step: 2507, loss: 0.03305814042687416, acc: 0.9922, auc: 0.9993, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:36.457097, step: 2508, loss: 0.011346797458827496, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:36.598951, step: 2509, loss: 0.018684038892388344, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:36.733921, step: 2510, loss: 0.007182053290307522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:36.869470, step: 2511, loss: 0.03949321061372757, acc: 0.9844, auc: 1.0, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:22:36.997594, step: 2512, loss: 0.03965838998556137, acc: 0.9922, auc: 0.9993, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:37.120861, step: 2513, loss: 0.013958399184048176, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:37.253343, step: 2514, loss: 0.009162291884422302, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:37.372380, step: 2515, loss: 0.016026373952627182, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:37.501800, step: 2516, loss: 0.003396916203200817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:37.628822, step: 2517, loss: 0.054052941501140594, acc: 0.9844, auc: 0.9988, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:22:37.764379, step: 2518, loss: 0.026509232819080353, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:22:37.902767, step: 2519, loss: 0.009271705523133278, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:38.032650, step: 2520, loss: 0.008863205090165138, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:38.164523, step: 2521, loss: 0.007483566179871559, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:38.295514, step: 2522, loss: 0.008051570504903793, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:38.424258, step: 2523, loss: 0.0034875471610575914, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:38.555961, step: 2524, loss: 0.04688115790486336, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:38.700436, step: 2525, loss: 0.014351213350892067, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:22:38.827664, step: 2526, loss: 0.01081002689898014, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:38.963930, step: 2527, loss: 0.009558695368468761, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:22:39.099139, step: 2528, loss: 0.020248524844646454, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:39.241350, step: 2529, loss: 0.02720797248184681, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:39.365444, step: 2530, loss: 0.14672261476516724, acc: 0.9609, auc: 0.9983, precision: 0.9638, recall: 0.9609\n",
      "2018-12-27T14:22:39.498111, step: 2531, loss: 0.19200503826141357, acc: 0.9531, auc: 0.9973, precision: 0.9538, recall: 0.9565\n",
      "2018-12-27T14:22:39.632817, step: 2532, loss: 0.12290939688682556, acc: 0.9453, auc: 0.9995, precision: 0.947, recall: 0.9493\n",
      "2018-12-27T14:22:39.765911, step: 2533, loss: 0.03893059492111206, acc: 0.9766, auc: 0.9995, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:22:39.914669, step: 2534, loss: 0.07563187181949615, acc: 0.9688, auc: 0.9977, precision: 0.9706, recall: 0.9655\n",
      "2018-12-27T14:22:40.055352, step: 2535, loss: 0.0028961515054106712, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:40.183844, step: 2536, loss: 0.011733142659068108, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:40.312827, step: 2537, loss: 0.031509850174188614, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:22:40.452294, step: 2538, loss: 0.0055659860372543335, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:40.577925, step: 2539, loss: 0.014306975528597832, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:40.714393, step: 2540, loss: 0.00619629817083478, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:40.857630, step: 2541, loss: 0.004957855213433504, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:41.015819, step: 2542, loss: 0.008118556812405586, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:41.151483, step: 2543, loss: 0.02570870891213417, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:22:41.278330, step: 2544, loss: 0.07483464479446411, acc: 0.9844, auc: 0.998, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:22:41.413625, step: 2545, loss: 0.03470824658870697, acc: 0.9688, auc: 1.0, precision: 0.9683, recall: 0.971\n",
      "2018-12-27T14:22:41.551964, step: 2546, loss: 0.029680956155061722, acc: 0.9922, auc: 0.9995, precision: 0.99, recall: 0.9937\n",
      "2018-12-27T14:22:41.680210, step: 2547, loss: 0.06228245049715042, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:22:41.815684, step: 2548, loss: 0.004163309931755066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:41.940796, step: 2549, loss: 0.012118473649024963, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:42.073012, step: 2550, loss: 0.007637377828359604, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:42.206202, step: 2551, loss: 0.007862702012062073, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:42.343716, step: 2552, loss: 0.019487451761960983, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:42.477859, step: 2553, loss: 0.03573160991072655, acc: 0.9922, auc: 0.9993, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:22:42.618987, step: 2554, loss: 0.03669014573097229, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:22:42.753873, step: 2555, loss: 0.05115760862827301, acc: 0.9688, auc: 1.0, precision: 0.9747, recall: 0.9623\n",
      "2018-12-27T14:22:42.884695, step: 2556, loss: 0.03107646107673645, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:22:43.014738, step: 2557, loss: 0.049008436501026154, acc: 0.9766, auc: 0.9992, precision: 0.9772, recall: 0.9745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:22:43.149884, step: 2558, loss: 0.008271168917417526, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:43.281047, step: 2559, loss: 0.004289999604225159, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:43.418726, step: 2560, loss: 0.05803646519780159, acc: 0.9766, auc: 0.9988, precision: 0.9763, recall: 0.9769\n",
      "2018-12-27T14:22:43.550452, step: 2561, loss: 0.008403235115110874, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:22:43.685778, step: 2562, loss: 0.009265651926398277, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:43.814814, step: 2563, loss: 0.014294497668743134, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:43.961194, step: 2564, loss: 0.014010561630129814, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:44.096464, step: 2565, loss: 0.023899968713521957, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:22:44.225909, step: 2566, loss: 0.05688651651144028, acc: 0.9766, auc: 1.0, precision: 0.9773, recall: 0.9769\n",
      "2018-12-27T14:22:44.361496, step: 2567, loss: 0.1274433583021164, acc: 0.9531, auc: 1.0, precision: 0.9474, recall: 0.961\n",
      "2018-12-27T14:22:44.480627, step: 2568, loss: 0.4736371338367462, acc: 0.8828, auc: 0.9988, precision: 0.9, recall: 0.8897\n",
      "2018-12-27T14:22:44.623220, step: 2569, loss: 0.1828451007604599, acc: 0.9062, auc: 1.0, precision: 0.9167, recall: 0.9118\n",
      "2018-12-27T14:22:44.745848, step: 2570, loss: 0.05797514691948891, acc: 0.9766, auc: 0.9988, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:22:44.885137, step: 2571, loss: 0.00852382555603981, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:45.013602, step: 2572, loss: 0.0011943487916141748, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:45.139894, step: 2573, loss: 0.029257934540510178, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:45.276539, step: 2574, loss: 0.02712368778884411, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:45.416899, step: 2575, loss: 0.005866356194019318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:45.554107, step: 2576, loss: 0.04733923450112343, acc: 0.9922, auc: 0.9971, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:45.686838, step: 2577, loss: 0.006109143607318401, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:45.819168, step: 2578, loss: 0.043312348425388336, acc: 0.9922, auc: 0.9995, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:45.948090, step: 2579, loss: 0.026255259290337563, acc: 0.9922, auc: 0.9995, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:22:46.081981, step: 2580, loss: 0.048434458673000336, acc: 0.9766, auc: 0.999, precision: 0.9789, recall: 0.975\n",
      "2018-12-27T14:22:46.216847, step: 2581, loss: 0.049534931778907776, acc: 0.9766, auc: 1.0, precision: 0.9805, recall: 0.9722\n",
      "2018-12-27T14:22:46.343606, step: 2582, loss: 0.01079707033932209, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:46.474472, step: 2583, loss: 0.004007765091955662, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:46.611826, step: 2584, loss: 0.033379025757312775, acc: 0.9922, auc: 0.9993, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:22:46.741779, step: 2585, loss: 0.016510186716914177, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:46.888852, step: 2586, loss: 0.023862460628151894, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:47.017645, step: 2587, loss: 0.015157656744122505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:47.143242, step: 2588, loss: 0.012919526547193527, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:22:47.280581, step: 2589, loss: 0.009935316629707813, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:47.423855, step: 2590, loss: 0.02584049291908741, acc: 0.9922, auc: 0.9993, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:47.565379, step: 2591, loss: 0.0397607646882534, acc: 0.9922, auc: 0.9995, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:22:47.707752, step: 2592, loss: 0.01145949587225914, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:47.836495, step: 2593, loss: 0.019228463992476463, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:47.965110, step: 2594, loss: 0.036331064999103546, acc: 0.9844, auc: 0.9993, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:22:48.105149, step: 2595, loss: 0.04374068230390549, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:22:48.240413, step: 2596, loss: 0.0309092178940773, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:22:48.374928, step: 2597, loss: 0.03117639757692814, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:22:48.517243, step: 2598, loss: 0.006078539881855249, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:48.655088, step: 2599, loss: 0.014145216904580593, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:22:48.782100, step: 2600, loss: 0.02063727378845215, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:22:53.675270, step: 2600, loss: 1.7600729371372021, acc: 0.6657052631578949, auc: 0.7319052631578947, precision: 0.6695842105263159, recall: 0.667071052631579\n",
      "2018-12-27T14:22:53.801335, step: 2601, loss: 0.09750917553901672, acc: 0.9531, auc: 0.9978, precision: 0.9567, recall: 0.9504\n",
      "2018-12-27T14:22:53.939484, step: 2602, loss: 0.02716583013534546, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:54.093427, step: 2603, loss: 0.016608713194727898, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:22:54.244582, step: 2604, loss: 0.01865493133664131, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:54.380122, step: 2605, loss: 0.04495307058095932, acc: 0.9766, auc: 0.9995, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:22:54.508443, step: 2606, loss: 0.04779808595776558, acc: 0.9766, auc: 0.9987, precision: 0.9772, recall: 0.9739\n",
      "2018-12-27T14:22:54.640967, step: 2607, loss: 0.008477206341922283, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:54.778723, step: 2608, loss: 0.023915458470582962, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:22:54.914081, step: 2609, loss: 0.05873910337686539, acc: 0.9922, auc: 0.999, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:22:55.043325, step: 2610, loss: 0.04025295376777649, acc: 0.9766, auc: 0.9995, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:22:55.183528, step: 2611, loss: 0.013139426708221436, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:55.322517, step: 2612, loss: 0.004321003798395395, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:55.453392, step: 2613, loss: 0.013582929968833923, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:55.588405, step: 2614, loss: 0.0518183596432209, acc: 0.9688, auc: 0.999, precision: 0.9655, recall: 0.9706\n",
      "2018-12-27T14:22:55.711696, step: 2615, loss: 0.05099070817232132, acc: 0.9531, auc: 1.0, precision: 0.9545, recall: 0.9559\n",
      "2018-12-27T14:22:55.852924, step: 2616, loss: 0.0782085657119751, acc: 0.9766, auc: 0.9995, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:22:55.978895, step: 2617, loss: 0.20333333313465118, acc: 0.9297, auc: 0.9978, precision: 0.9308, recall: 0.9375\n",
      "2018-12-27T14:22:56.100857, step: 2618, loss: 0.19107984006404877, acc: 0.9453, auc: 0.9931, precision: 0.9512, recall: 0.9395\n",
      "2018-12-27T14:22:56.238286, step: 2619, loss: 0.04948461428284645, acc: 0.9766, auc: 0.9998, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:22:56.378127, step: 2620, loss: 0.02579560875892639, acc: 0.9922, auc: 0.9998, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:22:56.527988, step: 2621, loss: 0.027081763371825218, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:56.662611, step: 2622, loss: 0.013129327446222305, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:56.800710, step: 2623, loss: 0.028027281165122986, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:22:56.938513, step: 2624, loss: 0.02514650672674179, acc: 0.9922, auc: 0.9998, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:22:57.070631, step: 2625, loss: 0.05296381935477257, acc: 0.9766, auc: 0.9993, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:22:57.207131, step: 2626, loss: 0.06559357792139053, acc: 0.9844, auc: 0.9995, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:22:57.341152, step: 2627, loss: 0.01644999533891678, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:22:57.470088, step: 2628, loss: 0.02788291871547699, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:22:57.604356, step: 2629, loss: 0.06903192400932312, acc: 0.9766, auc: 0.9975, precision: 0.9772, recall: 0.9754\n",
      "2018-12-27T14:22:57.746685, step: 2630, loss: 0.0461401492357254, acc: 0.9844, auc: 0.9988, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:22:57.873077, step: 2631, loss: 0.02626795880496502, acc: 0.9922, auc: 0.9995, precision: 0.9906, recall: 0.9934\n",
      "2018-12-27T14:22:57.998861, step: 2632, loss: 0.011468548327684402, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:58.132589, step: 2633, loss: 0.011833982542157173, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:22:58.265342, step: 2634, loss: 0.010707185603678226, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:58.403370, step: 2635, loss: 0.015827974304556847, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2018-12-27T14:22:58.684072, step: 2636, loss: 0.0019972692243754864, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:58.818360, step: 2637, loss: 0.0015985454665496945, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:58.953774, step: 2638, loss: 0.009281621314585209, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:59.090835, step: 2639, loss: 0.0020164446905255318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:22:59.227436, step: 2640, loss: 0.03535633534193039, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:22:59.360363, step: 2641, loss: 0.01312947180122137, acc: 0.9922, auc: 1.0, precision: 0.9934, recall: 0.9906\n",
      "2018-12-27T14:22:59.484682, step: 2642, loss: 0.03708094358444214, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:22:59.619937, step: 2643, loss: 0.06443915516138077, acc: 0.9766, auc: 0.9998, precision: 0.975, recall: 0.9789\n",
      "2018-12-27T14:22:59.749461, step: 2644, loss: 0.02654454857110977, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:22:59.887896, step: 2645, loss: 0.013820488937199116, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:00.008439, step: 2646, loss: 0.012631154619157314, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:23:00.144540, step: 2647, loss: 0.005184071604162455, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:00.279461, step: 2648, loss: 0.06230969354510307, acc: 0.9844, auc: 0.9975, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:00.420208, step: 2649, loss: 0.01418438833206892, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:00.551036, step: 2650, loss: 0.020017625764012337, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:00.673839, step: 2651, loss: 0.067183718085289, acc: 0.9766, auc: 0.9992, precision: 0.9808, recall: 0.9717\n",
      "2018-12-27T14:23:00.801526, step: 2652, loss: 0.06014270707964897, acc: 0.9922, auc: 0.999, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:00.941511, step: 2653, loss: 0.017529163509607315, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:23:01.069653, step: 2654, loss: 0.01762552559375763, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:01.213108, step: 2655, loss: 0.008170787245035172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:01.349907, step: 2656, loss: 0.023808039724826813, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:01.470783, step: 2657, loss: 0.046134162694215775, acc: 0.9844, auc: 0.9993, precision: 0.9861, recall: 0.9828\n",
      "2018-12-27T14:23:01.607063, step: 2658, loss: 0.03277800977230072, acc: 0.9766, auc: 0.999, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:23:01.745108, step: 2659, loss: 0.06386341154575348, acc: 0.9766, auc: 0.998, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:23:01.885668, step: 2660, loss: 0.008611232042312622, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:02.015885, step: 2661, loss: 0.04763951897621155, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:02.141939, step: 2662, loss: 0.008509758859872818, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:02.279473, step: 2663, loss: 0.012145251967012882, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:02.407692, step: 2664, loss: 0.04580290988087654, acc: 0.9844, auc: 1.0, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:23:02.545193, step: 2665, loss: 0.04131050035357475, acc: 0.9766, auc: 1.0, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:23:02.669204, step: 2666, loss: 0.05452630668878555, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:23:02.800267, step: 2667, loss: 0.06597955524921417, acc: 0.9688, auc: 0.999, precision: 0.9722, recall: 0.9667\n",
      "2018-12-27T14:23:02.938802, step: 2668, loss: 0.04955415427684784, acc: 0.9766, auc: 1.0, precision: 0.9797, recall: 0.9737\n",
      "2018-12-27T14:23:03.074550, step: 2669, loss: 0.05884508416056633, acc: 0.9766, auc: 0.999, precision: 0.98, recall: 0.9732\n",
      "2018-12-27T14:23:03.206099, step: 2670, loss: 0.024642616510391235, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:23:03.333825, step: 2671, loss: 0.04167330637574196, acc: 0.9766, auc: 0.9998, precision: 0.9776, recall: 0.9766\n",
      "2018-12-27T14:23:03.474631, step: 2672, loss: 0.009375573135912418, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:03.600496, step: 2673, loss: 0.009466961957514286, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:03.735637, step: 2674, loss: 0.0039327084086835384, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:03.870203, step: 2675, loss: 0.03888562694191933, acc: 0.9844, auc: 0.9998, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:23:03.997465, step: 2676, loss: 0.007057009730488062, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:04.131998, step: 2677, loss: 0.022829905152320862, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:23:04.263021, step: 2678, loss: 0.053140219300985336, acc: 0.9844, auc: 0.9995, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:23:04.397590, step: 2679, loss: 0.062136661261320114, acc: 0.9844, auc: 0.999, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:23:04.530816, step: 2680, loss: 0.01747114583849907, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:04.667418, step: 2681, loss: 0.008275767788290977, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:04.803553, step: 2682, loss: 0.010871145874261856, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:04.929428, step: 2683, loss: 0.011293355375528336, acc: 0.9922, auc: 1.0, precision: 0.9902, recall: 0.9936\n",
      "2018-12-27T14:23:05.053552, step: 2684, loss: 0.027697648853063583, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:05.180778, step: 2685, loss: 0.04097956791520119, acc: 0.9688, auc: 1.0, precision: 0.9688, recall: 0.9706\n",
      "2018-12-27T14:23:05.316837, step: 2686, loss: 0.10531802475452423, acc: 0.9766, auc: 1.0, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:23:05.451616, step: 2687, loss: 0.04409061372280121, acc: 0.9766, auc: 0.9998, precision: 0.9783, recall: 0.9758\n",
      "2018-12-27T14:23:05.591682, step: 2688, loss: 0.008410904556512833, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:05.726228, step: 2689, loss: 0.006259546149522066, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:05.856855, step: 2690, loss: 0.004095053765922785, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:05.984184, step: 2691, loss: 0.007614574395120144, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:23:06.121388, step: 2692, loss: 0.03356415033340454, acc: 0.9766, auc: 0.9998, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:23:06.251671, step: 2693, loss: 0.02556050568819046, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:06.377886, step: 2694, loss: 0.02521388791501522, acc: 0.9844, auc: 0.9997, precision: 0.9836, recall: 0.9836\n",
      "2018-12-27T14:23:06.512014, step: 2695, loss: 0.02541511505842209, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:23:06.639960, step: 2696, loss: 0.012870322912931442, acc: 0.9922, auc: 1.0, precision: 0.9904, recall: 0.9935\n",
      "2018-12-27T14:23:06.778803, step: 2697, loss: 0.02036300115287304, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:06.906056, step: 2698, loss: 0.01649116724729538, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9909\n",
      "2018-12-27T14:23:07.046409, step: 2699, loss: 0.012341955676674843, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:07.167276, step: 2700, loss: 0.025410998612642288, acc: 0.9922, auc: 0.9993, precision: 0.9924, recall: 0.9921\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:23:11.995464, step: 2700, loss: 1.72224559909419, acc: 0.6784473684210525, auc: 0.7464263157894735, precision: 0.6779315789473684, recall: 0.6780236842105263\n",
      "2018-12-27T14:23:12.132703, step: 2701, loss: 0.009088809601962566, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:12.280496, step: 2702, loss: 0.030135851353406906, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:23:12.413300, step: 2703, loss: 0.12452524900436401, acc: 0.9609, auc: 0.9985, precision: 0.9667, recall: 0.9569\n",
      "2018-12-27T14:23:12.541061, step: 2704, loss: 0.2514662742614746, acc: 0.9297, auc: 0.9993, precision: 0.9348, recall: 0.9338\n",
      "2018-12-27T14:23:12.674451, step: 2705, loss: 0.27657029032707214, acc: 0.9219, auc: 0.9975, precision: 0.9367, recall: 0.9153\n",
      "2018-12-27T14:23:12.799882, step: 2706, loss: 0.08451445400714874, acc: 0.9766, auc: 0.9995, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:23:12.924391, step: 2707, loss: 0.04179593175649643, acc: 0.9844, auc: 0.9995, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:23:13.058352, step: 2708, loss: 0.014856578782200813, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:23:13.184014, step: 2709, loss: 0.02874290943145752, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:13.323711, step: 2710, loss: 0.00448230467736721, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:13.459573, step: 2711, loss: 0.018856123089790344, acc: 0.9922, auc: 0.9998, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:13.609164, step: 2712, loss: 0.016804981976747513, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:13.735261, step: 2713, loss: 0.027862805873155594, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:13.876674, step: 2714, loss: 0.01674296334385872, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:23:14.002133, step: 2715, loss: 0.0017827949486672878, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:14.133424, step: 2716, loss: 0.019211072474718094, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:23:14.267983, step: 2717, loss: 0.05573008954524994, acc: 0.9766, auc: 0.999, precision: 0.9736, recall: 0.9771\n",
      "2018-12-27T14:23:14.405825, step: 2718, loss: 0.04800770804286003, acc: 0.9766, auc: 0.9998, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:23:14.536104, step: 2719, loss: 0.007364043965935707, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:14.678086, step: 2720, loss: 0.05498155951499939, acc: 0.9922, auc: 0.999, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:14.812184, step: 2721, loss: 0.0016447289381176233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:14.952062, step: 2722, loss: 0.011333957314491272, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:15.079818, step: 2723, loss: 0.0043299212120473385, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:15.218231, step: 2724, loss: 0.013002650812268257, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:15.350077, step: 2725, loss: 0.011663596145808697, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:23:15.478798, step: 2726, loss: 0.011991351842880249, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:15.614415, step: 2727, loss: 0.002681600395590067, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:15.744539, step: 2728, loss: 0.009791970252990723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:15.884833, step: 2729, loss: 0.09058842062950134, acc: 0.9844, auc: 0.9973, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:16.009368, step: 2730, loss: 0.008437125943601131, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:16.154525, step: 2731, loss: 0.03174465894699097, acc: 0.9922, auc: 0.9993, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:16.292069, step: 2732, loss: 0.07427075505256653, acc: 0.9766, auc: 0.9985, precision: 0.9766, recall: 0.9776\n",
      "2018-12-27T14:23:16.429183, step: 2733, loss: 0.1090770736336708, acc: 0.9688, auc: 0.9993, precision: 0.9697, recall: 0.9697\n",
      "2018-12-27T14:23:16.554739, step: 2734, loss: 0.025468558073043823, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:16.699838, step: 2735, loss: 0.0276007242500782, acc: 0.9922, auc: 0.9995, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:23:16.844453, step: 2736, loss: 0.011664819903671741, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:16.971037, step: 2737, loss: 0.041109245270490646, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:23:17.120098, step: 2738, loss: 0.02772868238389492, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:17.259332, step: 2739, loss: 0.019747953861951828, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:17.405450, step: 2740, loss: 0.01499541848897934, acc: 0.9922, auc: 0.9998, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:23:17.534633, step: 2741, loss: 0.003733583027496934, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:17.662282, step: 2742, loss: 0.03208420053124428, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:17.788255, step: 2743, loss: 0.030879398807883263, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:17.916890, step: 2744, loss: 0.035269398242235184, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:23:18.043766, step: 2745, loss: 0.06076257303357124, acc: 0.9922, auc: 0.9998, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:23:18.171409, step: 2746, loss: 0.01696295104920864, acc: 0.9922, auc: 0.9998, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:18.307163, step: 2747, loss: 0.018529927358031273, acc: 0.9922, auc: 0.9998, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:23:18.433555, step: 2748, loss: 0.029586471617221832, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:18.565365, step: 2749, loss: 0.0356740802526474, acc: 0.9922, auc: 0.9998, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:23:18.704515, step: 2750, loss: 0.020107071846723557, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:18.838297, step: 2751, loss: 0.03668605163693428, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:18.967881, step: 2752, loss: 0.0024513504467904568, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:19.100738, step: 2753, loss: 0.030737506225705147, acc: 0.9922, auc: 0.9995, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:19.224841, step: 2754, loss: 0.021248800680041313, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:23:19.349878, step: 2755, loss: 0.09399183094501495, acc: 0.9609, auc: 0.9998, precision: 0.9583, recall: 0.9658\n",
      "2018-12-27T14:23:19.474659, step: 2756, loss: 0.29678061604499817, acc: 0.9062, auc: 0.9978, precision: 0.9231, recall: 0.9032\n",
      "2018-12-27T14:23:19.603610, step: 2757, loss: 0.25793343782424927, acc: 0.9141, auc: 0.9985, precision: 0.9236, recall: 0.9179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:23:19.730323, step: 2758, loss: 0.05578235909342766, acc: 0.9766, auc: 1.0, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:23:19.867000, step: 2759, loss: 0.02761830762028694, acc: 0.9844, auc: 1.0, precision: 0.9831, recall: 0.9859\n",
      "2018-12-27T14:23:20.001601, step: 2760, loss: 0.037553902715444565, acc: 0.9922, auc: 0.9983, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:20.127019, step: 2761, loss: 0.04604947194457054, acc: 0.9766, auc: 0.999, precision: 0.9772, recall: 0.975\n",
      "2018-12-27T14:23:20.278257, step: 2762, loss: 0.08251972496509552, acc: 0.9844, auc: 0.998, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:23:20.403169, step: 2763, loss: 0.03420469909906387, acc: 0.9766, auc: 0.9995, precision: 0.9769, recall: 0.9763\n",
      "2018-12-27T14:23:20.529172, step: 2764, loss: 0.010659444145858288, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:20.667858, step: 2765, loss: 0.030376527458429337, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:23:20.825698, step: 2766, loss: 0.018521536141633987, acc: 0.9844, auc: 1.0, precision: 0.9839, recall: 0.9853\n",
      "2018-12-27T14:23:20.964117, step: 2767, loss: 0.022132448852062225, acc: 0.9922, auc: 0.9998, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:21.106552, step: 2768, loss: 0.03393612802028656, acc: 0.9922, auc: 0.9993, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:21.232155, step: 2769, loss: 0.01598348841071129, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:23:21.361084, step: 2770, loss: 0.019564390182495117, acc: 0.9922, auc: 1.0, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:23:21.492002, step: 2771, loss: 0.012635559774935246, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:21.635369, step: 2772, loss: 0.03589563071727753, acc: 0.9922, auc: 0.9995, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:21.778573, step: 2773, loss: 0.026733331382274628, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:21.924061, step: 2774, loss: 0.0207231342792511, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:22.064580, step: 2775, loss: 0.019781559705734253, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:22.208314, step: 2776, loss: 0.06535381078720093, acc: 0.9844, auc: 0.9983, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:23:22.342494, step: 2777, loss: 0.026313282549381256, acc: 0.9922, auc: 0.9998, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:23:22.472349, step: 2778, loss: 0.018771842122077942, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:22.605722, step: 2779, loss: 0.06874158978462219, acc: 0.9609, auc: 0.9973, precision: 0.9616, recall: 0.9613\n",
      "2018-12-27T14:23:22.739638, step: 2780, loss: 0.06525588780641556, acc: 0.9688, auc: 0.9978, precision: 0.9687, recall: 0.9687\n",
      "2018-12-27T14:23:22.872303, step: 2781, loss: 0.007491597440093756, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:23.008441, step: 2782, loss: 0.09289233386516571, acc: 0.9688, auc: 0.9975, precision: 0.9672, recall: 0.9718\n",
      "2018-12-27T14:23:23.141167, step: 2783, loss: 0.018057093024253845, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:23:23.277118, step: 2784, loss: 0.042781293392181396, acc: 0.9844, auc: 0.9995, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:23:23.408540, step: 2785, loss: 0.01826675981283188, acc: 0.9844, auc: 1.0, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:23:23.548031, step: 2786, loss: 0.040035124868154526, acc: 0.9844, auc: 0.999, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:23.685063, step: 2787, loss: 0.02362162247300148, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:23:23.818776, step: 2788, loss: 0.028057683259248734, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:23.969369, step: 2789, loss: 0.029477249830961227, acc: 0.9844, auc: 1.0, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:23:24.113686, step: 2790, loss: 0.018794970586895943, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "start training model\n",
      "2018-12-27T14:23:24.479153, step: 2791, loss: 0.0014364258386194706, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:24.604600, step: 2792, loss: 0.0012951060198247433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:24.729238, step: 2793, loss: 0.011761119589209557, acc: 0.9922, auc: 1.0, precision: 0.9937, recall: 0.99\n",
      "2018-12-27T14:23:24.858801, step: 2794, loss: 0.012025075033307076, acc: 0.9922, auc: 1.0, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:23:24.998193, step: 2795, loss: 0.03024393506348133, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:25.126027, step: 2796, loss: 0.012693031691014767, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:25.272095, step: 2797, loss: 0.0026307322550565004, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:25.405021, step: 2798, loss: 0.0063918461091816425, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:25.545261, step: 2799, loss: 0.012939940206706524, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:23:25.692122, step: 2800, loss: 0.013778632506728172, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:23:30.506269, step: 2800, loss: 1.8116121323485124, acc: 0.6737263157894737, auc: 0.7497710526315791, precision: 0.6768842105263161, recall: 0.6737999999999998\n",
      "2018-12-27T14:23:30.646293, step: 2801, loss: 0.02543439343571663, acc: 0.9844, auc: 1.0, precision: 0.9821, recall: 0.9865\n",
      "2018-12-27T14:23:30.780423, step: 2802, loss: 0.012843075208365917, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:30.910415, step: 2803, loss: 0.015450958162546158, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:31.049535, step: 2804, loss: 0.010448746383190155, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:23:31.187803, step: 2805, loss: 0.0024016976822167635, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:31.325022, step: 2806, loss: 0.011187241412699223, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:31.461889, step: 2807, loss: 0.0020109019242227077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:31.603755, step: 2808, loss: 0.004529887810349464, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:31.745717, step: 2809, loss: 0.0031403498724102974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:31.879609, step: 2810, loss: 0.0003414344391785562, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:32.020306, step: 2811, loss: 0.003323046723380685, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:32.160032, step: 2812, loss: 0.010574644431471825, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:32.299578, step: 2813, loss: 0.02729315683245659, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:23:32.437256, step: 2814, loss: 0.07880278676748276, acc: 0.9844, auc: 0.9993, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:23:32.574819, step: 2815, loss: 0.04354914650321007, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:23:32.718977, step: 2816, loss: 0.008731822483241558, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:32.863619, step: 2817, loss: 0.0045417239889502525, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:33.000126, step: 2818, loss: 0.02329530008137226, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:33.135051, step: 2819, loss: 0.03239106014370918, acc: 0.9844, auc: 1.0, precision: 0.9857, recall: 0.9833\n",
      "2018-12-27T14:23:33.259217, step: 2820, loss: 0.010753214359283447, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:33.390665, step: 2821, loss: 0.0021717350464314222, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:33.535210, step: 2822, loss: 0.007515897508710623, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:33.666585, step: 2823, loss: 0.08319464325904846, acc: 0.9688, auc: 0.999, precision: 0.9643, recall: 0.9737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:23:33.801913, step: 2824, loss: 0.14911283552646637, acc: 0.9609, auc: 1.0, precision: 0.9597, recall: 0.9648\n",
      "2018-12-27T14:23:33.947969, step: 2825, loss: 0.13171522319316864, acc: 0.9453, auc: 1.0, precision: 0.9573, recall: 0.934\n",
      "2018-12-27T14:23:34.089272, step: 2826, loss: 0.024675441905856133, acc: 0.9844, auc: 1.0, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:23:34.222463, step: 2827, loss: 0.015081522986292839, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:23:34.350808, step: 2828, loss: 0.0046531944535672665, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:34.479999, step: 2829, loss: 0.0006835561944171786, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:34.609117, step: 2830, loss: 0.026470297947525978, acc: 0.9922, auc: 0.9995, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:34.747557, step: 2831, loss: 0.03218584135174751, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:34.884629, step: 2832, loss: 0.010792559012770653, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:23:35.028938, step: 2833, loss: 0.01573421247303486, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:35.161804, step: 2834, loss: 0.02056211233139038, acc: 0.9922, auc: 0.9998, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:35.294349, step: 2835, loss: 0.024060480296611786, acc: 0.9844, auc: 1.0, precision: 0.9868, recall: 0.9815\n",
      "2018-12-27T14:23:35.423427, step: 2836, loss: 0.00976730976253748, acc: 0.9922, auc: 1.0, precision: 0.9935, recall: 0.9904\n",
      "2018-12-27T14:23:35.552695, step: 2837, loss: 0.021169550716876984, acc: 0.9844, auc: 1.0, precision: 0.9863, recall: 0.9825\n",
      "2018-12-27T14:23:35.694987, step: 2838, loss: 0.014197217300534248, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:23:35.830245, step: 2839, loss: 0.011024007573723793, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:35.957579, step: 2840, loss: 0.024434633553028107, acc: 0.9922, auc: 1.0, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:23:36.092401, step: 2841, loss: 0.06076205521821976, acc: 0.9766, auc: 0.998, precision: 0.9771, recall: 0.9758\n",
      "2018-12-27T14:23:36.228724, step: 2842, loss: 0.01645648293197155, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:36.368241, step: 2843, loss: 0.10678868740797043, acc: 0.9609, auc: 0.998, precision: 0.9609, recall: 0.9638\n",
      "2018-12-27T14:23:36.504987, step: 2844, loss: 0.11593413352966309, acc: 0.9609, auc: 1.0, precision: 0.9691, recall: 0.9519\n",
      "2018-12-27T14:23:36.644956, step: 2845, loss: 0.023632176220417023, acc: 0.9922, auc: 1.0, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:23:36.788535, step: 2846, loss: 0.01298316940665245, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:36.918788, step: 2847, loss: 0.031068861484527588, acc: 0.9922, auc: 0.999, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:37.059308, step: 2848, loss: 0.0053425938822329044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:37.199646, step: 2849, loss: 0.003402099944651127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:37.336571, step: 2850, loss: 0.036932043731212616, acc: 0.9766, auc: 0.9995, precision: 0.9761, recall: 0.977\n",
      "2018-12-27T14:23:37.473999, step: 2851, loss: 0.04594942927360535, acc: 0.9766, auc: 0.999, precision: 0.9767, recall: 0.9766\n",
      "2018-12-27T14:23:37.608919, step: 2852, loss: 0.005360663868486881, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:37.750850, step: 2853, loss: 0.006830855738371611, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:37.887789, step: 2854, loss: 0.01897614262998104, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:23:38.025001, step: 2855, loss: 0.03186940774321556, acc: 0.9766, auc: 1.0, precision: 0.9779, recall: 0.9762\n",
      "2018-12-27T14:23:38.166132, step: 2856, loss: 0.03952672332525253, acc: 0.9844, auc: 0.9998, precision: 0.9828, recall: 0.9861\n",
      "2018-12-27T14:23:38.302799, step: 2857, loss: 0.03802745044231415, acc: 0.9922, auc: 0.9998, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:23:38.448399, step: 2858, loss: 0.007639875635504723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:38.587243, step: 2859, loss: 0.005788516718894243, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:38.718946, step: 2860, loss: 0.003394148778170347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:38.849142, step: 2861, loss: 0.014685574918985367, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:38.980267, step: 2862, loss: 0.04860556870698929, acc: 0.9766, auc: 0.9995, precision: 0.9772, recall: 0.9752\n",
      "2018-12-27T14:23:39.116217, step: 2863, loss: 0.06286188960075378, acc: 0.9844, auc: 0.9995, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:23:39.247212, step: 2864, loss: 0.018164055421948433, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:39.391738, step: 2865, loss: 0.00998840481042862, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:39.530575, step: 2866, loss: 0.014526942744851112, acc: 0.9922, auc: 1.0, precision: 0.9912, recall: 0.9931\n",
      "2018-12-27T14:23:39.658714, step: 2867, loss: 0.015705391764640808, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:39.783997, step: 2868, loss: 0.017724722623825073, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:39.912743, step: 2869, loss: 0.039766326546669006, acc: 0.9844, auc: 0.9995, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:23:40.050945, step: 2870, loss: 0.016648536548018456, acc: 0.9922, auc: 1.0, precision: 0.9911, recall: 0.9932\n",
      "2018-12-27T14:23:40.191801, step: 2871, loss: 0.02075423300266266, acc: 0.9844, auc: 0.9997, precision: 0.987, recall: 0.9811\n",
      "2018-12-27T14:23:40.343892, step: 2872, loss: 0.005234271753579378, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:40.491503, step: 2873, loss: 0.03680170699954033, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:40.636836, step: 2874, loss: 0.020244378596544266, acc: 0.9844, auc: 1.0, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:23:40.772193, step: 2875, loss: 0.10445113480091095, acc: 0.9766, auc: 0.9973, precision: 0.9792, recall: 0.9746\n",
      "2018-12-27T14:23:40.903639, step: 2876, loss: 0.10891874134540558, acc: 0.9375, auc: 1.0, precision: 0.9437, recall: 0.9385\n",
      "2018-12-27T14:23:41.031345, step: 2877, loss: 0.049787767231464386, acc: 0.9766, auc: 1.0, precision: 0.9795, recall: 0.9741\n",
      "2018-12-27T14:23:41.157564, step: 2878, loss: 0.02534487098455429, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:41.301043, step: 2879, loss: 0.034316521137952805, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:23:41.438541, step: 2880, loss: 0.0060863131657242775, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:41.565288, step: 2881, loss: 0.02983012981712818, acc: 0.9922, auc: 0.9995, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:41.700656, step: 2882, loss: 0.018850933760404587, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:41.827166, step: 2883, loss: 0.006391366943717003, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:41.976517, step: 2884, loss: 0.004978965036571026, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:42.104968, step: 2885, loss: 0.016045112162828445, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:23:42.232958, step: 2886, loss: 0.028450272977352142, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:23:42.379058, step: 2887, loss: 0.01859356090426445, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:42.506425, step: 2888, loss: 0.00549136009067297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:42.639446, step: 2889, loss: 0.02892870083451271, acc: 0.9844, auc: 0.9997, precision: 0.987, recall: 0.9811\n",
      "2018-12-27T14:23:42.768390, step: 2890, loss: 0.013820996508002281, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:42.902122, step: 2891, loss: 0.07826537638902664, acc: 0.9766, auc: 0.998, precision: 0.9768, recall: 0.9764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:23:43.031558, step: 2892, loss: 0.0865529403090477, acc: 0.9688, auc: 0.998, precision: 0.9701, recall: 0.9673\n",
      "2018-12-27T14:23:43.155953, step: 2893, loss: 0.2167140245437622, acc: 0.9297, auc: 0.9988, precision: 0.9338, recall: 0.9348\n",
      "2018-12-27T14:23:43.275409, step: 2894, loss: 0.12203770875930786, acc: 0.9688, auc: 0.9985, precision: 0.9737, recall: 0.9643\n",
      "2018-12-27T14:23:43.396341, step: 2895, loss: 0.06173110753297806, acc: 0.9531, auc: 0.9985, precision: 0.9541, recall: 0.9541\n",
      "2018-12-27T14:23:43.514176, step: 2896, loss: 0.014713570475578308, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:43.645181, step: 2897, loss: 0.0081546725705266, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:43.773655, step: 2898, loss: 0.012718499638140202, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:43.902936, step: 2899, loss: 0.009471697732806206, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:44.039131, step: 2900, loss: 0.013038908131420612, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:23:48.697695, step: 2900, loss: 1.7818772290882312, acc: 0.6675552631578947, auc: 0.736213157894737, precision: 0.6691631578947369, recall: 0.6687236842105261\n",
      "2018-12-27T14:23:48.829035, step: 2901, loss: 0.00621177488937974, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:48.958173, step: 2902, loss: 0.007682205177843571, acc: 0.9922, auc: 1.0, precision: 0.9896, recall: 0.9938\n",
      "2018-12-27T14:23:49.085587, step: 2903, loss: 0.006902566645294428, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:49.213307, step: 2904, loss: 0.0057558161206543446, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:49.356472, step: 2905, loss: 0.052201855927705765, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:49.485050, step: 2906, loss: 0.0372723825275898, acc: 0.9844, auc: 0.9998, precision: 0.9865, recall: 0.9821\n",
      "2018-12-27T14:23:49.613135, step: 2907, loss: 0.03689669072628021, acc: 0.9844, auc: 0.9993, precision: 0.9833, recall: 0.9857\n",
      "2018-12-27T14:23:49.744151, step: 2908, loss: 0.09791293740272522, acc: 0.9766, auc: 0.9963, precision: 0.9756, recall: 0.9771\n",
      "2018-12-27T14:23:49.884231, step: 2909, loss: 0.029784809798002243, acc: 0.9844, auc: 0.9998, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:23:50.020273, step: 2910, loss: 0.016751470044255257, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:50.154410, step: 2911, loss: 0.020282404497265816, acc: 0.9844, auc: 1.0, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:23:50.294221, step: 2912, loss: 0.0540056973695755, acc: 0.9844, auc: 0.9997, precision: 0.9808, recall: 0.9872\n",
      "2018-12-27T14:23:50.430972, step: 2913, loss: 0.06270982325077057, acc: 0.9766, auc: 0.9993, precision: 0.9786, recall: 0.9754\n",
      "2018-12-27T14:23:50.576073, step: 2914, loss: 0.021816886961460114, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:50.711503, step: 2915, loss: 0.01292993314564228, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:50.853217, step: 2916, loss: 0.048746995627880096, acc: 0.9766, auc: 0.9983, precision: 0.977, recall: 0.9761\n",
      "2018-12-27T14:23:50.988601, step: 2917, loss: 0.04341421276330948, acc: 0.9844, auc: 0.9988, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:23:51.121482, step: 2918, loss: 0.010402746498584747, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:51.249167, step: 2919, loss: 0.030737074092030525, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:51.385264, step: 2920, loss: 0.02778295800089836, acc: 0.9844, auc: 0.9998, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:51.511317, step: 2921, loss: 0.02402692660689354, acc: 0.9922, auc: 0.9997, precision: 0.9933, recall: 0.9907\n",
      "2018-12-27T14:23:51.631428, step: 2922, loss: 0.02356567047536373, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:51.769588, step: 2923, loss: 0.015837084501981735, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:51.897215, step: 2924, loss: 0.0028200356755405664, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:52.026933, step: 2925, loss: 0.030907589942216873, acc: 0.9766, auc: 0.9993, precision: 0.9758, recall: 0.9771\n",
      "2018-12-27T14:23:52.171822, step: 2926, loss: 0.01566598191857338, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:23:52.317168, step: 2927, loss: 0.009405405260622501, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:52.446261, step: 2928, loss: 0.07374397665262222, acc: 0.9688, auc: 0.9979, precision: 0.9708, recall: 0.9631\n",
      "2018-12-27T14:23:52.579485, step: 2929, loss: 0.02807685174047947, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:23:52.733735, step: 2930, loss: 0.04128523916006088, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:23:52.864791, step: 2931, loss: 0.052236493676900864, acc: 0.9844, auc: 0.9993, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:23:52.995437, step: 2932, loss: 0.021332910284399986, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:23:53.136577, step: 2933, loss: 0.028754232451319695, acc: 0.9922, auc: 0.9993, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:23:53.265152, step: 2934, loss: 0.0034527331590652466, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:53.431520, step: 2935, loss: 0.02643279731273651, acc: 0.9922, auc: 0.9998, precision: 0.9928, recall: 0.9917\n",
      "2018-12-27T14:23:53.570807, step: 2936, loss: 0.007226093672215939, acc: 0.9922, auc: 1.0, precision: 0.9923, recall: 0.9922\n",
      "2018-12-27T14:23:53.697984, step: 2937, loss: 0.01636062189936638, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:23:53.826378, step: 2938, loss: 0.02142525278031826, acc: 0.9922, auc: 0.9997, precision: 0.9909, recall: 0.9932\n",
      "2018-12-27T14:23:53.968145, step: 2939, loss: 0.05148180574178696, acc: 0.9766, auc: 0.999, precision: 0.9772, recall: 0.9745\n",
      "2018-12-27T14:23:54.102627, step: 2940, loss: 0.005122063681483269, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:54.243903, step: 2941, loss: 0.02413947694003582, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:23:54.373315, step: 2942, loss: 0.06289274990558624, acc: 0.9766, auc: 1.0, precision: 0.9789, recall: 0.975\n",
      "2018-12-27T14:23:54.508981, step: 2943, loss: 0.14063958823680878, acc: 0.9375, auc: 1.0, precision: 0.9429, recall: 0.9394\n",
      "2018-12-27T14:23:54.641770, step: 2944, loss: 0.04558325558900833, acc: 0.9766, auc: 1.0, precision: 0.9803, recall: 0.9727\n",
      "2018-12-27T14:23:54.771008, step: 2945, loss: 0.03950923681259155, acc: 0.9922, auc: 0.999, precision: 0.9922, recall: 0.9923\n",
      "start training model\n",
      "2018-12-27T14:23:55.081269, step: 2946, loss: 0.07383693009614944, acc: 0.9922, auc: 0.996, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:23:55.210925, step: 2947, loss: 0.00328405131585896, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:55.351535, step: 2948, loss: 0.026434186846017838, acc: 0.9844, auc: 0.9995, precision: 0.9842, recall: 0.9842\n",
      "2018-12-27T14:23:55.488565, step: 2949, loss: 0.04216623678803444, acc: 0.9844, auc: 0.9985, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:23:55.633175, step: 2950, loss: 0.004877233877778053, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:55.784951, step: 2951, loss: 0.007099049165844917, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:55.921928, step: 2952, loss: 0.019714027643203735, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:23:56.062234, step: 2953, loss: 0.004709798842668533, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:56.206112, step: 2954, loss: 0.00901113823056221, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:56.339547, step: 2955, loss: 0.004141864366829395, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:56.480815, step: 2956, loss: 0.0033253473229706287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:56.621623, step: 2957, loss: 0.00595467071980238, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:23:56.762381, step: 2958, loss: 0.016681790351867676, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:56.891675, step: 2959, loss: 0.06442627310752869, acc: 0.9844, auc: 1.0, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:23:57.020900, step: 2960, loss: 0.013350773602724075, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.162471, step: 2961, loss: 0.010183863341808319, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.294277, step: 2962, loss: 0.00548678170889616, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.433262, step: 2963, loss: 0.00365798594430089, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.576379, step: 2964, loss: 0.0025451891124248505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.713808, step: 2965, loss: 0.004744087345898151, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:57.858726, step: 2966, loss: 0.01539683248847723, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:23:57.998051, step: 2967, loss: 0.00491543160751462, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:58.139473, step: 2968, loss: 0.002335847355425358, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:58.268817, step: 2969, loss: 0.007947497069835663, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:58.404165, step: 2970, loss: 0.0030329464934766293, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:58.532442, step: 2971, loss: 0.006026550196111202, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:58.657602, step: 2972, loss: 0.030050138011574745, acc: 0.9922, auc: 1.0, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:23:58.785559, step: 2973, loss: 0.051037903875112534, acc: 0.9766, auc: 1.0, precision: 0.9805, recall: 0.9722\n",
      "2018-12-27T14:23:58.923502, step: 2974, loss: 0.020211780443787575, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:59.072331, step: 2975, loss: 0.0068834684789180756, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:59.215540, step: 2976, loss: 0.010061102919280529, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:59.360118, step: 2977, loss: 0.010988609865307808, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:23:59.498330, step: 2978, loss: 0.012285711243748665, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:23:59.637449, step: 2979, loss: 0.005846490617841482, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:23:59.775885, step: 2980, loss: 0.04329377040266991, acc: 0.9922, auc: 0.9995, precision: 0.9906, recall: 0.9934\n",
      "2018-12-27T14:23:59.909354, step: 2981, loss: 0.08354808390140533, acc: 0.9688, auc: 1.0, precision: 0.9722, recall: 0.9667\n",
      "2018-12-27T14:24:00.049691, step: 2982, loss: 0.04152701422572136, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:24:00.206510, step: 2983, loss: 0.03727177903056145, acc: 0.9844, auc: 0.9998, precision: 0.9836, recall: 0.9855\n",
      "2018-12-27T14:24:00.333007, step: 2984, loss: 0.022027291357517242, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:24:00.475651, step: 2985, loss: 0.01740269921720028, acc: 0.9922, auc: 1.0, precision: 0.9931, recall: 0.9912\n",
      "2018-12-27T14:24:00.604746, step: 2986, loss: 0.02132168412208557, acc: 0.9922, auc: 1.0, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:24:00.741413, step: 2987, loss: 0.03289966285228729, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:00.893553, step: 2988, loss: 0.02193685621023178, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:24:01.040539, step: 2989, loss: 0.04102795198559761, acc: 0.9766, auc: 0.9995, precision: 0.9766, recall: 0.9767\n",
      "2018-12-27T14:24:01.160975, step: 2990, loss: 0.00483349384739995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:01.299029, step: 2991, loss: 0.008878464810550213, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:24:01.431350, step: 2992, loss: 0.011273350566625595, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:01.575172, step: 2993, loss: 0.004737458191812038, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:01.712839, step: 2994, loss: 0.016121523454785347, acc: 0.9844, auc: 1.0, precision: 0.9867, recall: 0.9818\n",
      "2018-12-27T14:24:01.868730, step: 2995, loss: 0.006164535880088806, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:02.007801, step: 2996, loss: 0.016510367393493652, acc: 0.9922, auc: 0.9998, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:24:02.157405, step: 2997, loss: 0.022597571834921837, acc: 0.9766, auc: 1.0, precision: 0.9758, recall: 0.9783\n",
      "2018-12-27T14:24:02.285546, step: 2998, loss: 0.01885201223194599, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:24:02.430126, step: 2999, loss: 0.03224239870905876, acc: 0.9922, auc: 0.9995, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:24:02.570358, step: 3000, loss: 0.050298891961574554, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:24:07.385829, step: 3000, loss: 2.0232616223787008, acc: 0.6673421052631576, auc: 0.7473105263157896, precision: 0.6780947368421051, recall: 0.669292105263158\n",
      "2018-12-27T14:24:07.510251, step: 3001, loss: 0.08537721633911133, acc: 0.9609, auc: 0.9982, precision: 0.9562, recall: 0.9642\n",
      "2018-12-27T14:24:07.645095, step: 3002, loss: 0.04917513579130173, acc: 0.9844, auc: 0.9988, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:24:07.776840, step: 3003, loss: 0.023306190967559814, acc: 0.9922, auc: 0.9998, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:24:07.913650, step: 3004, loss: 0.0069834571331739426, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:08.048249, step: 3005, loss: 0.0022050219122320414, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:08.200523, step: 3006, loss: 0.003600846976041794, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:08.343923, step: 3007, loss: 0.014962688088417053, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:24:08.483530, step: 3008, loss: 0.05069767311215401, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:24:08.614085, step: 3009, loss: 0.037607815116643906, acc: 0.9844, auc: 1.0, precision: 0.9859, recall: 0.9831\n",
      "2018-12-27T14:24:08.744252, step: 3010, loss: 0.08520465344190598, acc: 0.9844, auc: 0.9973, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:08.873209, step: 3011, loss: 0.015071699395775795, acc: 0.9922, auc: 1.0, precision: 0.9925, recall: 0.9919\n",
      "2018-12-27T14:24:09.006250, step: 3012, loss: 0.011572916060686111, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:24:09.144219, step: 3013, loss: 0.0023543727584183216, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:09.266889, step: 3014, loss: 0.03654680401086807, acc: 0.9922, auc: 0.9995, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:24:09.412093, step: 3015, loss: 0.10376455634832382, acc: 0.9531, auc: 1.0, precision: 0.9589, recall: 0.9508\n",
      "2018-12-27T14:24:09.541526, step: 3016, loss: 0.11980631947517395, acc: 0.9609, auc: 0.9976, precision: 0.9623, recall: 0.9606\n",
      "2018-12-27T14:24:09.668813, step: 3017, loss: 0.13844318687915802, acc: 0.9453, auc: 1.0, precision: 0.9327, recall: 0.9578\n",
      "2018-12-27T14:24:09.803476, step: 3018, loss: 0.21749258041381836, acc: 0.9375, auc: 0.9995, precision: 0.9429, recall: 0.9394\n",
      "2018-12-27T14:24:09.942240, step: 3019, loss: 0.09955018758773804, acc: 0.9609, auc: 0.9998, precision: 0.9638, recall: 0.9609\n",
      "2018-12-27T14:24:10.081980, step: 3020, loss: 0.008183691650629044, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:10.208765, step: 3021, loss: 0.0380144864320755, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:10.347745, step: 3022, loss: 0.02920154109597206, acc: 0.9844, auc: 0.9998, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:24:10.488118, step: 3023, loss: 0.038638368248939514, acc: 0.9844, auc: 0.9997, precision: 0.9868, recall: 0.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:24:10.616267, step: 3024, loss: 0.007029212545603514, acc: 0.9922, auc: 1.0, precision: 0.9924, recall: 0.9921\n",
      "2018-12-27T14:24:10.744036, step: 3025, loss: 0.0319538339972496, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:10.883331, step: 3026, loss: 0.04600795358419418, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:24:11.012625, step: 3027, loss: 0.03411990404129028, acc: 0.9844, auc: 0.9997, precision: 0.9839, recall: 0.9839\n",
      "2018-12-27T14:24:11.158625, step: 3028, loss: 0.006250094622373581, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:11.288171, step: 3029, loss: 0.03493577986955643, acc: 0.9922, auc: 0.9993, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:24:11.428027, step: 3030, loss: 0.04926406592130661, acc: 0.9844, auc: 0.999, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:11.554412, step: 3031, loss: 0.0027616608422249556, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:11.683001, step: 3032, loss: 0.004992968402802944, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:11.812543, step: 3033, loss: 0.040627606213092804, acc: 0.9766, auc: 0.999, precision: 0.9768, recall: 0.9764\n",
      "2018-12-27T14:24:11.949456, step: 3034, loss: 0.02765294909477234, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:24:12.084018, step: 3035, loss: 0.006833347491919994, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:12.214665, step: 3036, loss: 0.006995712406933308, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:12.353063, step: 3037, loss: 0.003691937541589141, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:12.480731, step: 3038, loss: 0.015418896451592445, acc: 0.9922, auc: 1.0, precision: 0.9918, recall: 0.9926\n",
      "2018-12-27T14:24:12.620817, step: 3039, loss: 0.024511005729436874, acc: 0.9922, auc: 0.9998, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:24:12.762806, step: 3040, loss: 0.021454866975545883, acc: 0.9922, auc: 1.0, precision: 0.9926, recall: 0.9918\n",
      "2018-12-27T14:24:12.889895, step: 3041, loss: 0.059811756014823914, acc: 0.9766, auc: 0.9995, precision: 0.9769, recall: 0.9773\n",
      "2018-12-27T14:24:13.018458, step: 3042, loss: 0.1350003331899643, acc: 0.9453, auc: 0.9963, precision: 0.9447, recall: 0.9485\n",
      "2018-12-27T14:24:13.168263, step: 3043, loss: 0.025153471156954765, acc: 0.9922, auc: 1.0, precision: 0.9929, recall: 0.9915\n",
      "2018-12-27T14:24:13.311359, step: 3044, loss: 0.07616433501243591, acc: 0.9766, auc: 0.9978, precision: 0.9754, recall: 0.9786\n",
      "2018-12-27T14:24:13.446425, step: 3045, loss: 0.08241622149944305, acc: 0.9844, auc: 0.9998, precision: 0.9846, recall: 0.9846\n",
      "2018-12-27T14:24:13.572839, step: 3046, loss: 0.11967622488737106, acc: 0.9453, auc: 0.9998, precision: 0.9514, recall: 0.9444\n",
      "2018-12-27T14:24:13.714304, step: 3047, loss: 0.06208144128322601, acc: 0.9844, auc: 1.0, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:24:13.850884, step: 3048, loss: 0.019173838198184967, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:24:13.976930, step: 3049, loss: 0.0022310963831841946, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:14.121868, step: 3050, loss: 0.0399884358048439, acc: 0.9844, auc: 0.9993, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:14.274829, step: 3051, loss: 0.01899982988834381, acc: 0.9844, auc: 1.0, precision: 0.9851, recall: 0.9841\n",
      "2018-12-27T14:24:14.425151, step: 3052, loss: 0.005398530047386885, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:14.552873, step: 3053, loss: 0.014104364439845085, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:14.692565, step: 3054, loss: 0.031340714544057846, acc: 0.9922, auc: 0.9998, precision: 0.9932, recall: 0.9911\n",
      "2018-12-27T14:24:14.833305, step: 3055, loss: 0.019592978060245514, acc: 0.9922, auc: 1.0, precision: 0.9917, recall: 0.9928\n",
      "2018-12-27T14:24:14.966457, step: 3056, loss: 0.02219688519835472, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:24:15.110300, step: 3057, loss: 0.04074767976999283, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:24:15.238509, step: 3058, loss: 0.049985721707344055, acc: 0.9766, auc: 1.0, precision: 0.9803, recall: 0.9727\n",
      "2018-12-27T14:24:15.374304, step: 3059, loss: 0.06282097846269608, acc: 0.9844, auc: 0.9998, precision: 0.9855, recall: 0.9836\n",
      "2018-12-27T14:24:15.504083, step: 3060, loss: 0.01615040749311447, acc: 0.9922, auc: 0.9998, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:24:15.642945, step: 3061, loss: 0.024726152420043945, acc: 0.9844, auc: 1.0, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:24:15.780493, step: 3062, loss: 0.011185020208358765, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:15.924463, step: 3063, loss: 0.011492207646369934, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:16.077486, step: 3064, loss: 0.025007694959640503, acc: 0.9766, auc: 1.0, precision: 0.9789, recall: 0.975\n",
      "2018-12-27T14:24:16.210782, step: 3065, loss: 0.009163949638605118, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:16.345786, step: 3066, loss: 0.0067974659614264965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:16.472712, step: 3067, loss: 0.01443855557590723, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:16.613491, step: 3068, loss: 0.004683378618210554, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:16.739591, step: 3069, loss: 0.05711624398827553, acc: 0.9844, auc: 0.9983, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:16.881960, step: 3070, loss: 0.053763557225465775, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9848\n",
      "2018-12-27T14:24:17.004856, step: 3071, loss: 0.09612315893173218, acc: 0.9453, auc: 0.9993, precision: 0.9444, recall: 0.9514\n",
      "2018-12-27T14:24:17.153020, step: 3072, loss: 0.062118202447891235, acc: 0.9766, auc: 1.0, precision: 0.9815, recall: 0.97\n",
      "2018-12-27T14:24:17.300504, step: 3073, loss: 0.06894053518772125, acc: 0.9609, auc: 0.999, precision: 0.9603, recall: 0.9643\n",
      "2018-12-27T14:24:17.450925, step: 3074, loss: 0.06143003702163696, acc: 0.9844, auc: 0.9995, precision: 0.9841, recall: 0.9851\n",
      "2018-12-27T14:24:17.592667, step: 3075, loss: 0.030679602175951004, acc: 0.9844, auc: 0.9993, precision: 0.9861, recall: 0.9828\n",
      "2018-12-27T14:24:17.726208, step: 3076, loss: 0.04443496838212013, acc: 0.9922, auc: 0.9983, precision: 0.9922, recall: 0.9923\n",
      "2018-12-27T14:24:17.862733, step: 3077, loss: 0.028599843382835388, acc: 0.9922, auc: 1.0, precision: 0.9915, recall: 0.9929\n",
      "2018-12-27T14:24:17.988565, step: 3078, loss: 0.020134810358285904, acc: 0.9844, auc: 1.0, precision: 0.9878, recall: 0.9792\n",
      "2018-12-27T14:24:18.126835, step: 3079, loss: 0.047323115170001984, acc: 0.9844, auc: 0.9998, precision: 0.9848, recall: 0.9844\n",
      "2018-12-27T14:24:18.253814, step: 3080, loss: 0.014700664207339287, acc: 0.9922, auc: 1.0, precision: 0.9919, recall: 0.9925\n",
      "2018-12-27T14:24:18.383610, step: 3081, loss: 0.014579959213733673, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "2018-12-27T14:24:18.507248, step: 3082, loss: 0.02096092328429222, acc: 0.9844, auc: 1.0, precision: 0.9853, recall: 0.9839\n",
      "2018-12-27T14:24:18.657213, step: 3083, loss: 0.01657988503575325, acc: 0.9922, auc: 0.9997, precision: 0.9938, recall: 0.9898\n",
      "2018-12-27T14:24:18.788075, step: 3084, loss: 0.006322200875729322, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:18.921781, step: 3085, loss: 0.003526459913700819, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:19.051270, step: 3086, loss: 0.015117974951863289, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:24:19.183341, step: 3087, loss: 0.02865089848637581, acc: 0.9922, auc: 0.9995, precision: 0.9914, recall: 0.993\n",
      "2018-12-27T14:24:19.316864, step: 3088, loss: 0.039185237139463425, acc: 0.9922, auc: 0.9983, precision: 0.993, recall: 0.9914\n",
      "2018-12-27T14:24:19.442110, step: 3089, loss: 0.06582595407962799, acc: 0.9844, auc: 0.9995, precision: 0.9808, recall: 0.9872\n",
      "2018-12-27T14:24:19.587560, step: 3090, loss: 0.08443901687860489, acc: 0.9688, auc: 0.9973, precision: 0.9681, recall: 0.9681\n",
      "2018-12-27T14:24:19.733865, step: 3091, loss: 0.08642389625310898, acc: 0.9609, auc: 1.0, precision: 0.9615, recall: 0.9632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-27T14:24:19.866526, step: 3092, loss: 0.0424620546400547, acc: 0.9844, auc: 0.9995, precision: 0.9843, recall: 0.9843\n",
      "2018-12-27T14:24:19.995123, step: 3093, loss: 0.01494593732059002, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:20.124805, step: 3094, loss: 0.02443850226700306, acc: 0.9844, auc: 0.9998, precision: 0.9844, recall: 0.9844\n",
      "2018-12-27T14:24:20.261450, step: 3095, loss: 0.007260921411216259, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:20.403155, step: 3096, loss: 0.005617574788630009, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:20.539291, step: 3097, loss: 0.02310865931212902, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2018-12-27T14:24:20.677944, step: 3098, loss: 0.08150719106197357, acc: 0.9688, auc: 0.9985, precision: 0.9733, recall: 0.9649\n",
      "2018-12-27T14:24:20.822322, step: 3099, loss: 0.06479298323392868, acc: 0.9844, auc: 0.9988, precision: 0.9861, recall: 0.9828\n",
      "2018-12-27T14:24:20.967336, step: 3100, loss: 0.012399956583976746, acc: 0.9922, auc: 1.0, precision: 0.9921, recall: 0.9924\n",
      "\n",
      "Evaluation:\n",
      "2018-12-27T14:24:25.787840, step: 3100, loss: 1.7747235549123663, acc: 0.6743394736842105, auc: 0.7425842105263156, precision: 0.6747315789473683, recall: 0.675021052631579\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "charEmbedding = data.charEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        \n",
    "        cnn = CharCNN(config, charEmbedding)\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.RMSPropOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"trainLoss\", cnn.loss)\n",
    "        \n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/charCNN/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: batchX,\n",
    "              cnn.inputY: batchY,\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/charCNN/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(cnn.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
