{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, dump_token_embeddings, Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 256  # 这个值和ELMo的词向量大小一致\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    optionFile = \"modelParams/elmo_options.json\"\n",
    "    weightFile = \"modelParams/elmo_weights.hdf5\"\n",
    "    vocabFile = \"modelParams/vocab.txt\"\n",
    "    tokenEmbeddingFile = 'modelParams/elmo_token_embeddings.hdf5'\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        self._optionFile = config.optionFile\n",
    "        self._weightFile = config.weightFile\n",
    "        self._vocabFile = config.vocabFile\n",
    "        self._tokenEmbeddingFile = config.tokenEmbeddingFile\n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _genVocabFile(self, reviews):\n",
    "        \"\"\"\n",
    "        用我们的训练数据生成一个词汇文件，并加入三个特殊字符\n",
    "        \"\"\"\n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        wordCount = Counter(allWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        words = [item[0] for item in sortWordCount.items()]\n",
    "        allTokens = ['<S>', '</S>', '<UNK>'] + words\n",
    "        with open(self._vocabFile, 'w') as fout:\n",
    "            fout.write('\\n'.join(allTokens))\n",
    "    \n",
    "    def _fixedSeq(self, reviews):\n",
    "        \"\"\"\n",
    "        将长度超过200的截断为200的长度\n",
    "        \"\"\"\n",
    "        return [review[:self._sequenceLength] for review in reviews]\n",
    "    \n",
    "    def _genElmoEmbedding(self):\n",
    "        \"\"\"\n",
    "        调用ELMO源码中的dump_token_embeddings方法，基于字符的表示生成词的向量表示。并保存成hdf5文件，文件中的\"embedding\"键对应的value就是\n",
    "        词汇表文件中各词汇的向量表示，这些词汇的向量表示之后会作为BiLM的初始化输入。\n",
    "        \"\"\"\n",
    "        dump_token_embeddings(\n",
    "            self._vocabFile, self._optionFile, self._weightFile, self._tokenEmbeddingFile)\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        y = [[item] for item in y]\n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = x[:trainIndex]\n",
    "        trainLabels = y[:trainIndex]\n",
    "        \n",
    "        evalReviews = x[trainIndex:]\n",
    "        evalLabels = y[trainIndex:]\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        \n",
    "#         self._genVocabFile(reviews) # 生成vocabFile\n",
    "#         self._genElmoEmbedding()  # 生成elmo_token_embedding\n",
    "        \n",
    "        reviews = self._fixedSeq(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "                \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: 20000\n",
      "train label shape: 20000\n",
      "eval data shape: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(len(data.trainReviews)))\n",
    "print(\"train label shape: {}\".format(len(data.trainLabels)))\n",
    "print(\"eval data shape: {}\".format(len(data.evalReviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "        # 每一个epoch时，都要打乱数据集\n",
    "        midVal = list(zip(x, y))\n",
    "        random.shuffle(midVal)\n",
    "        x, y = zip(*midVal)\n",
    "        x = list(x)\n",
    "        y = list(y)\n",
    "\n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX =x[start: end]\n",
    "            batchY = y[start: end]\n",
    "\n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.float32, [None, config.sequenceLength, config.model.embeddingSize], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            embeddingW = tf.get_variable(\n",
    "                \"embeddingW\",\n",
    "                shape=[config.model.embeddingSize, config.model.embeddingSize],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            reshapeInputX = tf.reshape(self.inputX, shape=[-1, config.model.embeddingSize])\n",
    "            \n",
    "            self.embeddedWords = tf.reshape(tf.matmul(reshapeInputX, embeddingW), shape=[-1, config.sequenceLength, config.model.embeddingSize])\n",
    "            self.embeddedWords = tf.nn.dropout(self.embeddedWords, self.dropoutKeepProb)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "INFO:tensorflow:Summary name embeddingW:0/grad/hist is illegal; using embeddingW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embeddingW:0/grad/sparsity is illegal; using embeddingW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/ELMo/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-07T19:30:44.103310, step: 1, loss: 0.7134121060371399, acc: 0.4141, auc: 0.4008, precision: 0.25, recall: 0.0282\n",
      "2019-01-07T19:30:45.840655, step: 2, loss: 0.6727493405342102, acc: 0.4609, auc: 0.6672, precision: 0.6667, recall: 0.0286\n",
      "2019-01-07T19:30:48.112047, step: 3, loss: 0.6814926862716675, acc: 0.5625, auc: 0.6169, precision: 0.6429, recall: 0.2812\n",
      "2019-01-07T19:30:50.343944, step: 4, loss: 0.6925935745239258, acc: 0.5156, auc: 0.5422, precision: 0.8, recall: 0.1176\n",
      "2019-01-07T19:30:52.377307, step: 5, loss: 0.6441318988800049, acc: 0.6172, auc: 0.7096, precision: 0.7188, recall: 0.3651\n",
      "2019-01-07T19:30:53.944997, step: 6, loss: 0.6385297775268555, acc: 0.5234, auc: 0.6887, precision: 0.6522, recall: 0.2206\n",
      "2019-01-07T19:30:55.613486, step: 7, loss: 0.6117496490478516, acc: 0.6328, auc: 0.7463, precision: 0.8095, recall: 0.2833\n",
      "2019-01-07T19:30:57.356193, step: 8, loss: 0.6429930925369263, acc: 0.5547, auc: 0.6879, precision: 0.75, recall: 0.1846\n",
      "2019-01-07T19:30:59.446539, step: 9, loss: 0.6276957988739014, acc: 0.6562, auc: 0.6868, precision: 0.7143, recall: 0.283\n",
      "2019-01-07T19:31:01.120099, step: 10, loss: 0.5971968770027161, acc: 0.6094, auc: 0.7698, precision: 0.913, recall: 0.3043\n",
      "2019-01-07T19:31:02.862705, step: 11, loss: 0.6013011336326599, acc: 0.5781, auc: 0.7386, precision: 0.7143, recall: 0.4167\n",
      "2019-01-07T19:31:04.558692, step: 12, loss: 0.6640322804450989, acc: 0.6719, auc: 0.7458, precision: 0.6232, recall: 0.7288\n",
      "2019-01-07T19:31:06.687856, step: 13, loss: 0.5645319223403931, acc: 0.6797, auc: 0.7851, precision: 0.8158, recall: 0.4769\n",
      "2019-01-07T19:31:08.483886, step: 14, loss: 0.5591650009155273, acc: 0.625, auc: 0.8223, precision: 0.9167, recall: 0.3235\n",
      "2019-01-07T19:31:10.152911, step: 15, loss: 0.6143022775650024, acc: 0.6328, auc: 0.7279, precision: 0.7941, recall: 0.403\n",
      "2019-01-07T19:31:11.833254, step: 16, loss: 0.5345460176467896, acc: 0.7109, auc: 0.8215, precision: 0.78, recall: 0.6\n",
      "2019-01-07T19:31:13.475267, step: 17, loss: 0.6445561051368713, acc: 0.6875, auc: 0.7254, precision: 0.6957, recall: 0.5517\n",
      "2019-01-07T19:31:15.053048, step: 18, loss: 0.5732543468475342, acc: 0.6562, auc: 0.7654, precision: 0.7105, recall: 0.45\n",
      "2019-01-07T19:31:16.731227, step: 19, loss: 0.539820671081543, acc: 0.7031, auc: 0.8132, precision: 0.9412, recall: 0.4706\n",
      "2019-01-07T19:31:18.357023, step: 20, loss: 0.536496639251709, acc: 0.75, auc: 0.801, precision: 0.8333, recall: 0.5833\n",
      "2019-01-07T19:31:20.532388, step: 21, loss: 0.5567790865898132, acc: 0.6719, auc: 0.7852, precision: 0.7778, recall: 0.5224\n",
      "2019-01-07T19:31:22.266744, step: 22, loss: 0.5088480710983276, acc: 0.7422, auc: 0.8336, precision: 0.8113, recall: 0.6515\n",
      "2019-01-07T19:31:23.877795, step: 23, loss: 0.5307449102401733, acc: 0.7266, auc: 0.8248, precision: 0.7826, recall: 0.5902\n",
      "2019-01-07T19:31:26.035213, step: 24, loss: 0.494819700717926, acc: 0.7109, auc: 0.8534, precision: 0.7949, recall: 0.5167\n",
      "2019-01-07T19:31:28.186081, step: 25, loss: 0.44985899329185486, acc: 0.7812, auc: 0.8801, precision: 0.9111, recall: 0.6308\n",
      "2019-01-07T19:31:29.737763, step: 26, loss: 0.5554978251457214, acc: 0.7188, auc: 0.7941, precision: 0.7647, recall: 0.7222\n",
      "2019-01-07T19:31:31.671431, step: 27, loss: 0.6268795132637024, acc: 0.7188, auc: 0.8434, precision: 0.6437, recall: 0.918\n",
      "2019-01-07T19:31:33.690016, step: 28, loss: 0.5583466291427612, acc: 0.6953, auc: 0.7856, precision: 0.7551, recall: 0.5781\n",
      "2019-01-07T19:31:35.335192, step: 29, loss: 0.46041861176490784, acc: 0.7734, auc: 0.8846, precision: 0.9375, recall: 0.5263\n",
      "2019-01-07T19:31:37.013196, step: 30, loss: 0.6724156737327576, acc: 0.6406, auc: 0.8339, precision: 0.9655, recall: 0.3836\n",
      "2019-01-07T19:31:38.649638, step: 31, loss: 0.5592218637466431, acc: 0.6875, auc: 0.7932, precision: 0.8293, recall: 0.5075\n",
      "2019-01-07T19:31:40.505886, step: 32, loss: 0.5452408790588379, acc: 0.7656, auc: 0.847, precision: 0.7353, recall: 0.8065\n",
      "2019-01-07T19:31:42.130714, step: 33, loss: 0.6909370422363281, acc: 0.6641, auc: 0.7363, precision: 0.6197, recall: 0.7333\n",
      "2019-01-07T19:31:43.848803, step: 34, loss: 0.5199342966079712, acc: 0.7812, auc: 0.8254, precision: 0.8056, recall: 0.8056\n",
      "2019-01-07T19:31:45.523746, step: 35, loss: 0.5562877655029297, acc: 0.7578, auc: 0.8144, precision: 0.76, recall: 0.6667\n",
      "2019-01-07T19:31:47.183012, step: 36, loss: 0.5353860855102539, acc: 0.6875, auc: 0.8204, precision: 0.875, recall: 0.5\n",
      "2019-01-07T19:31:48.956846, step: 37, loss: 0.53525710105896, acc: 0.6641, auc: 0.8131, precision: 0.7619, recall: 0.2963\n",
      "2019-01-07T19:31:50.802639, step: 38, loss: 0.49173006415367126, acc: 0.7031, auc: 0.8977, precision: 1.0, recall: 0.4154\n",
      "2019-01-07T19:31:52.564361, step: 39, loss: 0.5446023941040039, acc: 0.6562, auc: 0.8252, precision: 0.92, recall: 0.3538\n",
      "2019-01-07T19:31:54.204938, step: 40, loss: 0.5855516791343689, acc: 0.5781, auc: 0.7692, precision: 0.775, recall: 0.4079\n",
      "2019-01-07T19:31:56.453235, step: 41, loss: 0.5418710708618164, acc: 0.7578, auc: 0.8445, precision: 0.7119, recall: 0.75\n",
      "2019-01-07T19:31:58.111658, step: 42, loss: 0.5575596690177917, acc: 0.6953, auc: 0.8085, precision: 0.6857, recall: 0.7385\n",
      "2019-01-07T19:31:59.775445, step: 43, loss: 0.5077488422393799, acc: 0.7812, auc: 0.8467, precision: 0.8333, recall: 0.7031\n",
      "2019-01-07T19:32:01.539407, step: 44, loss: 0.5585774779319763, acc: 0.7578, auc: 0.8193, precision: 0.7742, recall: 0.7385\n",
      "2019-01-07T19:32:03.164168, step: 45, loss: 0.5663617849349976, acc: 0.6953, auc: 0.7758, precision: 0.825, recall: 0.5077\n",
      "2019-01-07T19:32:04.858744, step: 46, loss: 0.4662686884403229, acc: 0.75, auc: 0.8826, precision: 0.8636, recall: 0.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:32:07.026175, step: 47, loss: 0.5496591329574585, acc: 0.6484, auc: 0.8222, precision: 0.8529, recall: 0.4203\n",
      "2019-01-07T19:32:08.576589, step: 48, loss: 0.5208593606948853, acc: 0.6875, auc: 0.8337, precision: 0.8571, recall: 0.4615\n",
      "2019-01-07T19:32:10.289107, step: 49, loss: 0.48199906945228577, acc: 0.7578, auc: 0.8461, precision: 0.8421, recall: 0.5614\n",
      "2019-01-07T19:32:11.984827, step: 50, loss: 0.5084757804870605, acc: 0.7109, auc: 0.8352, precision: 0.85, recall: 0.5231\n",
      "2019-01-07T19:32:13.665589, step: 51, loss: 0.48681581020355225, acc: 0.7969, auc: 0.862, precision: 0.8372, recall: 0.6545\n",
      "2019-01-07T19:32:15.303584, step: 52, loss: 0.5239123106002808, acc: 0.6875, auc: 0.8213, precision: 0.6667, recall: 0.5965\n",
      "2019-01-07T19:32:16.920389, step: 53, loss: 0.5375458598136902, acc: 0.7031, auc: 0.8189, precision: 0.7679, recall: 0.6324\n",
      "2019-01-07T19:32:18.660985, step: 54, loss: 0.46002197265625, acc: 0.7734, auc: 0.8698, precision: 0.7857, recall: 0.7213\n",
      "2019-01-07T19:32:20.832936, step: 55, loss: 0.5069010257720947, acc: 0.7266, auc: 0.8453, precision: 0.8649, recall: 0.5161\n",
      "2019-01-07T19:32:22.582887, step: 56, loss: 0.4802490472793579, acc: 0.7344, auc: 0.8759, precision: 0.9429, recall: 0.5077\n",
      "2019-01-07T19:32:24.249954, step: 57, loss: 0.5224007368087769, acc: 0.7109, auc: 0.8172, precision: 0.8367, recall: 0.5857\n",
      "2019-01-07T19:32:25.909066, step: 58, loss: 0.5681714415550232, acc: 0.7422, auc: 0.8033, precision: 0.8148, recall: 0.6567\n",
      "2019-01-07T19:32:27.557850, step: 59, loss: 0.516281247138977, acc: 0.7656, auc: 0.8365, precision: 0.8077, recall: 0.6774\n",
      "2019-01-07T19:32:29.280844, step: 60, loss: 0.43034234642982483, acc: 0.7656, auc: 0.8868, precision: 0.8361, recall: 0.7183\n",
      "2019-01-07T19:32:30.982675, step: 61, loss: 0.543235182762146, acc: 0.7188, auc: 0.8202, precision: 0.7115, recall: 0.6379\n",
      "2019-01-07T19:32:32.661821, step: 62, loss: 0.4352260231971741, acc: 0.7734, auc: 0.8837, precision: 0.9074, recall: 0.6712\n",
      "2019-01-07T19:32:34.446050, step: 63, loss: 0.583091676235199, acc: 0.6719, auc: 0.7749, precision: 0.766, recall: 0.5373\n",
      "2019-01-07T19:32:36.145673, step: 64, loss: 0.3853971064090729, acc: 0.8125, auc: 0.9234, precision: 0.9273, recall: 0.7183\n",
      "2019-01-07T19:32:37.908909, step: 65, loss: 0.430711030960083, acc: 0.8047, auc: 0.8879, precision: 0.8679, recall: 0.7188\n",
      "2019-01-07T19:32:39.684155, step: 66, loss: 0.4782486855983734, acc: 0.7812, auc: 0.8608, precision: 0.74, recall: 0.7115\n",
      "2019-01-07T19:32:41.332324, step: 67, loss: 0.5565792322158813, acc: 0.7109, auc: 0.792, precision: 0.6957, recall: 0.5818\n",
      "2019-01-07T19:32:43.007755, step: 68, loss: 0.49217069149017334, acc: 0.7344, auc: 0.8508, precision: 0.8485, recall: 0.4912\n",
      "2019-01-07T19:32:44.653910, step: 69, loss: 0.5520052909851074, acc: 0.7188, auc: 0.7983, precision: 0.8235, recall: 0.4828\n",
      "2019-01-07T19:32:46.268341, step: 70, loss: 0.5514218807220459, acc: 0.7266, auc: 0.8045, precision: 0.7755, recall: 0.6129\n",
      "2019-01-07T19:32:47.948682, step: 71, loss: 0.4125455617904663, acc: 0.8047, auc: 0.9006, precision: 0.88, recall: 0.6984\n",
      "2019-01-07T19:32:49.642762, step: 72, loss: 0.37447160482406616, acc: 0.8594, auc: 0.9276, precision: 0.8958, recall: 0.7679\n",
      "2019-01-07T19:32:51.399609, step: 73, loss: 0.5238070487976074, acc: 0.7578, auc: 0.8149, precision: 0.8148, recall: 0.6769\n",
      "2019-01-07T19:32:53.029402, step: 74, loss: 0.4359983205795288, acc: 0.8125, auc: 0.8873, precision: 0.7966, recall: 0.7966\n",
      "2019-01-07T19:32:54.836949, step: 75, loss: 0.5121653079986572, acc: 0.7891, auc: 0.8444, precision: 0.7969, recall: 0.7846\n",
      "2019-01-07T19:32:56.467837, step: 76, loss: 0.46949195861816406, acc: 0.7266, auc: 0.8713, precision: 0.8571, recall: 0.5538\n",
      "2019-01-07T19:32:58.078320, step: 77, loss: 0.5245318412780762, acc: 0.7266, auc: 0.8606, precision: 0.9512, recall: 0.5417\n",
      "2019-01-07T19:32:59.740780, step: 78, loss: 0.48832032084465027, acc: 0.7188, auc: 0.8515, precision: 0.8889, recall: 0.6154\n",
      "2019-01-07T19:33:01.360214, step: 79, loss: 0.4875852167606354, acc: 0.75, auc: 0.8689, precision: 0.7581, recall: 0.7344\n",
      "2019-01-07T19:33:03.224016, step: 80, loss: 0.5483386516571045, acc: 0.7734, auc: 0.8011, precision: 0.8082, recall: 0.7973\n",
      "2019-01-07T19:33:05.075898, step: 81, loss: 0.5073555111885071, acc: 0.7734, auc: 0.8672, precision: 0.75, recall: 0.8308\n",
      "2019-01-07T19:33:06.689390, step: 82, loss: 0.4575634002685547, acc: 0.8203, auc: 0.875, precision: 0.85, recall: 0.7846\n",
      "2019-01-07T19:33:08.415627, step: 83, loss: 0.5159865021705627, acc: 0.7422, auc: 0.817, precision: 0.8537, recall: 0.5645\n",
      "2019-01-07T19:33:10.050117, step: 84, loss: 0.5472917556762695, acc: 0.6641, auc: 0.817, precision: 0.825, recall: 0.4783\n",
      "2019-01-07T19:33:12.057663, step: 85, loss: 0.4867143929004669, acc: 0.6953, auc: 0.8514, precision: 0.8125, recall: 0.4407\n",
      "2019-01-07T19:33:14.136505, step: 86, loss: 0.4402931034564972, acc: 0.7734, auc: 0.8864, precision: 0.8913, recall: 0.6308\n",
      "2019-01-07T19:33:16.337441, step: 87, loss: 0.4593181610107422, acc: 0.7969, auc: 0.8787, precision: 0.8519, recall: 0.7188\n",
      "2019-01-07T19:33:17.954078, step: 88, loss: 0.5013104677200317, acc: 0.7578, auc: 0.833, precision: 0.7833, recall: 0.7231\n",
      "2019-01-07T19:33:19.604065, step: 89, loss: 0.4535333216190338, acc: 0.8281, auc: 0.8851, precision: 0.8448, recall: 0.7903\n",
      "2019-01-07T19:33:21.161926, step: 90, loss: 0.4692155718803406, acc: 0.7344, auc: 0.86, precision: 0.898, recall: 0.6027\n",
      "2019-01-07T19:33:22.913557, step: 91, loss: 0.34608563780784607, acc: 0.875, auc: 0.948, precision: 0.9016, recall: 0.8462\n",
      "2019-01-07T19:33:24.976051, step: 92, loss: 0.4542086124420166, acc: 0.7656, auc: 0.8728, precision: 0.8182, recall: 0.6923\n",
      "2019-01-07T19:33:26.707106, step: 93, loss: 0.47758370637893677, acc: 0.7578, auc: 0.8598, precision: 0.9091, recall: 0.597\n",
      "2019-01-07T19:33:28.316905, step: 94, loss: 0.404996395111084, acc: 0.8281, auc: 0.9041, precision: 0.9333, recall: 0.6885\n",
      "2019-01-07T19:33:29.923958, step: 95, loss: 0.43245211243629456, acc: 0.8203, auc: 0.8879, precision: 0.8571, recall: 0.7619\n",
      "2019-01-07T19:33:31.631898, step: 96, loss: 0.4859805405139923, acc: 0.7578, auc: 0.8468, precision: 0.8039, recall: 0.6613\n",
      "2019-01-07T19:33:33.325320, step: 97, loss: 0.3991283178329468, acc: 0.8281, auc: 0.9084, precision: 0.8462, recall: 0.7586\n",
      "2019-01-07T19:33:34.937003, step: 98, loss: 0.43064647912979126, acc: 0.75, auc: 0.8789, precision: 0.881, recall: 0.5781\n",
      "2019-01-07T19:33:37.061234, step: 99, loss: 0.498721182346344, acc: 0.7344, auc: 0.8782, precision: 0.9744, recall: 0.5352\n",
      "2019-01-07T19:33:38.707937, step: 100, loss: 0.4111703038215637, acc: 0.7969, auc: 0.8955, precision: 0.8246, recall: 0.746\n",
      "\n",
      "Evaluation:\n",
      "2019-01-07T19:34:48.373451, step: 100, loss: 0.46380538818163747, acc: 0.7864641025641026, auc: 0.8684282051282052, precision: 0.8127410256410256, recall: 0.7498820512820512\n",
      "2019-01-07T19:34:50.084455, step: 101, loss: 0.5485406517982483, acc: 0.7734, auc: 0.8104, precision: 0.8235, recall: 0.6774\n",
      "2019-01-07T19:34:51.742936, step: 102, loss: 0.5002821087837219, acc: 0.7812, auc: 0.8498, precision: 0.7547, recall: 0.7273\n",
      "2019-01-07T19:34:53.401953, step: 103, loss: 0.48142021894454956, acc: 0.7656, auc: 0.8511, precision: 0.7778, recall: 0.6364\n",
      "2019-01-07T19:34:55.120805, step: 104, loss: 0.5169845223426819, acc: 0.7344, auc: 0.8426, precision: 0.8182, recall: 0.5806\n",
      "2019-01-07T19:34:56.796086, step: 105, loss: 0.41538357734680176, acc: 0.7734, auc: 0.9009, precision: 0.907, recall: 0.6094\n",
      "2019-01-07T19:34:59.031363, step: 106, loss: 0.5034124851226807, acc: 0.75, auc: 0.8373, precision: 0.7869, recall: 0.7164\n",
      "2019-01-07T19:35:00.685107, step: 107, loss: 0.47684168815612793, acc: 0.8047, auc: 0.8809, precision: 0.7636, recall: 0.7778\n",
      "2019-01-07T19:35:02.317371, step: 108, loss: 0.3656690716743469, acc: 0.8516, auc: 0.933, precision: 0.8545, recall: 0.8103\n",
      "2019-01-07T19:35:03.994520, step: 109, loss: 0.4403175711631775, acc: 0.8047, auc: 0.885, precision: 0.94, recall: 0.6812\n",
      "2019-01-07T19:35:05.600834, step: 110, loss: 0.5035355687141418, acc: 0.7266, auc: 0.8327, precision: 0.7857, recall: 0.5593\n",
      "2019-01-07T19:35:07.308702, step: 111, loss: 0.42317476868629456, acc: 0.7734, auc: 0.8926, precision: 0.9048, recall: 0.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:35:08.971647, step: 112, loss: 0.41812634468078613, acc: 0.8203, auc: 0.8955, precision: 0.9348, recall: 0.6825\n",
      "2019-01-07T19:35:10.640816, step: 113, loss: 0.458964079618454, acc: 0.7969, auc: 0.8784, precision: 0.8269, recall: 0.7167\n",
      "2019-01-07T19:35:12.372822, step: 114, loss: 0.46779465675354004, acc: 0.7656, auc: 0.8616, precision: 0.7692, recall: 0.6897\n",
      "2019-01-07T19:35:14.035467, step: 115, loss: 0.4172801077365875, acc: 0.7812, auc: 0.8906, precision: 0.8462, recall: 0.6875\n",
      "2019-01-07T19:35:15.751887, step: 116, loss: 0.42990991473197937, acc: 0.7656, auc: 0.8821, precision: 0.7885, recall: 0.6833\n",
      "2019-01-07T19:35:17.449226, step: 117, loss: 0.4330533444881439, acc: 0.7891, auc: 0.8799, precision: 0.8333, recall: 0.678\n",
      "2019-01-07T19:35:19.154455, step: 118, loss: 0.5181000232696533, acc: 0.7344, auc: 0.8367, precision: 0.8627, recall: 0.6197\n",
      "2019-01-07T19:35:21.483591, step: 119, loss: 0.4686945378780365, acc: 0.7969, auc: 0.8554, precision: 0.871, recall: 0.75\n",
      "2019-01-07T19:35:23.199128, step: 120, loss: 0.5023648738861084, acc: 0.7656, auc: 0.8352, precision: 0.8596, recall: 0.6901\n",
      "2019-01-07T19:35:24.867970, step: 121, loss: 0.42691218852996826, acc: 0.8438, auc: 0.904, precision: 0.8267, recall: 0.8986\n",
      "2019-01-07T19:35:26.595310, step: 122, loss: 0.41164690256118774, acc: 0.8203, auc: 0.8912, precision: 0.8955, recall: 0.7895\n",
      "2019-01-07T19:35:28.304807, step: 123, loss: 0.4054749011993408, acc: 0.8203, auc: 0.9101, precision: 0.8286, recall: 0.8406\n",
      "2019-01-07T19:35:30.052218, step: 124, loss: 0.5166648626327515, acc: 0.7656, auc: 0.8355, precision: 0.82, recall: 0.6613\n",
      "2019-01-07T19:35:32.308042, step: 125, loss: 0.4809924066066742, acc: 0.7578, auc: 0.8557, precision: 0.7451, recall: 0.6786\n",
      "2019-01-07T19:35:34.056321, step: 126, loss: 0.43004098534584045, acc: 0.7578, auc: 0.8858, precision: 0.8611, recall: 0.5439\n",
      "2019-01-07T19:35:36.349199, step: 127, loss: 0.44279980659484863, acc: 0.7188, auc: 0.9109, precision: 0.9355, recall: 0.4603\n",
      "2019-01-07T19:35:38.090461, step: 128, loss: 0.49798476696014404, acc: 0.7266, auc: 0.8331, precision: 0.8065, recall: 0.463\n",
      "2019-01-07T19:35:39.650368, step: 129, loss: 0.4622749090194702, acc: 0.7656, auc: 0.8631, precision: 0.8636, recall: 0.6129\n",
      "2019-01-07T19:35:41.438032, step: 130, loss: 0.3976336419582367, acc: 0.8438, auc: 0.9071, precision: 0.8621, recall: 0.8065\n",
      "2019-01-07T19:35:43.138044, step: 131, loss: 0.45756107568740845, acc: 0.8047, auc: 0.8849, precision: 0.8033, recall: 0.7903\n",
      "2019-01-07T19:35:45.143880, step: 132, loss: 0.49617359042167664, acc: 0.7656, auc: 0.8481, precision: 0.7818, recall: 0.7049\n",
      "2019-01-07T19:35:46.857555, step: 133, loss: 0.47054523229599, acc: 0.7188, auc: 0.8622, precision: 0.8571, recall: 0.5455\n",
      "2019-01-07T19:35:48.971190, step: 134, loss: 0.4393419325351715, acc: 0.7422, auc: 0.8848, precision: 0.8571, recall: 0.6176\n",
      "2019-01-07T19:35:50.649189, step: 135, loss: 0.4992716610431671, acc: 0.7578, auc: 0.8457, precision: 0.8958, recall: 0.6232\n",
      "2019-01-07T19:35:52.502350, step: 136, loss: 0.46040111780166626, acc: 0.7812, auc: 0.8622, precision: 0.88, recall: 0.6667\n",
      "2019-01-07T19:35:54.658096, step: 137, loss: 0.4596490263938904, acc: 0.8359, auc: 0.9088, precision: 0.7812, recall: 0.8772\n",
      "2019-01-07T19:35:56.388552, step: 138, loss: 0.40195387601852417, acc: 0.8203, auc: 0.9116, precision: 0.8226, recall: 0.8095\n",
      "2019-01-07T19:35:58.177502, step: 139, loss: 0.4270061254501343, acc: 0.7734, auc: 0.8817, precision: 0.8772, recall: 0.6944\n",
      "2019-01-07T19:35:59.877274, step: 140, loss: 0.5040615200996399, acc: 0.7656, auc: 0.8315, precision: 0.814, recall: 0.614\n",
      "2019-01-07T19:36:01.560180, step: 141, loss: 0.4512535631656647, acc: 0.7578, auc: 0.8897, precision: 0.9111, recall: 0.6029\n",
      "2019-01-07T19:36:03.864816, step: 142, loss: 0.37658238410949707, acc: 0.8359, auc: 0.9083, precision: 0.8095, recall: 0.7234\n",
      "2019-01-07T19:36:05.941050, step: 143, loss: 0.48047029972076416, acc: 0.7969, auc: 0.8629, precision: 0.9245, recall: 0.6901\n",
      "2019-01-07T19:36:07.675527, step: 144, loss: 0.4155304431915283, acc: 0.8203, auc: 0.895, precision: 0.9524, recall: 0.6557\n",
      "2019-01-07T19:36:09.328786, step: 145, loss: 0.5244355797767639, acc: 0.75, auc: 0.8281, precision: 0.9302, recall: 0.5797\n",
      "2019-01-07T19:36:11.120402, step: 146, loss: 0.48687657713890076, acc: 0.7812, auc: 0.8563, precision: 0.7887, recall: 0.8116\n",
      "2019-01-07T19:36:12.895265, step: 147, loss: 0.592477560043335, acc: 0.7812, auc: 0.8165, precision: 0.7463, recall: 0.8197\n",
      "2019-01-07T19:36:14.567677, step: 148, loss: 0.4295284152030945, acc: 0.8125, auc: 0.8987, precision: 0.7941, recall: 0.8438\n",
      "2019-01-07T19:36:16.210642, step: 149, loss: 0.4330236315727234, acc: 0.7891, auc: 0.8956, precision: 0.7619, recall: 0.8\n",
      "2019-01-07T19:36:18.542847, step: 150, loss: 0.5032787919044495, acc: 0.6953, auc: 0.8398, precision: 0.88, recall: 0.5714\n",
      "2019-01-07T19:36:20.468105, step: 151, loss: 0.43802309036254883, acc: 0.7422, auc: 0.8936, precision: 0.8889, recall: 0.5246\n",
      "2019-01-07T19:36:22.199339, step: 152, loss: 0.3986932933330536, acc: 0.7969, auc: 0.9029, precision: 0.9048, recall: 0.6333\n",
      "2019-01-07T19:36:24.429617, step: 153, loss: 0.4504939317703247, acc: 0.7422, auc: 0.8833, precision: 0.8936, recall: 0.6\n",
      "2019-01-07T19:36:26.481970, step: 154, loss: 0.40867307782173157, acc: 0.8203, auc: 0.8918, precision: 0.825, recall: 0.6735\n",
      "2019-01-07T19:36:28.237116, step: 155, loss: 0.42254874110221863, acc: 0.8047, auc: 0.8914, precision: 0.8269, recall: 0.7288\n",
      "2019-01-07T19:36:30.100726, step: 156, loss: 0.3905501365661621, acc: 0.8359, auc: 0.9091, precision: 0.8852, recall: 0.7941\n",
      "start training model\n",
      "2019-01-07T19:36:31.794168, step: 157, loss: 0.46433794498443604, acc: 0.7656, auc: 0.8666, precision: 0.7857, recall: 0.7097\n",
      "2019-01-07T19:36:33.489695, step: 158, loss: 0.42703554034233093, acc: 0.8281, auc: 0.9018, precision: 0.84, recall: 0.75\n",
      "2019-01-07T19:36:35.151155, step: 159, loss: 0.4375489354133606, acc: 0.7656, auc: 0.8907, precision: 0.9535, recall: 0.5942\n",
      "2019-01-07T19:36:36.830160, step: 160, loss: 0.3627168536186218, acc: 0.8203, auc: 0.9326, precision: 0.9762, recall: 0.6508\n",
      "2019-01-07T19:36:39.087094, step: 161, loss: 0.4594196081161499, acc: 0.7344, auc: 0.8639, precision: 0.85, recall: 0.5484\n",
      "2019-01-07T19:36:41.162336, step: 162, loss: 0.4173692762851715, acc: 0.8047, auc: 0.8937, precision: 0.8936, recall: 0.6774\n",
      "2019-01-07T19:36:43.017065, step: 163, loss: 0.4287112355232239, acc: 0.8281, auc: 0.8921, precision: 0.8333, recall: 0.7407\n",
      "2019-01-07T19:36:44.812444, step: 164, loss: 0.47343209385871887, acc: 0.7891, auc: 0.863, precision: 0.8167, recall: 0.7538\n",
      "2019-01-07T19:36:46.477980, step: 165, loss: 0.36023327708244324, acc: 0.875, auc: 0.9334, precision: 0.8947, recall: 0.8361\n",
      "2019-01-07T19:36:48.135838, step: 166, loss: 0.44493478536605835, acc: 0.7891, auc: 0.8767, precision: 0.8627, recall: 0.6875\n",
      "2019-01-07T19:36:49.834335, step: 167, loss: 0.3316422402858734, acc: 0.8203, auc: 0.9399, precision: 0.907, recall: 0.6724\n",
      "2019-01-07T19:36:51.530092, step: 168, loss: 0.4224399924278259, acc: 0.7812, auc: 0.9017, precision: 0.9697, recall: 0.5424\n",
      "2019-01-07T19:36:53.257858, step: 169, loss: 0.4318024218082428, acc: 0.7656, auc: 0.8926, precision: 0.8571, recall: 0.6462\n",
      "2019-01-07T19:36:55.155624, step: 170, loss: 0.30550143122673035, acc: 0.8594, auc: 0.9482, precision: 0.9787, recall: 0.7302\n",
      "2019-01-07T19:36:57.449387, step: 171, loss: 0.40910691022872925, acc: 0.8281, auc: 0.9066, precision: 0.8125, recall: 0.8387\n",
      "2019-01-07T19:36:59.172883, step: 172, loss: 0.41968491673469543, acc: 0.7656, auc: 0.8863, precision: 0.8261, recall: 0.76\n",
      "2019-01-07T19:37:00.972165, step: 173, loss: 0.3974021077156067, acc: 0.8438, auc: 0.9104, precision: 0.8438, recall: 0.8438\n",
      "2019-01-07T19:37:02.664939, step: 174, loss: 0.41575491428375244, acc: 0.7891, auc: 0.8923, precision: 0.918, recall: 0.7179\n",
      "2019-01-07T19:37:04.478619, step: 175, loss: 0.41802090406417847, acc: 0.7891, auc: 0.8886, precision: 0.8261, recall: 0.6667\n",
      "2019-01-07T19:37:06.143843, step: 176, loss: 0.4494130313396454, acc: 0.7812, auc: 0.8811, precision: 0.8679, recall: 0.6866\n",
      "2019-01-07T19:37:07.850806, step: 177, loss: 0.4546501338481903, acc: 0.7656, auc: 0.8728, precision: 0.8298, recall: 0.6393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-07T19:37:09.617823, step: 178, loss: 0.4835854172706604, acc: 0.7344, auc: 0.8638, precision: 0.8627, recall: 0.6197\n",
      "2019-01-07T19:37:11.900036, step: 179, loss: 0.484719842672348, acc: 0.7422, auc: 0.8475, precision: 0.7895, recall: 0.6818\n",
      "2019-01-07T19:37:14.172660, step: 180, loss: 0.43855419754981995, acc: 0.8438, auc: 0.8976, precision: 0.8333, recall: 0.9028\n",
      "2019-01-07T19:37:16.182450, step: 181, loss: 0.47969862818717957, acc: 0.7969, auc: 0.8747, precision: 0.7705, recall: 0.7966\n",
      "2019-01-07T19:37:18.407787, step: 182, loss: 0.4235095977783203, acc: 0.7969, auc: 0.8867, precision: 0.8596, recall: 0.7313\n",
      "2019-01-07T19:37:20.112804, step: 183, loss: 0.3972729444503784, acc: 0.7969, auc: 0.9076, precision: 0.9348, recall: 0.6515\n",
      "2019-01-07T19:37:22.152642, step: 184, loss: 0.42737120389938354, acc: 0.7656, auc: 0.8929, precision: 0.8333, recall: 0.6034\n",
      "2019-01-07T19:37:23.920184, step: 185, loss: 0.36068832874298096, acc: 0.8281, auc: 0.9263, precision: 0.9767, recall: 0.6667\n",
      "2019-01-07T19:37:25.652933, step: 186, loss: 0.44491416215896606, acc: 0.7422, auc: 0.8894, precision: 0.9189, recall: 0.5312\n",
      "2019-01-07T19:37:27.418175, step: 187, loss: 0.4750785827636719, acc: 0.8125, auc: 0.8537, precision: 0.9231, recall: 0.7059\n",
      "2019-01-07T19:37:29.202983, step: 188, loss: 0.47092247009277344, acc: 0.7656, auc: 0.8531, precision: 0.8305, recall: 0.7101\n",
      "2019-01-07T19:37:30.942588, step: 189, loss: 0.5294285416603088, acc: 0.7969, auc: 0.8402, precision: 0.7887, recall: 0.8358\n",
      "2019-01-07T19:37:32.784932, step: 190, loss: 0.41292256116867065, acc: 0.8281, auc: 0.9107, precision: 0.8, recall: 0.8955\n",
      "2019-01-07T19:37:34.452736, step: 191, loss: 0.47619903087615967, acc: 0.7812, auc: 0.8668, precision: 0.7639, recall: 0.8333\n",
      "2019-01-07T19:37:36.264040, step: 192, loss: 0.4818323254585266, acc: 0.75, auc: 0.8497, precision: 0.875, recall: 0.5645\n",
      "2019-01-07T19:37:37.903179, step: 193, loss: 0.3275164067745209, acc: 0.7969, auc: 0.9582, precision: 0.9545, recall: 0.6364\n",
      "2019-01-07T19:37:39.728731, step: 194, loss: 0.4492480158805847, acc: 0.6875, auc: 0.8901, precision: 0.8649, recall: 0.4776\n",
      "2019-01-07T19:37:41.529645, step: 195, loss: 0.37292417883872986, acc: 0.8828, auc: 0.9187, precision: 0.9487, recall: 0.74\n",
      "2019-01-07T19:37:43.280037, step: 196, loss: 0.3653481602668762, acc: 0.8125, auc: 0.9342, precision: 0.9412, recall: 0.6957\n",
      "2019-01-07T19:37:45.193959, step: 197, loss: 0.45370590686798096, acc: 0.7969, auc: 0.8686, precision: 0.84, recall: 0.7\n",
      "2019-01-07T19:37:46.901469, step: 198, loss: 0.38149210810661316, acc: 0.8672, auc: 0.9229, precision: 0.8621, recall: 0.8475\n",
      "2019-01-07T19:37:48.647945, step: 199, loss: 0.3523762822151184, acc: 0.7969, auc: 0.9208, precision: 0.8657, recall: 0.7733\n",
      "2019-01-07T19:37:50.434954, step: 200, loss: 0.48011356592178345, acc: 0.7734, auc: 0.8554, precision: 0.7547, recall: 0.7143\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "# 定义计算图\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = BiLSTMAttention(config)\n",
    "        \n",
    "        # 实例化BiLM对象，这个必须放置在全局下，不能在elmo函数中定义，否则会出现重复生成tensorflow节点。\n",
    "        with tf.variable_scope(\"bilm\", reuse=True):\n",
    "            bilm = BidirectionalLanguageModel(\n",
    "                    config.optionFile,\n",
    "                    config.weightFile,\n",
    "                    use_character_inputs=False,\n",
    "                    embedding_weight_file=config.tokenEmbeddingFile\n",
    "                    )\n",
    "        inputData = tf.placeholder('int32', shape=(None, None))\n",
    "        \n",
    "        # 调用bilm中的__call__方法生成op对象\n",
    "        inputEmbeddingsOp = bilm(inputData) \n",
    "        \n",
    "        # 计算ELMo向量表示\n",
    "        elmoInput = weight_layers('input', inputEmbeddingsOp, l2_coef=0.0)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/textCNN/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        def elmo(reviews):\n",
    "            \"\"\"\n",
    "            对每一个输入的batch都动态的生成词向量表示\n",
    "            \"\"\"\n",
    "\n",
    "#           tf.reset_default_graph()\n",
    "            # TokenBatcher是生成词表示的batch类\n",
    "            batcher = TokenBatcher(config.vocabFile)\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                # 生成batch数据\n",
    "                inputDataIndex = batcher.batch_sentences(reviews)\n",
    "\n",
    "                # 计算ELMo的向量表示\n",
    "                elmoInputVec = sess.run(\n",
    "                    [elmoInput['weighted_op']],\n",
    "                    feed_dict={inputData: inputDataIndex}\n",
    "                )\n",
    "\n",
    "            return elmoInputVec\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            \n",
    "            feed_dict = {\n",
    "              cnn.inputX: elmo(batchX)[0],  # inputX直接用动态生成的ELMo向量表示代入\n",
    "              cnn.inputY: np.array(batchY, dtype=\"float32\"),\n",
    "              cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.inputX: elmo(batchX)[0],\n",
    "              cnn.inputY: np.array(batchY, dtype=\"float32\"),\n",
    "              cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions, cnn.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(cnn.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
