{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 2\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM+Attention/summarys\n",
      "\n",
      "start training model\n",
      "2018-12-28T15:24:17.850716, step: 1, loss: 0.7303193807601929, acc: 0.4609, auc: 0.4585, precision: 0.3333, recall: 0.0299\n",
      "2018-12-28T15:24:18.279513, step: 2, loss: 0.8423295021057129, acc: 0.4375, auc: 0.4834, precision: 0.4356, recall: 0.7458\n",
      "2018-12-28T15:24:18.649039, step: 3, loss: 0.773879885673523, acc: 0.5156, auc: 0.4586, precision: 0.6364, recall: 0.1077\n",
      "2018-12-28T15:24:19.005337, step: 4, loss: 0.8196670413017273, acc: 0.4766, auc: 0.4795, precision: 0.75, recall: 0.0435\n",
      "2018-12-28T15:24:19.415834, step: 5, loss: 0.7229666709899902, acc: 0.4531, auc: 0.58, precision: 0.8333, recall: 0.1282\n",
      "2018-12-28T15:24:19.798114, step: 6, loss: 0.7233154773712158, acc: 0.5078, auc: 0.5132, precision: 0.4872, recall: 0.3065\n",
      "2018-12-28T15:24:20.226828, step: 7, loss: 0.8467093706130981, acc: 0.4766, auc: 0.4556, precision: 0.4624, recall: 0.7167\n",
      "2018-12-28T15:24:20.624670, step: 8, loss: 0.7496055364608765, acc: 0.5547, auc: 0.5515, precision: 0.5432, recall: 0.6875\n",
      "2018-12-28T15:24:21.028124, step: 9, loss: 0.710146427154541, acc: 0.4844, auc: 0.5144, precision: 0.5306, recall: 0.3768\n",
      "2018-12-28T15:24:21.435430, step: 10, loss: 0.698042631149292, acc: 0.5469, auc: 0.5492, precision: 0.6087, recall: 0.2222\n",
      "2018-12-28T15:24:21.828103, step: 11, loss: 0.7113831043243408, acc: 0.5938, auc: 0.4925, precision: 0.7778, recall: 0.1228\n",
      "2018-12-28T15:24:22.281712, step: 12, loss: 0.6890966296195984, acc: 0.4766, auc: 0.6293, precision: 0.5, recall: 0.0149\n",
      "2018-12-28T15:24:22.708671, step: 13, loss: 0.7541261315345764, acc: 0.5, auc: 0.4654, precision: 0.0, recall: 0.0\n",
      "2018-12-28T15:24:23.117783, step: 14, loss: 0.6968572735786438, acc: 0.5859, auc: 0.5138, precision: 1.0, recall: 0.0185\n",
      "2018-12-28T15:24:23.519532, step: 15, loss: 0.717898964881897, acc: 0.5312, auc: 0.4924, precision: 1.0, recall: 0.0323\n",
      "2018-12-28T15:24:23.910747, step: 16, loss: 0.7183476090431213, acc: 0.4844, auc: 0.519, precision: 0.6667, recall: 0.0299\n",
      "2018-12-28T15:24:24.316882, step: 17, loss: 0.6799620389938354, acc: 0.5469, auc: 0.5941, precision: 0.8333, recall: 0.0806\n",
      "2018-12-28T15:24:24.714947, step: 18, loss: 0.7016047239303589, acc: 0.5234, auc: 0.5515, precision: 0.3846, recall: 0.0862\n",
      "2018-12-28T15:24:25.133198, step: 19, loss: 0.7080107927322388, acc: 0.5, auc: 0.5131, precision: 0.5294, recall: 0.1385\n",
      "2018-12-28T15:24:25.534117, step: 20, loss: 0.7204374074935913, acc: 0.4688, auc: 0.4933, precision: 0.3704, recall: 0.1639\n",
      "2018-12-28T15:24:25.936757, step: 21, loss: 0.7060072422027588, acc: 0.4766, auc: 0.5308, precision: 0.4865, recall: 0.2727\n",
      "2018-12-28T15:24:26.337021, step: 22, loss: 0.7101264595985413, acc: 0.5469, auc: 0.5573, precision: 0.4091, recall: 0.1667\n",
      "2018-12-28T15:24:26.701164, step: 23, loss: 0.686893105506897, acc: 0.4766, auc: 0.5547, precision: 0.6, recall: 0.1286\n",
      "2018-12-28T15:24:27.133359, step: 24, loss: 0.683013379573822, acc: 0.5234, auc: 0.5734, precision: 1.0, recall: 0.0317\n",
      "2018-12-28T15:24:27.559083, step: 25, loss: 0.6778931021690369, acc: 0.5625, auc: 0.6, precision: 0.75, recall: 0.1\n",
      "2018-12-28T15:24:28.019890, step: 26, loss: 0.713559627532959, acc: 0.4688, auc: 0.5335, precision: 0.7143, recall: 0.0704\n",
      "2018-12-28T15:24:28.443009, step: 27, loss: 0.7003100514411926, acc: 0.3984, auc: 0.6176, precision: 0.25, recall: 0.0133\n",
      "2018-12-28T15:24:28.875547, step: 28, loss: 0.6628274917602539, acc: 0.5547, auc: 0.6271, precision: 0.625, recall: 0.0847\n",
      "2018-12-28T15:24:29.255942, step: 29, loss: 0.6813476085662842, acc: 0.5312, auc: 0.5729, precision: 0.7778, recall: 0.2\n",
      "2018-12-28T15:24:29.622770, step: 30, loss: 0.6749886274337769, acc: 0.5391, auc: 0.6186, precision: 0.6429, recall: 0.1429\n",
      "2018-12-28T15:24:30.026234, step: 31, loss: 0.6965550184249878, acc: 0.5703, auc: 0.5512, precision: 0.5, recall: 0.1818\n",
      "2018-12-28T15:24:30.424357, step: 32, loss: 0.7180560827255249, acc: 0.4922, auc: 0.4822, precision: 0.4762, recall: 0.1562\n",
      "2018-12-28T15:24:30.799424, step: 33, loss: 0.6582143306732178, acc: 0.5234, auc: 0.6584, precision: 0.7143, recall: 0.1493\n",
      "2018-12-28T15:24:31.175625, step: 34, loss: 0.7093254327774048, acc: 0.4766, auc: 0.504, precision: 0.5, recall: 0.0597\n",
      "2018-12-28T15:24:31.578717, step: 35, loss: 0.6820636987686157, acc: 0.4922, auc: 0.5976, precision: 0.5, recall: 0.0769\n",
      "2018-12-28T15:24:31.961677, step: 36, loss: 0.702938437461853, acc: 0.4844, auc: 0.5121, precision: 0.7, recall: 0.1\n",
      "2018-12-28T15:24:32.327227, step: 37, loss: 0.6599609851837158, acc: 0.5469, auc: 0.6549, precision: 0.8333, recall: 0.1515\n",
      "2018-12-28T15:24:32.690455, step: 38, loss: 0.6862937211990356, acc: 0.5781, auc: 0.6017, precision: 0.4167, recall: 0.0962\n",
      "2018-12-28T15:24:33.090533, step: 39, loss: 0.6525205969810486, acc: 0.5078, auc: 0.6887, precision: 0.7143, recall: 0.0758\n",
      "2018-12-28T15:24:33.500441, step: 40, loss: 0.6751327514648438, acc: 0.5391, auc: 0.6352, precision: 0.4286, recall: 0.0517\n",
      "2018-12-28T15:24:33.870231, step: 41, loss: 0.6792119741439819, acc: 0.4531, auc: 0.6311, precision: 0.6667, recall: 0.0282\n",
      "2018-12-28T15:24:34.237367, step: 42, loss: 0.6624789237976074, acc: 0.5312, auc: 0.6455, precision: 0.6667, recall: 0.0328\n",
      "2018-12-28T15:24:34.659572, step: 43, loss: 0.6559832692146301, acc: 0.5312, auc: 0.6812, precision: 1.0, recall: 0.0625\n",
      "2018-12-28T15:24:35.040205, step: 44, loss: 0.65240478515625, acc: 0.5, auc: 0.6982, precision: 0.6667, recall: 0.0308\n",
      "2018-12-28T15:24:35.440222, step: 45, loss: 0.6708592176437378, acc: 0.5703, auc: 0.6193, precision: 0.75, recall: 0.1017\n",
      "2018-12-28T15:24:35.861950, step: 46, loss: 0.6699068546295166, acc: 0.5234, auc: 0.631, precision: 0.375, recall: 0.0508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T15:24:36.271613, step: 47, loss: 0.6775609254837036, acc: 0.5391, auc: 0.5994, precision: 1.0, recall: 0.0781\n",
      "2018-12-28T15:24:36.671129, step: 48, loss: 0.647729218006134, acc: 0.4922, auc: 0.7065, precision: 0.8333, recall: 0.0725\n",
      "2018-12-28T15:24:37.039531, step: 49, loss: 0.660158634185791, acc: 0.5, auc: 0.6496, precision: 0.6667, recall: 0.0896\n",
      "2018-12-28T15:24:37.441499, step: 50, loss: 0.6141077280044556, acc: 0.5781, auc: 0.7909, precision: 1.0, recall: 0.25\n",
      "2018-12-28T15:24:37.845798, step: 51, loss: 0.6374475955963135, acc: 0.6016, auc: 0.7316, precision: 0.7667, recall: 0.3433\n",
      "2018-12-28T15:24:38.237975, step: 52, loss: 0.6290765404701233, acc: 0.6562, auc: 0.7696, precision: 0.8438, recall: 0.4091\n",
      "2018-12-28T15:24:38.630018, step: 53, loss: 0.620800256729126, acc: 0.6875, auc: 0.8028, precision: 0.84, recall: 0.3684\n",
      "2018-12-28T15:24:38.997095, step: 54, loss: 0.6119220852851868, acc: 0.5859, auc: 0.7759, precision: 0.9, recall: 0.1475\n",
      "2018-12-28T15:24:39.391889, step: 55, loss: 0.6027079224586487, acc: 0.5156, auc: 0.8442, precision: 1.0, recall: 0.0312\n",
      "2018-12-28T15:24:39.785024, step: 56, loss: 0.6208940744400024, acc: 0.5391, auc: 0.7639, precision: 0.7273, recall: 0.125\n",
      "2018-12-28T15:24:40.182714, step: 57, loss: 0.5363178253173828, acc: 0.7344, auc: 0.8921, precision: 0.878, recall: 0.5538\n",
      "2018-12-28T15:24:40.564506, step: 58, loss: 0.6392598152160645, acc: 0.75, auc: 0.8028, precision: 0.7015, recall: 0.7966\n",
      "2018-12-28T15:24:40.948870, step: 59, loss: 0.5360333919525146, acc: 0.6562, auc: 0.849, precision: 0.8235, recall: 0.4242\n",
      "2018-12-28T15:24:41.336345, step: 60, loss: 0.5670222640037537, acc: 0.6328, auc: 0.8312, precision: 0.9583, recall: 0.3333\n",
      "2018-12-28T15:24:41.745120, step: 61, loss: 0.5150421857833862, acc: 0.7109, auc: 0.874, precision: 0.8611, recall: 0.4921\n",
      "2018-12-28T15:24:42.123377, step: 62, loss: 0.5131833553314209, acc: 0.7188, auc: 0.8517, precision: 0.8571, recall: 0.5915\n",
      "2018-12-28T15:24:42.525661, step: 63, loss: 0.5192018151283264, acc: 0.7969, auc: 0.8626, precision: 0.7778, recall: 0.75\n",
      "2018-12-28T15:24:42.919501, step: 64, loss: 0.5385916233062744, acc: 0.7031, auc: 0.8275, precision: 0.8438, recall: 0.45\n",
      "2018-12-28T15:24:43.314764, step: 65, loss: 0.5914270877838135, acc: 0.6328, auc: 0.8159, precision: 0.8611, recall: 0.4247\n",
      "2018-12-28T15:24:43.724890, step: 66, loss: 0.5222290754318237, acc: 0.8125, auc: 0.8902, precision: 0.7183, recall: 0.9273\n",
      "2018-12-28T15:24:44.146041, step: 67, loss: 0.5026854872703552, acc: 0.7969, auc: 0.8468, precision: 0.7887, recall: 0.8358\n",
      "2018-12-28T15:24:44.552082, step: 68, loss: 0.4722599983215332, acc: 0.7344, auc: 0.8833, precision: 0.9091, recall: 0.4918\n",
      "2018-12-28T15:24:44.986375, step: 69, loss: 0.45823758840560913, acc: 0.7422, auc: 0.8828, precision: 0.8788, recall: 0.5\n",
      "2018-12-28T15:24:45.369541, step: 70, loss: 0.4574836194515228, acc: 0.7656, auc: 0.8709, precision: 0.8226, recall: 0.7286\n",
      "2018-12-28T15:24:45.727142, step: 71, loss: 0.5195289850234985, acc: 0.7734, auc: 0.8396, precision: 0.7273, recall: 0.7407\n",
      "2018-12-28T15:24:46.118081, step: 72, loss: 0.44962018728256226, acc: 0.7891, auc: 0.8745, precision: 0.9074, recall: 0.6901\n",
      "2018-12-28T15:24:46.481568, step: 73, loss: 0.47172650694847107, acc: 0.7812, auc: 0.8584, precision: 0.8776, recall: 0.6615\n",
      "2018-12-28T15:24:46.844464, step: 74, loss: 0.4590047299861908, acc: 0.7812, auc: 0.8655, precision: 0.8261, recall: 0.6552\n",
      "2018-12-28T15:24:47.231149, step: 75, loss: 0.44452428817749023, acc: 0.7969, auc: 0.8806, precision: 0.9302, recall: 0.6349\n",
      "2018-12-28T15:24:47.680729, step: 76, loss: 0.4748423099517822, acc: 0.7734, auc: 0.8576, precision: 0.8276, recall: 0.7164\n",
      "2018-12-28T15:24:48.114077, step: 77, loss: 0.5154395699501038, acc: 0.7031, auc: 0.8351, precision: 0.6379, recall: 0.6852\n",
      "2018-12-28T15:24:48.519053, step: 78, loss: 0.4172019362449646, acc: 0.8203, auc: 0.8894, precision: 0.8605, recall: 0.6852\n",
      "2018-12-28T15:24:48.906285, step: 79, loss: 0.4511340260505676, acc: 0.7578, auc: 0.8901, precision: 0.931, recall: 0.4821\n",
      "2018-12-28T15:24:49.312656, step: 80, loss: 0.3747060298919678, acc: 0.8359, auc: 0.9162, precision: 0.902, recall: 0.7419\n",
      "2018-12-28T15:24:49.731005, step: 81, loss: 0.5098376274108887, acc: 0.75, auc: 0.8419, precision: 0.75, recall: 0.7258\n",
      "2018-12-28T15:24:50.161474, step: 82, loss: 0.42814165353775024, acc: 0.8047, auc: 0.8933, precision: 0.8182, recall: 0.806\n",
      "2018-12-28T15:24:50.554216, step: 83, loss: 0.378795325756073, acc: 0.8047, auc: 0.9181, precision: 0.8889, recall: 0.6667\n",
      "2018-12-28T15:24:50.978807, step: 84, loss: 0.3596128523349762, acc: 0.8125, auc: 0.9479, precision: 0.9796, recall: 0.6761\n",
      "2018-12-28T15:24:51.360639, step: 85, loss: 0.3919706344604492, acc: 0.8438, auc: 0.9108, precision: 0.8533, recall: 0.8767\n",
      "2018-12-28T15:24:51.782211, step: 86, loss: 0.4711957573890686, acc: 0.7812, auc: 0.8899, precision: 0.7403, recall: 0.8769\n",
      "2018-12-28T15:24:52.264196, step: 87, loss: 0.4467155337333679, acc: 0.7578, auc: 0.8745, precision: 0.7581, recall: 0.746\n",
      "2018-12-28T15:24:52.649437, step: 88, loss: 0.4617975354194641, acc: 0.7578, auc: 0.8911, precision: 0.9333, recall: 0.6\n",
      "2018-12-28T15:24:53.038168, step: 89, loss: 0.3234180510044098, acc: 0.8359, auc: 0.9609, precision: 0.9815, recall: 0.726\n",
      "2018-12-28T15:24:53.453876, step: 90, loss: 0.4555061459541321, acc: 0.7578, auc: 0.8674, precision: 0.7391, recall: 0.6415\n",
      "2018-12-28T15:24:53.844072, step: 91, loss: 0.47016143798828125, acc: 0.7969, auc: 0.8721, precision: 0.7838, recall: 0.8529\n",
      "2018-12-28T15:24:54.251888, step: 92, loss: 0.40039893984794617, acc: 0.8672, auc: 0.8981, precision: 0.8939, recall: 0.8551\n",
      "2018-12-28T15:24:54.680037, step: 93, loss: 0.3672146499156952, acc: 0.8281, auc: 0.9191, precision: 0.871, recall: 0.7941\n",
      "2018-12-28T15:24:55.095423, step: 94, loss: 0.34117770195007324, acc: 0.8047, auc: 0.9401, precision: 0.9375, recall: 0.6716\n",
      "2018-12-28T15:24:55.501401, step: 95, loss: 0.32097703218460083, acc: 0.8516, auc: 0.9514, precision: 0.95, recall: 0.7808\n",
      "2018-12-28T15:24:55.907081, step: 96, loss: 0.3492632508277893, acc: 0.875, auc: 0.935, precision: 0.8596, recall: 0.8596\n",
      "2018-12-28T15:24:56.330223, step: 97, loss: 0.44761401414871216, acc: 0.8125, auc: 0.8983, precision: 0.7647, recall: 0.8667\n",
      "2018-12-28T15:24:56.722374, step: 98, loss: 0.35324543714523315, acc: 0.8281, auc: 0.9272, precision: 0.8889, recall: 0.75\n",
      "2018-12-28T15:24:57.118759, step: 99, loss: 0.29062721133232117, acc: 0.875, auc: 0.9606, precision: 0.9667, recall: 0.8056\n",
      "2018-12-28T15:24:57.482169, step: 100, loss: 0.34292954206466675, acc: 0.8125, auc: 0.9372, precision: 0.8545, recall: 0.746\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T15:25:12.882948, step: 100, loss: 0.360078522249272, acc: 0.8371736842105264, auc: 0.9212552631578945, precision: 0.8726947368421052, recall: 0.7949315789473682\n",
      "2018-12-28T15:25:13.244244, step: 101, loss: 0.3305206298828125, acc: 0.8516, auc: 0.9342, precision: 0.8889, recall: 0.7869\n",
      "2018-12-28T15:25:13.663054, step: 102, loss: 0.4231894016265869, acc: 0.8125, auc: 0.8935, precision: 0.8276, recall: 0.7742\n",
      "2018-12-28T15:25:14.063353, step: 103, loss: 0.433459997177124, acc: 0.8438, auc: 0.9396, precision: 0.7463, recall: 0.9434\n",
      "2018-12-28T15:25:14.460619, step: 104, loss: 0.36580896377563477, acc: 0.8281, auc: 0.9275, precision: 0.9423, recall: 0.7206\n",
      "2018-12-28T15:25:14.873061, step: 105, loss: 0.3152594268321991, acc: 0.8438, auc: 0.9411, precision: 0.8393, recall: 0.8103\n",
      "2018-12-28T15:25:15.297532, step: 106, loss: 0.40075695514678955, acc: 0.7891, auc: 0.9286, precision: 0.9762, recall: 0.6119\n",
      "2018-12-28T15:25:15.707251, step: 107, loss: 0.3559727072715759, acc: 0.8203, auc: 0.9216, precision: 0.85, recall: 0.7846\n",
      "2018-12-28T15:25:16.082803, step: 108, loss: 0.37665751576423645, acc: 0.8359, auc: 0.9149, precision: 0.8485, recall: 0.8358\n",
      "2018-12-28T15:25:16.502737, step: 109, loss: 0.4407067596912384, acc: 0.8203, auc: 0.9087, precision: 0.75, recall: 0.9524\n",
      "2018-12-28T15:25:16.942409, step: 110, loss: 0.3603331446647644, acc: 0.8438, auc: 0.9241, precision: 0.8305, recall: 0.8305\n",
      "2018-12-28T15:25:17.354762, step: 111, loss: 0.3651047646999359, acc: 0.8047, auc: 0.9155, precision: 0.8548, recall: 0.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T15:25:17.800757, step: 112, loss: 0.3419433832168579, acc: 0.8203, auc: 0.9365, precision: 0.8936, recall: 0.7\n",
      "2018-12-28T15:25:18.230146, step: 113, loss: 0.3657776713371277, acc: 0.8203, auc: 0.9168, precision: 0.8793, recall: 0.7612\n",
      "2018-12-28T15:25:18.650496, step: 114, loss: 0.3790334165096283, acc: 0.8359, auc: 0.9054, precision: 0.8676, recall: 0.831\n",
      "2018-12-28T15:25:19.052069, step: 115, loss: 0.34116294980049133, acc: 0.8672, auc: 0.931, precision: 0.8571, recall: 0.8955\n",
      "2018-12-28T15:25:19.461934, step: 116, loss: 0.347822368144989, acc: 0.8672, auc: 0.9445, precision: 0.8143, recall: 0.9344\n",
      "2018-12-28T15:25:19.873570, step: 117, loss: 0.3734709918498993, acc: 0.8359, auc: 0.9153, precision: 0.8529, recall: 0.8406\n",
      "2018-12-28T15:25:20.287646, step: 118, loss: 0.33916595578193665, acc: 0.8594, auc: 0.9337, precision: 0.9231, recall: 0.8219\n",
      "2018-12-28T15:25:20.706575, step: 119, loss: 0.3645850121974945, acc: 0.7891, auc: 0.932, precision: 0.9211, recall: 0.5932\n",
      "2018-12-28T15:25:21.124191, step: 120, loss: 0.3158797025680542, acc: 0.8359, auc: 0.9484, precision: 0.9259, recall: 0.7463\n",
      "2018-12-28T15:25:21.487162, step: 121, loss: 0.38219550251960754, acc: 0.8438, auc: 0.9133, precision: 0.8421, recall: 0.8136\n",
      "2018-12-28T15:25:21.883732, step: 122, loss: 0.440786749124527, acc: 0.7812, auc: 0.8953, precision: 0.7286, recall: 0.85\n",
      "2018-12-28T15:25:22.294331, step: 123, loss: 0.30077558755874634, acc: 0.8828, auc: 0.941, precision: 0.8904, recall: 0.9028\n",
      "2018-12-28T15:25:22.731462, step: 124, loss: 0.35789164900779724, acc: 0.8672, auc: 0.9177, precision: 0.8966, recall: 0.8254\n",
      "2018-12-28T15:25:23.093969, step: 125, loss: 0.4172474145889282, acc: 0.8047, auc: 0.9008, precision: 0.902, recall: 0.697\n",
      "2018-12-28T15:25:23.472492, step: 126, loss: 0.33269235491752625, acc: 0.8125, auc: 0.937, precision: 0.8974, recall: 0.6364\n",
      "2018-12-28T15:25:23.894650, step: 127, loss: 0.2870734632015228, acc: 0.9062, auc: 0.9466, precision: 0.9762, recall: 0.7885\n",
      "2018-12-28T15:25:24.295324, step: 128, loss: 0.2856413722038269, acc: 0.8906, auc: 0.9569, precision: 0.9538, recall: 0.8493\n",
      "2018-12-28T15:25:24.721714, step: 129, loss: 0.38777947425842285, acc: 0.8281, auc: 0.9065, precision: 0.8361, recall: 0.8095\n",
      "2018-12-28T15:25:25.128813, step: 130, loss: 0.4595290422439575, acc: 0.7891, auc: 0.8727, precision: 0.7679, recall: 0.7544\n",
      "2018-12-28T15:25:25.527257, step: 131, loss: 0.31519997119903564, acc: 0.8594, auc: 0.9351, precision: 0.8833, recall: 0.8281\n",
      "2018-12-28T15:25:25.891992, step: 132, loss: 0.3476826548576355, acc: 0.875, auc: 0.9331, precision: 0.9444, recall: 0.7969\n",
      "2018-12-28T15:25:26.299908, step: 133, loss: 0.35399484634399414, acc: 0.8438, auc: 0.9245, precision: 0.9344, recall: 0.7808\n",
      "2018-12-28T15:25:26.695715, step: 134, loss: 0.30399376153945923, acc: 0.8906, auc: 0.9613, precision: 0.8393, recall: 0.9038\n",
      "2018-12-28T15:25:27.072657, step: 135, loss: 0.3847742974758148, acc: 0.8438, auc: 0.916, precision: 0.8571, recall: 0.8308\n",
      "2018-12-28T15:25:27.457197, step: 136, loss: 0.3937298059463501, acc: 0.8438, auc: 0.908, precision: 0.8852, recall: 0.806\n",
      "2018-12-28T15:25:27.853135, step: 137, loss: 0.4214145839214325, acc: 0.8047, auc: 0.9122, precision: 0.9608, recall: 0.6806\n",
      "2018-12-28T15:25:28.243625, step: 138, loss: 0.37181150913238525, acc: 0.8125, auc: 0.9176, precision: 0.8548, recall: 0.7794\n",
      "2018-12-28T15:25:28.635053, step: 139, loss: 0.3318117558956146, acc: 0.8516, auc: 0.936, precision: 0.8571, recall: 0.8438\n",
      "2018-12-28T15:25:29.047814, step: 140, loss: 0.3863697350025177, acc: 0.8281, auc: 0.9303, precision: 0.7541, recall: 0.8679\n",
      "2018-12-28T15:25:29.445687, step: 141, loss: 0.42586860060691833, acc: 0.7891, auc: 0.8906, precision: 0.8644, recall: 0.7286\n",
      "2018-12-28T15:25:29.852361, step: 142, loss: 0.31907224655151367, acc: 0.7969, auc: 0.948, precision: 0.92, recall: 0.6765\n",
      "2018-12-28T15:25:30.254119, step: 143, loss: 0.3175271451473236, acc: 0.8516, auc: 0.9447, precision: 0.88, recall: 0.7719\n",
      "2018-12-28T15:25:30.652383, step: 144, loss: 0.2638795077800751, acc: 0.8984, auc: 0.9634, precision: 0.9677, recall: 0.8451\n",
      "2018-12-28T15:25:31.030420, step: 145, loss: 0.3044984042644501, acc: 0.8828, auc: 0.9502, precision: 0.8889, recall: 0.875\n",
      "2018-12-28T15:25:31.420101, step: 146, loss: 0.31937167048454285, acc: 0.8906, auc: 0.9625, precision: 0.8382, recall: 0.95\n",
      "2018-12-28T15:25:31.817661, step: 147, loss: 0.30784153938293457, acc: 0.8672, auc: 0.9397, precision: 0.8704, recall: 0.8246\n",
      "2018-12-28T15:25:32.177418, step: 148, loss: 0.30896949768066406, acc: 0.875, auc: 0.9428, precision: 0.8833, recall: 0.8548\n",
      "2018-12-28T15:25:32.578915, step: 149, loss: 0.32704412937164307, acc: 0.8359, auc: 0.9402, precision: 0.9149, recall: 0.7167\n",
      "2018-12-28T15:25:32.984360, step: 150, loss: 0.2727673649787903, acc: 0.8203, auc: 0.9659, precision: 0.9474, recall: 0.6316\n",
      "2018-12-28T15:25:33.368416, step: 151, loss: 0.24275241792201996, acc: 0.8828, auc: 0.974, precision: 0.9821, recall: 0.7971\n",
      "2018-12-28T15:25:33.806623, step: 152, loss: 0.2838950455188751, acc: 0.8906, auc: 0.9521, precision: 0.9254, recall: 0.8732\n",
      "2018-12-28T15:25:34.208801, step: 153, loss: 0.37978896498680115, acc: 0.8594, auc: 0.9333, precision: 0.8226, recall: 0.8793\n",
      "2018-12-28T15:25:34.592470, step: 154, loss: 0.3040720224380493, acc: 0.875, auc: 0.9409, precision: 0.8793, recall: 0.85\n",
      "2018-12-28T15:25:34.975409, step: 155, loss: 0.2900758981704712, acc: 0.8516, auc: 0.9523, precision: 0.8615, recall: 0.8485\n",
      "start training model\n",
      "2018-12-28T15:25:35.445877, step: 156, loss: 0.2728818655014038, acc: 0.8438, auc: 0.9677, precision: 0.9565, recall: 0.7097\n",
      "2018-12-28T15:25:35.815949, step: 157, loss: 0.38588958978652954, acc: 0.8047, auc: 0.9157, precision: 0.8793, recall: 0.7391\n",
      "2018-12-28T15:25:36.211696, step: 158, loss: 0.29314613342285156, acc: 0.8828, auc: 0.9434, precision: 0.9375, recall: 0.7895\n",
      "2018-12-28T15:25:36.628062, step: 159, loss: 0.28604856133461, acc: 0.8906, auc: 0.9596, precision: 0.8438, recall: 0.931\n",
      "2018-12-28T15:25:37.064223, step: 160, loss: 0.23698988556861877, acc: 0.9141, auc: 0.9741, precision: 0.9206, recall: 0.9062\n",
      "2018-12-28T15:25:37.496548, step: 161, loss: 0.17820513248443604, acc: 0.9219, auc: 0.9852, precision: 0.9452, recall: 0.92\n",
      "2018-12-28T15:25:37.906662, step: 162, loss: 0.18935148417949677, acc: 0.9453, auc: 0.9705, precision: 0.9574, recall: 0.9\n",
      "2018-12-28T15:25:38.288348, step: 163, loss: 0.17999202013015747, acc: 0.9219, auc: 0.9819, precision: 1.0, recall: 0.8214\n",
      "2018-12-28T15:25:38.687144, step: 164, loss: 0.3049710988998413, acc: 0.8672, auc: 0.9491, precision: 0.94, recall: 0.7705\n",
      "2018-12-28T15:25:39.082446, step: 165, loss: 0.2428823560476303, acc: 0.9062, auc: 0.9616, precision: 0.9032, recall: 0.9032\n",
      "2018-12-28T15:25:39.490027, step: 166, loss: 0.20933400094509125, acc: 0.8984, auc: 0.9741, precision: 0.9167, recall: 0.873\n",
      "2018-12-28T15:25:39.875018, step: 167, loss: 0.3007056713104248, acc: 0.875, auc: 0.9431, precision: 0.875, recall: 0.875\n",
      "2018-12-28T15:25:40.256532, step: 168, loss: 0.3629894256591797, acc: 0.8516, auc: 0.9319, precision: 0.8113, recall: 0.8269\n",
      "2018-12-28T15:25:40.658716, step: 169, loss: 0.21285271644592285, acc: 0.8828, auc: 0.9753, precision: 0.8923, recall: 0.8788\n",
      "2018-12-28T15:25:41.045646, step: 170, loss: 0.15880711376667023, acc: 0.9141, auc: 0.9868, precision: 0.9661, recall: 0.8636\n",
      "2018-12-28T15:25:41.414126, step: 171, loss: 0.25142452120780945, acc: 0.9219, auc: 0.9593, precision: 0.9464, recall: 0.8833\n",
      "2018-12-28T15:25:41.827634, step: 172, loss: 0.268789142370224, acc: 0.8359, auc: 0.966, precision: 0.9649, recall: 0.7432\n",
      "2018-12-28T15:25:42.271882, step: 173, loss: 0.2128967046737671, acc: 0.9141, auc: 0.9656, precision: 0.9559, recall: 0.8904\n",
      "2018-12-28T15:25:42.673949, step: 174, loss: 0.26575347781181335, acc: 0.9141, auc: 0.9576, precision: 0.9275, recall: 0.9143\n",
      "2018-12-28T15:25:43.047576, step: 175, loss: 0.30656275153160095, acc: 0.8594, auc: 0.9565, precision: 0.8406, recall: 0.8923\n",
      "2018-12-28T15:25:43.455309, step: 176, loss: 0.1966264545917511, acc: 0.8828, auc: 0.9774, precision: 0.9219, recall: 0.8551\n",
      "2018-12-28T15:25:43.900617, step: 177, loss: 0.18199846148490906, acc: 0.9297, auc: 0.983, precision: 0.9574, recall: 0.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T15:25:44.318833, step: 178, loss: 0.2414558082818985, acc: 0.8906, auc: 0.9754, precision: 1.0, recall: 0.8\n",
      "2018-12-28T15:25:44.742511, step: 179, loss: 0.20628520846366882, acc: 0.8672, auc: 0.9885, precision: 0.9783, recall: 0.7377\n",
      "2018-12-28T15:25:45.126694, step: 180, loss: 0.28671014308929443, acc: 0.8906, auc: 0.949, precision: 0.8889, recall: 0.8889\n",
      "2018-12-28T15:25:45.538145, step: 181, loss: 0.27268320322036743, acc: 0.9219, auc: 0.9761, precision: 0.8784, recall: 0.9848\n",
      "2018-12-28T15:25:45.925862, step: 182, loss: 0.28477174043655396, acc: 0.8828, auc: 0.9516, precision: 0.8923, recall: 0.8788\n",
      "2018-12-28T15:25:46.330346, step: 183, loss: 0.1978938728570938, acc: 0.9141, auc: 0.9766, precision: 0.9219, recall: 0.9077\n",
      "2018-12-28T15:25:46.716600, step: 184, loss: 0.3135903775691986, acc: 0.8594, auc: 0.9587, precision: 0.9672, recall: 0.7867\n",
      "2018-12-28T15:25:47.096973, step: 185, loss: 0.2495298683643341, acc: 0.9062, auc: 0.9604, precision: 0.9322, recall: 0.873\n",
      "2018-12-28T15:25:47.526026, step: 186, loss: 0.20786035060882568, acc: 0.9453, auc: 0.9736, precision: 0.9831, recall: 0.9062\n",
      "2018-12-28T15:25:47.928790, step: 187, loss: 0.27095305919647217, acc: 0.8984, auc: 0.956, precision: 0.9333, recall: 0.8615\n",
      "2018-12-28T15:25:48.348387, step: 188, loss: 0.22303727269172668, acc: 0.9297, auc: 0.9696, precision: 0.9403, recall: 0.9265\n",
      "2018-12-28T15:25:48.723923, step: 189, loss: 0.23195037245750427, acc: 0.9062, auc: 0.9659, precision: 0.9516, recall: 0.8676\n",
      "2018-12-28T15:25:49.137191, step: 190, loss: 0.2510510981082916, acc: 0.9297, auc: 0.9659, precision: 0.9444, recall: 0.9315\n",
      "2018-12-28T15:25:49.534124, step: 191, loss: 0.23116961121559143, acc: 0.9219, auc: 0.9632, precision: 0.963, recall: 0.8667\n",
      "2018-12-28T15:25:49.944700, step: 192, loss: 0.15373706817626953, acc: 0.9297, auc: 0.9892, precision: 0.9531, recall: 0.9104\n",
      "2018-12-28T15:25:50.369904, step: 193, loss: 0.25748851895332336, acc: 0.875, auc: 0.9637, precision: 0.9062, recall: 0.8529\n",
      "2018-12-28T15:25:50.770418, step: 194, loss: 0.1879611611366272, acc: 0.9141, auc: 0.979, precision: 0.9655, recall: 0.8615\n",
      "2018-12-28T15:25:51.190123, step: 195, loss: 0.22025522589683533, acc: 0.9297, auc: 0.9714, precision: 0.9219, recall: 0.9365\n",
      "2018-12-28T15:25:51.561644, step: 196, loss: 0.21758435666561127, acc: 0.9297, auc: 0.9745, precision: 0.9412, recall: 0.8889\n",
      "2018-12-28T15:25:51.979451, step: 197, loss: 0.33065083622932434, acc: 0.875, auc: 0.9399, precision: 0.8906, recall: 0.8636\n",
      "2018-12-28T15:25:52.389465, step: 198, loss: 0.23380157351493835, acc: 0.9141, auc: 0.9653, precision: 0.9375, recall: 0.8955\n",
      "2018-12-28T15:25:52.809802, step: 199, loss: 0.30135664343833923, acc: 0.9141, auc: 0.9568, precision: 0.871, recall: 0.9474\n",
      "2018-12-28T15:25:53.220842, step: 200, loss: 0.29329776763916016, acc: 0.8906, auc: 0.9498, precision: 0.9057, recall: 0.8421\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T15:26:08.843020, step: 200, loss: 0.31665946935352524, acc: 0.8610289473684212, auc: 0.9429421052631577, precision: 0.9122631578947368, recall: 0.8031736842105264\n",
      "2018-12-28T15:26:09.253097, step: 201, loss: 0.22942951321601868, acc: 0.9141, auc: 0.9699, precision: 0.9825, recall: 0.8485\n",
      "2018-12-28T15:26:09.694173, step: 202, loss: 0.2706284523010254, acc: 0.8984, auc: 0.9548, precision: 0.9464, recall: 0.8413\n",
      "2018-12-28T15:26:10.078222, step: 203, loss: 0.22030770778656006, acc: 0.9297, auc: 0.9707, precision: 0.9434, recall: 0.8929\n",
      "2018-12-28T15:26:10.462705, step: 204, loss: 0.22725486755371094, acc: 0.8984, auc: 0.97, precision: 0.9118, recall: 0.8986\n",
      "2018-12-28T15:26:10.856441, step: 205, loss: 0.2534458637237549, acc: 0.8906, auc: 0.9714, precision: 0.8696, recall: 0.9231\n",
      "2018-12-28T15:26:11.249044, step: 206, loss: 0.21318209171295166, acc: 0.9219, auc: 0.9702, precision: 0.9286, recall: 0.9286\n",
      "2018-12-28T15:26:11.655439, step: 207, loss: 0.2667127549648285, acc: 0.875, auc: 0.9576, precision: 0.9, recall: 0.8036\n",
      "2018-12-28T15:26:12.047605, step: 208, loss: 0.2428959608078003, acc: 0.9141, auc: 0.9597, precision: 0.9474, recall: 0.871\n",
      "2018-12-28T15:26:12.454813, step: 209, loss: 0.22801268100738525, acc: 0.8828, auc: 0.9761, precision: 0.9444, recall: 0.8095\n",
      "2018-12-28T15:26:12.877429, step: 210, loss: 0.20800378918647766, acc: 0.8984, auc: 0.9813, precision: 0.9672, recall: 0.8429\n",
      "2018-12-28T15:26:13.271830, step: 211, loss: 0.27626821398735046, acc: 0.9141, auc: 0.9497, precision: 0.9077, recall: 0.9219\n",
      "2018-12-28T15:26:13.689731, step: 212, loss: 0.16297665238380432, acc: 0.9609, auc: 0.9897, precision: 0.9667, recall: 0.9508\n",
      "2018-12-28T15:26:14.109057, step: 213, loss: 0.21856024861335754, acc: 0.9062, auc: 0.9705, precision: 0.9062, recall: 0.9062\n",
      "2018-12-28T15:26:14.569174, step: 214, loss: 0.21284246444702148, acc: 0.9219, auc: 0.971, precision: 0.9155, recall: 0.942\n",
      "2018-12-28T15:26:14.981356, step: 215, loss: 0.1922527253627777, acc: 0.9219, auc: 0.9785, precision: 0.9636, recall: 0.8689\n",
      "2018-12-28T15:26:15.388027, step: 216, loss: 0.23721955716609955, acc: 0.8984, auc: 0.9687, precision: 0.8727, recall: 0.8889\n",
      "2018-12-28T15:26:15.782972, step: 217, loss: 0.1764974147081375, acc: 0.9219, auc: 0.9848, precision: 0.9683, recall: 0.8841\n",
      "2018-12-28T15:26:16.230534, step: 218, loss: 0.17745107412338257, acc: 0.9219, auc: 0.9819, precision: 0.95, recall: 0.8906\n",
      "2018-12-28T15:26:16.658429, step: 219, loss: 0.1626831591129303, acc: 0.9375, auc: 0.9811, precision: 0.9545, recall: 0.9265\n",
      "2018-12-28T15:26:17.068316, step: 220, loss: 0.2331189513206482, acc: 0.8984, auc: 0.9733, precision: 0.9138, recall: 0.8689\n",
      "2018-12-28T15:26:17.480332, step: 221, loss: 0.2032385766506195, acc: 0.9219, auc: 0.9778, precision: 0.9219, recall: 0.9219\n",
      "2018-12-28T15:26:17.925328, step: 222, loss: 0.22681434452533722, acc: 0.9141, auc: 0.9725, precision: 0.913, recall: 0.9265\n",
      "2018-12-28T15:26:18.317008, step: 223, loss: 0.2825656533241272, acc: 0.8828, auc: 0.9553, precision: 0.9032, recall: 0.8615\n",
      "2018-12-28T15:26:18.716739, step: 224, loss: 0.295528769493103, acc: 0.8828, auc: 0.9473, precision: 0.9286, recall: 0.8254\n",
      "2018-12-28T15:26:19.106106, step: 225, loss: 0.2037322223186493, acc: 0.8906, auc: 0.9795, precision: 0.9062, recall: 0.8788\n",
      "2018-12-28T15:26:19.514805, step: 226, loss: 0.2673657536506653, acc: 0.8906, auc: 0.9632, precision: 0.95, recall: 0.8382\n",
      "2018-12-28T15:26:19.942589, step: 227, loss: 0.26616647839546204, acc: 0.8672, auc: 0.9492, precision: 0.8667, recall: 0.78\n",
      "2018-12-28T15:26:20.307669, step: 228, loss: 0.1765371561050415, acc: 0.9219, auc: 0.9888, precision: 1.0, recall: 0.863\n",
      "2018-12-28T15:26:20.675021, step: 229, loss: 0.24737901985645294, acc: 0.9062, auc: 0.9604, precision: 0.9296, recall: 0.9041\n",
      "2018-12-28T15:26:21.097249, step: 230, loss: 0.34309250116348267, acc: 0.8672, auc: 0.9409, precision: 0.8592, recall: 0.8971\n",
      "2018-12-28T15:26:21.522579, step: 231, loss: 0.24717538058757782, acc: 0.9062, auc: 0.9716, precision: 0.8551, recall: 0.9672\n",
      "2018-12-28T15:26:21.943404, step: 232, loss: 0.23471760749816895, acc: 0.9141, auc: 0.9681, precision: 0.9184, recall: 0.8654\n",
      "2018-12-28T15:26:22.335440, step: 233, loss: 0.20127356052398682, acc: 0.9297, auc: 0.9731, precision: 0.9365, recall: 0.9219\n",
      "2018-12-28T15:26:22.759723, step: 234, loss: 0.3940321207046509, acc: 0.8359, auc: 0.9583, precision: 1.0, recall: 0.6912\n",
      "2018-12-28T15:26:23.192381, step: 235, loss: 0.22455179691314697, acc: 0.9062, auc: 0.9758, precision: 1.0, recall: 0.8065\n",
      "2018-12-28T15:26:23.619605, step: 236, loss: 0.19655418395996094, acc: 0.9297, auc: 0.9817, precision: 0.9242, recall: 0.9385\n",
      "2018-12-28T15:26:24.068902, step: 237, loss: 0.16817930340766907, acc: 0.9531, auc: 0.9921, precision: 0.9333, recall: 0.9655\n",
      "2018-12-28T15:26:24.539467, step: 238, loss: 0.2853109538555145, acc: 0.8984, auc: 0.9609, precision: 0.8732, recall: 0.9394\n",
      "2018-12-28T15:26:24.991841, step: 239, loss: 0.2268095314502716, acc: 0.9141, auc: 0.9732, precision: 0.8852, recall: 0.931\n",
      "2018-12-28T15:26:25.415360, step: 240, loss: 0.17468340694904327, acc: 0.9219, auc: 0.9867, precision: 0.9688, recall: 0.8857\n",
      "2018-12-28T15:26:25.840986, step: 241, loss: 0.271491676568985, acc: 0.8281, auc: 0.957, precision: 0.8723, recall: 0.7193\n",
      "2018-12-28T15:26:26.281547, step: 242, loss: 0.33730071783065796, acc: 0.8047, auc: 0.9568, precision: 0.9524, recall: 0.6349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T15:26:26.681223, step: 243, loss: 0.23175503313541412, acc: 0.8906, auc: 0.9714, precision: 0.9444, recall: 0.8226\n",
      "2018-12-28T15:26:27.041784, step: 244, loss: 0.23654238879680634, acc: 0.9062, auc: 0.9684, precision: 0.9153, recall: 0.8852\n",
      "2018-12-28T15:26:27.413297, step: 245, loss: 0.23612883687019348, acc: 0.9141, auc: 0.967, precision: 0.9155, recall: 0.9286\n",
      "2018-12-28T15:26:27.808219, step: 246, loss: 0.271047979593277, acc: 0.8984, auc: 0.9563, precision: 0.9077, recall: 0.8939\n",
      "2018-12-28T15:26:28.208321, step: 247, loss: 0.27771487832069397, acc: 0.8672, auc: 0.9747, precision: 0.8333, recall: 0.942\n",
      "2018-12-28T15:26:28.596031, step: 248, loss: 0.20502343773841858, acc: 0.9531, auc: 0.9772, precision: 0.9219, recall: 0.9833\n",
      "2018-12-28T15:26:28.989689, step: 249, loss: 0.176017627120018, acc: 0.9609, auc: 0.9885, precision: 1.0, recall: 0.9206\n",
      "2018-12-28T15:26:29.420379, step: 250, loss: 0.2479214072227478, acc: 0.8906, auc: 0.9617, precision: 0.9545, recall: 0.7778\n",
      "2018-12-28T15:26:29.815648, step: 251, loss: 0.21420985460281372, acc: 0.8984, auc: 0.9766, precision: 0.9592, recall: 0.8103\n",
      "2018-12-28T15:26:30.190061, step: 252, loss: 0.25595012307167053, acc: 0.8984, auc: 0.9654, precision: 0.9583, recall: 0.807\n",
      "2018-12-28T15:26:30.585669, step: 253, loss: 0.230096235871315, acc: 0.8906, auc: 0.9722, precision: 0.9298, recall: 0.8413\n",
      "2018-12-28T15:26:30.984754, step: 254, loss: 0.260579913854599, acc: 0.8828, auc: 0.9575, precision: 0.9455, recall: 0.8125\n",
      "2018-12-28T15:26:31.396538, step: 255, loss: 0.3549659252166748, acc: 0.8516, auc: 0.9402, precision: 0.7903, recall: 0.8909\n",
      "2018-12-28T15:26:31.783532, step: 256, loss: 0.2795315682888031, acc: 0.8516, auc: 0.9553, precision: 0.8448, recall: 0.8305\n",
      "2018-12-28T15:26:32.155623, step: 257, loss: 0.1531449556350708, acc: 0.9453, auc: 0.9898, precision: 0.9298, recall: 0.9464\n",
      "2018-12-28T15:26:32.541828, step: 258, loss: 0.34677255153656006, acc: 0.8594, auc: 0.9264, precision: 0.898, recall: 0.7719\n",
      "2018-12-28T15:26:32.949945, step: 259, loss: 0.25109589099884033, acc: 0.9062, auc: 0.9631, precision: 0.9344, recall: 0.8769\n",
      "2018-12-28T15:26:33.315040, step: 260, loss: 0.2280106395483017, acc: 0.9219, auc: 0.967, precision: 0.918, recall: 0.918\n",
      "2018-12-28T15:26:33.711347, step: 261, loss: 0.20479895174503326, acc: 0.9141, auc: 0.9753, precision: 0.9333, recall: 0.8889\n",
      "2018-12-28T15:26:34.133658, step: 262, loss: 0.21691489219665527, acc: 0.8984, auc: 0.9706, precision: 0.9123, recall: 0.8667\n",
      "2018-12-28T15:26:34.539737, step: 263, loss: 0.2058977335691452, acc: 0.9062, auc: 0.9795, precision: 0.9844, recall: 0.8514\n",
      "2018-12-28T15:26:34.928457, step: 264, loss: 0.32289689779281616, acc: 0.8828, auc: 0.9502, precision: 0.8413, recall: 0.9138\n",
      "2018-12-28T15:26:35.367655, step: 265, loss: 0.2553826570510864, acc: 0.8906, auc: 0.9678, precision: 0.8696, recall: 0.9231\n",
      "2018-12-28T15:26:35.751913, step: 266, loss: 0.24552980065345764, acc: 0.8984, auc: 0.967, precision: 0.9138, recall: 0.8689\n",
      "2018-12-28T15:26:36.119900, step: 267, loss: 0.19625762104988098, acc: 0.9453, auc: 0.9741, precision: 0.9701, recall: 0.9286\n",
      "2018-12-28T15:26:36.528372, step: 268, loss: 0.1608888804912567, acc: 0.9062, auc: 0.9867, precision: 0.9155, recall: 0.9155\n",
      "2018-12-28T15:26:36.940067, step: 269, loss: 0.3043985068798065, acc: 0.875, auc: 0.9497, precision: 0.9444, recall: 0.7969\n",
      "2018-12-28T15:26:37.342312, step: 270, loss: 0.18339963257312775, acc: 0.9062, auc: 0.9834, precision: 0.9048, recall: 0.9048\n",
      "2018-12-28T15:26:37.753975, step: 271, loss: 0.15548869967460632, acc: 0.9609, auc: 0.9849, precision: 0.9853, recall: 0.9437\n",
      "2018-12-28T15:26:38.170990, step: 272, loss: 0.2362949550151825, acc: 0.9062, auc: 0.9626, precision: 0.9483, recall: 0.8594\n",
      "2018-12-28T15:26:38.586997, step: 273, loss: 0.2614782452583313, acc: 0.875, auc: 0.9585, precision: 0.9344, recall: 0.8261\n",
      "2018-12-28T15:26:39.004200, step: 274, loss: 0.216017946600914, acc: 0.9219, auc: 0.9692, precision: 0.9231, recall: 0.9231\n",
      "2018-12-28T15:26:39.457068, step: 275, loss: 0.2309378683567047, acc: 0.9219, auc: 0.968, precision: 0.8971, recall: 0.9531\n",
      "2018-12-28T15:26:39.850066, step: 276, loss: 0.23466840386390686, acc: 0.9062, auc: 0.9696, precision: 0.9, recall: 0.9265\n",
      "2018-12-28T15:26:40.236844, step: 277, loss: 0.20186136662960052, acc: 0.9297, auc: 0.979, precision: 0.9821, recall: 0.873\n",
      "2018-12-28T15:26:40.676639, step: 278, loss: 0.2137281894683838, acc: 0.875, auc: 0.9739, precision: 0.9259, recall: 0.8065\n",
      "2018-12-28T15:26:41.094808, step: 279, loss: 0.22758834064006805, acc: 0.9141, auc: 0.9734, precision: 0.9818, recall: 0.8438\n",
      "2018-12-28T15:26:41.568091, step: 280, loss: 0.2178594321012497, acc: 0.9062, auc: 0.9714, precision: 0.8939, recall: 0.9219\n",
      "2018-12-28T15:26:41.996978, step: 281, loss: 0.1839400678873062, acc: 0.9375, auc: 0.9819, precision: 0.9138, recall: 0.9464\n",
      "2018-12-28T15:26:42.401533, step: 282, loss: 0.2725558876991272, acc: 0.8984, auc: 0.9634, precision: 0.9545, recall: 0.863\n",
      "2018-12-28T15:26:42.782357, step: 283, loss: 0.31774306297302246, acc: 0.8594, auc: 0.9438, precision: 0.873, recall: 0.8462\n",
      "2018-12-28T15:26:43.190863, step: 284, loss: 0.29452890157699585, acc: 0.9062, auc: 0.9482, precision: 0.9524, recall: 0.8696\n",
      "2018-12-28T15:26:43.570732, step: 285, loss: 0.23358847200870514, acc: 0.9141, auc: 0.9658, precision: 0.9219, recall: 0.9077\n",
      "2018-12-28T15:26:43.963216, step: 286, loss: 0.19487431645393372, acc: 0.9531, auc: 0.9739, precision: 0.9545, recall: 0.9545\n",
      "2018-12-28T15:26:44.364268, step: 287, loss: 0.1843845546245575, acc: 0.9297, auc: 0.9858, precision: 0.9048, recall: 0.95\n",
      "2018-12-28T15:26:44.755082, step: 288, loss: 0.26132217049598694, acc: 0.8906, auc: 0.9594, precision: 0.9333, recall: 0.8485\n",
      "2018-12-28T15:26:45.165173, step: 289, loss: 0.2853809893131256, acc: 0.8594, auc: 0.9528, precision: 0.9231, recall: 0.7742\n",
      "2018-12-28T15:26:45.592779, step: 290, loss: 0.15758228302001953, acc: 0.9453, auc: 0.988, precision: 0.9836, recall: 0.9091\n",
      "2018-12-28T15:26:46.029536, step: 291, loss: 0.25259819626808167, acc: 0.8828, auc: 0.9594, precision: 0.9444, recall: 0.8608\n",
      "2018-12-28T15:26:46.459360, step: 292, loss: 0.31834477186203003, acc: 0.8594, auc: 0.9409, precision: 0.8571, recall: 0.8571\n",
      "2018-12-28T15:26:46.854471, step: 293, loss: 0.2701267600059509, acc: 0.9062, auc: 0.9686, precision: 0.8596, recall: 0.9245\n",
      "2018-12-28T15:26:47.249081, step: 294, loss: 0.1673143059015274, acc: 0.9609, auc: 0.9751, precision: 0.9474, recall: 0.9863\n",
      "2018-12-28T15:26:47.693522, step: 295, loss: 0.23685425519943237, acc: 0.9141, auc: 0.9677, precision: 0.9104, recall: 0.9242\n",
      "2018-12-28T15:26:48.109548, step: 296, loss: 0.2631651759147644, acc: 0.875, auc: 0.9587, precision: 0.9153, recall: 0.8308\n",
      "2018-12-28T15:26:48.515003, step: 297, loss: 0.12421596795320511, acc: 0.9688, auc: 0.998, precision: 0.9831, recall: 0.9508\n",
      "2018-12-28T15:26:48.934455, step: 298, loss: 0.2935454249382019, acc: 0.8203, auc: 0.9566, precision: 0.9245, recall: 0.7206\n",
      "2018-12-28T15:26:49.347869, step: 299, loss: 0.25865262746810913, acc: 0.9219, auc: 0.9604, precision: 0.9661, recall: 0.8769\n",
      "2018-12-28T15:26:49.746656, step: 300, loss: 0.21611979603767395, acc: 0.875, auc: 0.9695, precision: 0.8889, recall: 0.8615\n",
      "\n",
      "Evaluation:\n",
      "2018-12-28T15:27:05.182466, step: 300, loss: 0.30874444661956085, acc: 0.8731368421052629, auc: 0.9458894736842106, precision: 0.8754578947368422, recall: 0.8756394736842106\n",
      "2018-12-28T15:27:05.571860, step: 301, loss: 0.21405473351478577, acc: 0.9141, auc: 0.9756, precision: 0.8986, recall: 0.9394\n",
      "2018-12-28T15:27:05.964116, step: 302, loss: 0.2577652335166931, acc: 0.8984, auc: 0.9707, precision: 0.8592, recall: 0.9531\n",
      "2018-12-28T15:27:06.382999, step: 303, loss: 0.2528302073478699, acc: 0.8984, auc: 0.9582, precision: 0.9167, recall: 0.873\n",
      "2018-12-28T15:27:06.810793, step: 304, loss: 0.3810296356678009, acc: 0.8438, auc: 0.9284, precision: 0.9, recall: 0.7941\n",
      "2018-12-28T15:27:07.206503, step: 305, loss: 0.29481804370880127, acc: 0.9062, auc: 0.9444, precision: 0.9362, recall: 0.8302\n",
      "2018-12-28T15:27:07.631841, step: 306, loss: 0.22121623158454895, acc: 0.9141, auc: 0.9692, precision: 0.9344, recall: 0.8906\n",
      "2018-12-28T15:27:08.040281, step: 307, loss: 0.22531725466251373, acc: 0.9141, auc: 0.968, precision: 0.8939, recall: 0.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-28T15:27:08.447424, step: 308, loss: 0.2206498682498932, acc: 0.9062, auc: 0.9734, precision: 0.9464, recall: 0.8548\n",
      "2018-12-28T15:27:08.861487, step: 309, loss: 0.23141241073608398, acc: 0.9062, auc: 0.9717, precision: 0.9636, recall: 0.8413\n",
      "2018-12-28T15:27:09.293316, step: 310, loss: 0.26841971278190613, acc: 0.8828, auc: 0.9547, precision: 0.8966, recall: 0.8525\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
