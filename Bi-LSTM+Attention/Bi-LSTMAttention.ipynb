{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 2\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            fwHiddenLayers = []\n",
    "            bwHiddenLayers = []\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "                fwHiddenLayers.append(lstmFwCell)\n",
    "                bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "            # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "            fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "            bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/hist is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0/grad/sparsity is illegal; using bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to /data4T/share/jiangxinyang848/textClassifier/Bi-LSTM+Attention/summarys\n",
      "\n",
      "start training model\n",
      "2019-01-02T16:35:57.373034, step: 1, loss: 0.6870827078819275, acc: 0.4766, auc: 0.6056, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:57.790741, step: 2, loss: 0.6916437745094299, acc: 0.4688, auc: 0.475, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:58.200782, step: 3, loss: 0.6728069186210632, acc: 0.4609, auc: 0.7175, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:58.600196, step: 4, loss: 0.6881740093231201, acc: 0.5234, auc: 0.6337, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:58.969352, step: 5, loss: 0.6829627752304077, acc: 0.5547, auc: 0.6419, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:59.328716, step: 6, loss: 0.6703990697860718, acc: 0.5, auc: 0.6794, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:35:59.730146, step: 7, loss: 0.6658670902252197, acc: 0.4922, auc: 0.7326, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:36:00.138000, step: 8, loss: 0.6618174314498901, acc: 0.5312, auc: 0.6988, precision: 1.0, recall: 0.0164\n",
      "2019-01-02T16:36:00.532159, step: 9, loss: 0.6450107097625732, acc: 0.4922, auc: 0.7819, precision: 0.0, recall: 0.0\n",
      "2019-01-02T16:36:00.965855, step: 10, loss: 0.6223022937774658, acc: 0.5547, auc: 0.8346, precision: 0.8, recall: 0.0667\n",
      "2019-01-02T16:36:01.373461, step: 11, loss: 0.6230171322822571, acc: 0.6328, auc: 0.7713, precision: 0.7333, recall: 0.2037\n",
      "2019-01-02T16:36:01.780235, step: 12, loss: 0.6189671158790588, acc: 0.5312, auc: 0.7783, precision: 1.0, recall: 0.1781\n",
      "2019-01-02T16:36:02.201199, step: 13, loss: 0.6245630383491516, acc: 0.5703, auc: 0.7309, precision: 0.7308, recall: 0.2836\n",
      "2019-01-02T16:36:02.599527, step: 14, loss: 0.5895833373069763, acc: 0.7734, auc: 0.7988, precision: 0.7627, recall: 0.75\n",
      "2019-01-02T16:36:03.032849, step: 15, loss: 0.5119653940200806, acc: 0.6797, auc: 0.8245, precision: 0.7872, recall: 0.5441\n",
      "2019-01-02T16:36:03.444971, step: 16, loss: 0.5412463545799255, acc: 0.7266, auc: 0.8375, precision: 0.9211, recall: 0.5224\n",
      "2019-01-02T16:36:03.851501, step: 17, loss: 0.5087888240814209, acc: 0.7734, auc: 0.8473, precision: 0.7627, recall: 0.75\n",
      "2019-01-02T16:36:04.230851, step: 18, loss: 0.5155947208404541, acc: 0.75, auc: 0.8439, precision: 0.7612, recall: 0.7612\n",
      "2019-01-02T16:36:04.651344, step: 19, loss: 0.4533039331436157, acc: 0.8281, auc: 0.8682, precision: 0.8, recall: 0.8\n",
      "2019-01-02T16:36:05.082314, step: 20, loss: 0.586825430393219, acc: 0.6641, auc: 0.8256, precision: 0.9, recall: 0.3051\n",
      "2019-01-02T16:36:05.482974, step: 21, loss: 0.49771755933761597, acc: 0.6641, auc: 0.8792, precision: 0.8889, recall: 0.375\n",
      "2019-01-02T16:36:05.888458, step: 22, loss: 0.5159816741943359, acc: 0.7578, auc: 0.8327, precision: 0.8167, recall: 0.7101\n",
      "2019-01-02T16:36:06.309202, step: 23, loss: 0.5669493675231934, acc: 0.7734, auc: 0.8685, precision: 0.6933, recall: 0.8966\n",
      "2019-01-02T16:36:06.686722, step: 24, loss: 0.5305500030517578, acc: 0.8203, auc: 0.8888, precision: 0.7273, recall: 0.9057\n",
      "2019-01-02T16:36:07.078837, step: 25, loss: 0.4852444529533386, acc: 0.7891, auc: 0.8907, precision: 0.8235, recall: 0.7\n",
      "2019-01-02T16:36:07.483809, step: 26, loss: 0.4717942476272583, acc: 0.7891, auc: 0.8718, precision: 0.9143, recall: 0.5714\n",
      "2019-01-02T16:36:07.920387, step: 27, loss: 0.49056166410446167, acc: 0.7109, auc: 0.8936, precision: 1.0, recall: 0.3833\n",
      "2019-01-02T16:36:08.360610, step: 28, loss: 0.4924152195453644, acc: 0.6953, auc: 0.907, precision: 0.963, recall: 0.4062\n",
      "2019-01-02T16:36:08.772216, step: 29, loss: 0.48414236307144165, acc: 0.7266, auc: 0.8889, precision: 0.9524, recall: 0.5479\n",
      "2019-01-02T16:36:09.178072, step: 30, loss: 0.4233477711677551, acc: 0.8438, auc: 0.9116, precision: 0.8909, recall: 0.7778\n",
      "2019-01-02T16:36:09.596712, step: 31, loss: 0.4669387936592102, acc: 0.8047, auc: 0.9165, precision: 0.75, recall: 0.9231\n",
      "2019-01-02T16:36:09.992861, step: 32, loss: 0.5157020688056946, acc: 0.8281, auc: 0.8857, precision: 0.7848, recall: 0.9254\n",
      "2019-01-02T16:36:10.410685, step: 33, loss: 0.4786358177661896, acc: 0.7734, auc: 0.8911, precision: 0.7576, recall: 0.7937\n",
      "2019-01-02T16:36:10.815828, step: 34, loss: 0.39478111267089844, acc: 0.8359, auc: 0.9093, precision: 0.8727, recall: 0.7742\n",
      "2019-01-02T16:36:11.216758, step: 35, loss: 0.38633644580841064, acc: 0.8359, auc: 0.9165, precision: 0.9773, recall: 0.6825\n",
      "2019-01-02T16:36:11.595654, step: 36, loss: 0.4216820299625397, acc: 0.7656, auc: 0.8997, precision: 0.8621, recall: 0.4902\n",
      "2019-01-02T16:36:11.988513, step: 37, loss: 0.4447094202041626, acc: 0.7891, auc: 0.8889, precision: 0.8542, recall: 0.6721\n",
      "2019-01-02T16:36:12.389768, step: 38, loss: 0.4756092429161072, acc: 0.7812, auc: 0.865, precision: 0.7812, recall: 0.7812\n",
      "2019-01-02T16:36:12.769769, step: 39, loss: 0.340606689453125, acc: 0.8516, auc: 0.9328, precision: 0.8214, recall: 0.8364\n",
      "2019-01-02T16:36:13.155408, step: 40, loss: 0.4491211175918579, acc: 0.7578, auc: 0.8785, precision: 0.8793, recall: 0.68\n",
      "2019-01-02T16:36:13.535507, step: 41, loss: 0.3571316599845886, acc: 0.8438, auc: 0.9311, precision: 0.9348, recall: 0.7167\n",
      "2019-01-02T16:36:13.945293, step: 42, loss: 0.444293737411499, acc: 0.7891, auc: 0.8799, precision: 0.8036, recall: 0.7377\n",
      "2019-01-02T16:36:14.327798, step: 43, loss: 0.33370649814605713, acc: 0.9062, auc: 0.9355, precision: 0.9344, recall: 0.8769\n",
      "2019-01-02T16:36:14.725783, step: 44, loss: 0.3306701183319092, acc: 0.8516, auc: 0.9372, precision: 0.8507, recall: 0.8636\n",
      "2019-01-02T16:36:15.117650, step: 45, loss: 0.4297947883605957, acc: 0.8359, auc: 0.8887, precision: 0.8958, recall: 0.7288\n",
      "2019-01-02T16:36:15.502008, step: 46, loss: 0.4429723918437958, acc: 0.8203, auc: 0.898, precision: 0.9111, recall: 0.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:36:15.902311, step: 47, loss: 0.423703134059906, acc: 0.8359, auc: 0.9013, precision: 0.7794, recall: 0.8983\n",
      "2019-01-02T16:36:16.299620, step: 48, loss: 0.3735532760620117, acc: 0.8438, auc: 0.9177, precision: 0.8906, recall: 0.8143\n",
      "2019-01-02T16:36:16.686492, step: 49, loss: 0.4724326729774475, acc: 0.7969, auc: 0.9242, precision: 0.7, recall: 0.9074\n",
      "2019-01-02T16:36:17.047067, step: 50, loss: 0.4178544282913208, acc: 0.8125, auc: 0.9034, precision: 0.8036, recall: 0.7759\n",
      "2019-01-02T16:36:17.445452, step: 51, loss: 0.4316416382789612, acc: 0.7734, auc: 0.8958, precision: 0.9138, recall: 0.6883\n",
      "2019-01-02T16:36:17.824763, step: 52, loss: 0.43982288241386414, acc: 0.8047, auc: 0.9086, precision: 0.9583, recall: 0.6667\n",
      "2019-01-02T16:36:18.206470, step: 53, loss: 0.4098266363143921, acc: 0.7891, auc: 0.9293, precision: 0.9762, recall: 0.6119\n",
      "2019-01-02T16:36:18.603881, step: 54, loss: 0.4098536968231201, acc: 0.8359, auc: 0.9123, precision: 0.92, recall: 0.7302\n",
      "2019-01-02T16:36:19.027429, step: 55, loss: 0.3724439740180969, acc: 0.8359, auc: 0.9253, precision: 0.8548, recall: 0.8154\n",
      "2019-01-02T16:36:19.445912, step: 56, loss: 0.3259436786174774, acc: 0.8594, auc: 0.9469, precision: 0.9344, recall: 0.8028\n",
      "2019-01-02T16:36:19.844829, step: 57, loss: 0.40952005982398987, acc: 0.8281, auc: 0.922, precision: 0.7973, recall: 0.8939\n",
      "2019-01-02T16:36:20.250697, step: 58, loss: 0.4166664481163025, acc: 0.8203, auc: 0.8872, precision: 0.8333, recall: 0.7937\n",
      "2019-01-02T16:36:20.638022, step: 59, loss: 0.41115885972976685, acc: 0.8203, auc: 0.9172, precision: 0.96, recall: 0.6957\n",
      "2019-01-02T16:36:21.015524, step: 60, loss: 0.37092673778533936, acc: 0.7969, auc: 0.9462, precision: 1.0, recall: 0.6438\n",
      "2019-01-02T16:36:21.407203, step: 61, loss: 0.33633458614349365, acc: 0.8672, auc: 0.9346, precision: 0.8615, recall: 0.875\n",
      "2019-01-02T16:36:21.814411, step: 62, loss: 0.508118748664856, acc: 0.8047, auc: 0.9125, precision: 0.7397, recall: 0.9\n",
      "2019-01-02T16:36:22.178612, step: 63, loss: 0.3536726236343384, acc: 0.875, auc: 0.925, precision: 0.8833, recall: 0.8548\n",
      "2019-01-02T16:36:22.571477, step: 64, loss: 0.3683279752731323, acc: 0.8359, auc: 0.9354, precision: 0.9107, recall: 0.7612\n",
      "2019-01-02T16:36:22.972778, step: 65, loss: 0.31749552488327026, acc: 0.8594, auc: 0.9487, precision: 0.9767, recall: 0.7119\n",
      "2019-01-02T16:36:23.423121, step: 66, loss: 0.41691336035728455, acc: 0.7422, auc: 0.9342, precision: 0.9778, recall: 0.5789\n",
      "2019-01-02T16:36:23.814577, step: 67, loss: 0.3607524633407593, acc: 0.7891, auc: 0.9395, precision: 0.8919, recall: 0.5893\n",
      "2019-01-02T16:36:24.206879, step: 68, loss: 0.363787442445755, acc: 0.7812, auc: 0.936, precision: 0.9, recall: 0.6\n",
      "2019-01-02T16:36:24.613926, step: 69, loss: 0.3079735040664673, acc: 0.8281, auc: 0.9567, precision: 0.8824, recall: 0.7377\n",
      "2019-01-02T16:36:24.986171, step: 70, loss: 0.3347926139831543, acc: 0.8359, auc: 0.9343, precision: 0.8929, recall: 0.7692\n",
      "2019-01-02T16:36:25.369830, step: 71, loss: 0.35939159989356995, acc: 0.8828, auc: 0.931, precision: 0.8714, recall: 0.9104\n",
      "2019-01-02T16:36:25.787540, step: 72, loss: 0.3352598547935486, acc: 0.8828, auc: 0.9414, precision: 0.8551, recall: 0.9219\n",
      "2019-01-02T16:36:26.211471, step: 73, loss: 0.31920385360717773, acc: 0.875, auc: 0.9345, precision: 0.9375, recall: 0.8333\n",
      "2019-01-02T16:36:26.621079, step: 74, loss: 0.34237217903137207, acc: 0.8281, auc: 0.9433, precision: 0.9556, recall: 0.6825\n",
      "2019-01-02T16:36:27.030312, step: 75, loss: 0.32557493448257446, acc: 0.8828, auc: 0.9369, precision: 0.9194, recall: 0.8507\n",
      "2019-01-02T16:36:27.420142, step: 76, loss: 0.29053404927253723, acc: 0.8672, auc: 0.9578, precision: 0.8413, recall: 0.8833\n",
      "2019-01-02T16:36:27.831573, step: 77, loss: 0.39593684673309326, acc: 0.8594, auc: 0.9372, precision: 0.7761, recall: 0.9455\n",
      "2019-01-02T16:36:28.225425, step: 78, loss: 0.3010733723640442, acc: 0.8594, auc: 0.9529, precision: 0.9423, recall: 0.7656\n",
      "2019-01-02T16:36:28.606406, step: 79, loss: 0.3158092498779297, acc: 0.8594, auc: 0.9514, precision: 0.9385, recall: 0.8133\n",
      "2019-01-02T16:36:28.999353, step: 80, loss: 0.3199984133243561, acc: 0.8438, auc: 0.9387, precision: 0.8929, recall: 0.7812\n",
      "2019-01-02T16:36:29.432079, step: 81, loss: 0.3076595366001129, acc: 0.8906, auc: 0.9387, precision: 0.8696, recall: 0.9231\n",
      "2019-01-02T16:36:29.822985, step: 82, loss: 0.28721845149993896, acc: 0.8828, auc: 0.9502, precision: 0.9057, recall: 0.8276\n",
      "2019-01-02T16:36:30.220794, step: 83, loss: 0.38405686616897583, acc: 0.8438, auc: 0.9076, precision: 0.8545, recall: 0.7966\n",
      "2019-01-02T16:36:30.605683, step: 84, loss: 0.34184733033180237, acc: 0.8516, auc: 0.9265, precision: 0.8689, recall: 0.8281\n",
      "2019-01-02T16:36:30.975818, step: 85, loss: 0.3467472195625305, acc: 0.8906, auc: 0.928, precision: 0.8689, recall: 0.8983\n",
      "2019-01-02T16:36:31.351235, step: 86, loss: 0.3462914824485779, acc: 0.8203, auc: 0.9293, precision: 0.8929, recall: 0.7463\n",
      "2019-01-02T16:36:31.764317, step: 87, loss: 0.3904198408126831, acc: 0.7969, auc: 0.898, precision: 0.8281, recall: 0.7794\n",
      "2019-01-02T16:36:32.192942, step: 88, loss: 0.25264936685562134, acc: 0.9297, auc: 0.9663, precision: 0.9242, recall: 0.9385\n",
      "2019-01-02T16:36:32.555155, step: 89, loss: 0.4191080927848816, acc: 0.8203, auc: 0.8885, precision: 0.8214, recall: 0.7797\n",
      "2019-01-02T16:36:32.949754, step: 90, loss: 0.29246973991394043, acc: 0.8516, auc: 0.9503, precision: 0.9038, recall: 0.7705\n",
      "2019-01-02T16:36:33.344441, step: 91, loss: 0.3142966032028198, acc: 0.8281, auc: 0.9498, precision: 0.95, recall: 0.6552\n",
      "2019-01-02T16:36:33.740916, step: 92, loss: 0.27454906702041626, acc: 0.9062, auc: 0.951, precision: 0.9286, recall: 0.8667\n",
      "2019-01-02T16:36:34.174039, step: 93, loss: 0.2253406047821045, acc: 0.9062, auc: 0.9702, precision: 0.9259, recall: 0.8621\n",
      "2019-01-02T16:36:34.596007, step: 94, loss: 0.2570659816265106, acc: 0.9219, auc: 0.9643, precision: 0.9138, recall: 0.9138\n",
      "2019-01-02T16:36:35.036030, step: 95, loss: 0.32155394554138184, acc: 0.875, auc: 0.93, precision: 0.925, recall: 0.74\n",
      "2019-01-02T16:36:35.456162, step: 96, loss: 0.325529545545578, acc: 0.8672, auc: 0.941, precision: 0.9231, recall: 0.7869\n",
      "2019-01-02T16:36:35.873958, step: 97, loss: 0.29112058877944946, acc: 0.875, auc: 0.9479, precision: 0.9403, recall: 0.84\n",
      "2019-01-02T16:36:36.279650, step: 98, loss: 0.33198118209838867, acc: 0.8516, auc: 0.9487, precision: 0.8125, recall: 0.8814\n",
      "2019-01-02T16:36:36.656876, step: 99, loss: 0.34239670634269714, acc: 0.875, auc: 0.9521, precision: 0.7971, recall: 0.9649\n",
      "2019-01-02T16:36:37.071275, step: 100, loss: 0.276319682598114, acc: 0.8828, auc: 0.955, precision: 0.9091, recall: 0.8696\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T16:36:53.201660, step: 100, loss: 0.331460341047018, acc: 0.8481589743589742, auc: 0.9360974358974358, precision: 0.9148923076923078, recall: 0.772128205128205\n",
      "2019-01-02T16:36:53.579535, step: 101, loss: 0.34687602519989014, acc: 0.8438, auc: 0.9178, precision: 0.873, recall: 0.8209\n",
      "2019-01-02T16:36:53.989202, step: 102, loss: 0.2910485565662384, acc: 0.8516, auc: 0.9607, precision: 0.94, recall: 0.746\n",
      "2019-01-02T16:36:54.377135, step: 103, loss: 0.37662583589553833, acc: 0.8516, auc: 0.9192, precision: 0.9348, recall: 0.7288\n",
      "2019-01-02T16:36:54.762502, step: 104, loss: 0.2988700568675995, acc: 0.8828, auc: 0.9559, precision: 0.9091, recall: 0.8333\n",
      "2019-01-02T16:36:55.163314, step: 105, loss: 0.32240742444992065, acc: 0.8672, auc: 0.9439, precision: 0.8644, recall: 0.85\n",
      "2019-01-02T16:36:55.537816, step: 106, loss: 0.34164202213287354, acc: 0.8203, auc: 0.9318, precision: 0.8644, recall: 0.7727\n",
      "2019-01-02T16:36:55.930228, step: 107, loss: 0.34936484694480896, acc: 0.8594, auc: 0.9241, precision: 0.9016, recall: 0.8209\n",
      "2019-01-02T16:36:56.374908, step: 108, loss: 0.3227155804634094, acc: 0.8516, auc: 0.9403, precision: 0.875, recall: 0.8358\n",
      "2019-01-02T16:36:56.785156, step: 109, loss: 0.45326074957847595, acc: 0.8281, auc: 0.8832, precision: 0.8493, recall: 0.8493\n",
      "2019-01-02T16:36:57.167031, step: 110, loss: 0.41401463747024536, acc: 0.7812, auc: 0.8963, precision: 0.7705, recall: 0.7705\n",
      "2019-01-02T16:36:57.592546, step: 111, loss: 0.4330475628376007, acc: 0.8203, auc: 0.8787, precision: 0.8286, recall: 0.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:36:57.997671, step: 112, loss: 0.3286125957965851, acc: 0.8594, auc: 0.9341, precision: 0.9091, recall: 0.7937\n",
      "2019-01-02T16:36:58.397663, step: 113, loss: 0.26148828864097595, acc: 0.8438, auc: 0.9695, precision: 0.9298, recall: 0.7681\n",
      "2019-01-02T16:36:58.861941, step: 114, loss: 0.38482046127319336, acc: 0.7969, auc: 0.9084, precision: 0.8846, recall: 0.697\n",
      "2019-01-02T16:36:59.318060, step: 115, loss: 0.33404529094696045, acc: 0.8438, auc: 0.9286, precision: 0.8776, recall: 0.7544\n",
      "2019-01-02T16:36:59.742193, step: 116, loss: 0.3449011445045471, acc: 0.8281, auc: 0.9238, precision: 0.8974, recall: 0.6604\n",
      "2019-01-02T16:37:00.169375, step: 117, loss: 0.3048555850982666, acc: 0.8203, auc: 0.9462, precision: 0.8824, recall: 0.7258\n",
      "2019-01-02T16:37:00.560327, step: 118, loss: 0.3426884114742279, acc: 0.8125, auc: 0.9236, precision: 0.8939, recall: 0.7763\n",
      "2019-01-02T16:37:00.937047, step: 119, loss: 0.2989532947540283, acc: 0.875, auc: 0.9473, precision: 0.8871, recall: 0.8594\n",
      "2019-01-02T16:37:01.322260, step: 120, loss: 0.33178192377090454, acc: 0.8672, auc: 0.9407, precision: 0.8507, recall: 0.8906\n",
      "2019-01-02T16:37:01.733218, step: 121, loss: 0.34023457765579224, acc: 0.8594, auc: 0.9303, precision: 0.8889, recall: 0.8358\n",
      "2019-01-02T16:37:02.119848, step: 122, loss: 0.3633570671081543, acc: 0.8125, auc: 0.9182, precision: 0.7966, recall: 0.7966\n",
      "2019-01-02T16:37:02.594210, step: 123, loss: 0.2442915141582489, acc: 0.8672, auc: 0.9723, precision: 0.9636, recall: 0.7794\n",
      "2019-01-02T16:37:03.003502, step: 124, loss: 0.27283239364624023, acc: 0.8828, auc: 0.9502, precision: 0.8936, recall: 0.8077\n",
      "2019-01-02T16:37:03.418827, step: 125, loss: 0.27454447746276855, acc: 0.9062, auc: 0.9549, precision: 0.9516, recall: 0.8676\n",
      "2019-01-02T16:37:03.870991, step: 126, loss: 0.48483508825302124, acc: 0.8125, auc: 0.888, precision: 0.9245, recall: 0.7101\n",
      "2019-01-02T16:37:04.304576, step: 127, loss: 0.31755584478378296, acc: 0.8906, auc: 0.9444, precision: 0.9552, recall: 0.8533\n",
      "2019-01-02T16:37:04.710182, step: 128, loss: 0.2330954670906067, acc: 0.9062, auc: 0.9701, precision: 0.9178, recall: 0.9178\n",
      "2019-01-02T16:37:05.088077, step: 129, loss: 0.2359119951725006, acc: 0.9141, auc: 0.9744, precision: 0.9077, recall: 0.9219\n",
      "2019-01-02T16:37:05.514129, step: 130, loss: 0.362765371799469, acc: 0.8281, auc: 0.9298, precision: 0.7966, recall: 0.8246\n",
      "2019-01-02T16:37:05.954094, step: 131, loss: 0.3208526372909546, acc: 0.8359, auc: 0.931, precision: 0.8393, recall: 0.7966\n",
      "2019-01-02T16:37:06.381489, step: 132, loss: 0.29859352111816406, acc: 0.8672, auc: 0.9545, precision: 0.9592, recall: 0.7581\n",
      "2019-01-02T16:37:06.803964, step: 133, loss: 0.34626373648643494, acc: 0.8359, auc: 0.9307, precision: 0.9388, recall: 0.7188\n",
      "2019-01-02T16:37:07.245194, step: 134, loss: 0.33135485649108887, acc: 0.875, auc: 0.9444, precision: 0.9828, recall: 0.7917\n",
      "2019-01-02T16:37:07.668055, step: 135, loss: 0.37139299511909485, acc: 0.8672, auc: 0.9145, precision: 0.8833, recall: 0.8413\n",
      "2019-01-02T16:37:08.098268, step: 136, loss: 0.3239020109176636, acc: 0.8438, auc: 0.9415, precision: 0.8421, recall: 0.8136\n",
      "2019-01-02T16:37:08.523487, step: 137, loss: 0.2912018299102783, acc: 0.8828, auc: 0.9624, precision: 0.8387, recall: 0.9123\n",
      "2019-01-02T16:37:08.925042, step: 138, loss: 0.30840998888015747, acc: 0.8438, auc: 0.9465, precision: 0.9344, recall: 0.7808\n",
      "2019-01-02T16:37:09.311371, step: 139, loss: 0.3116624653339386, acc: 0.875, auc: 0.9475, precision: 0.8615, recall: 0.8889\n",
      "2019-01-02T16:37:09.730640, step: 140, loss: 0.30261510610580444, acc: 0.8516, auc: 0.9426, precision: 0.9153, recall: 0.7941\n",
      "2019-01-02T16:37:10.169009, step: 141, loss: 0.2731533348560333, acc: 0.875, auc: 0.9531, precision: 0.8909, recall: 0.8305\n",
      "2019-01-02T16:37:10.531505, step: 142, loss: 0.33165502548217773, acc: 0.8516, auc: 0.9299, precision: 0.873, recall: 0.8333\n",
      "2019-01-02T16:37:10.978354, step: 143, loss: 0.23139913380146027, acc: 0.8984, auc: 0.9697, precision: 0.8933, recall: 0.9306\n",
      "2019-01-02T16:37:11.375631, step: 144, loss: 0.26979389786720276, acc: 0.8516, auc: 0.956, precision: 0.9138, recall: 0.791\n",
      "2019-01-02T16:37:11.820393, step: 145, loss: 0.24415528774261475, acc: 0.8672, auc: 0.9622, precision: 0.9091, recall: 0.8451\n",
      "2019-01-02T16:37:12.220723, step: 146, loss: 0.36971160769462585, acc: 0.8281, auc: 0.9221, precision: 0.8788, recall: 0.8056\n",
      "2019-01-02T16:37:12.632773, step: 147, loss: 0.3286963403224945, acc: 0.875, auc: 0.933, precision: 0.9091, recall: 0.8197\n",
      "2019-01-02T16:37:13.061657, step: 148, loss: 0.2669144570827484, acc: 0.8438, auc: 0.9614, precision: 0.9077, recall: 0.8082\n",
      "2019-01-02T16:37:13.531789, step: 149, loss: 0.19974270462989807, acc: 0.9141, auc: 0.9833, precision: 0.9286, recall: 0.8814\n",
      "2019-01-02T16:37:13.952450, step: 150, loss: 0.26988348364830017, acc: 0.8906, auc: 0.9569, precision: 0.9275, recall: 0.8767\n",
      "2019-01-02T16:37:14.363465, step: 151, loss: 0.34231072664260864, acc: 0.8828, auc: 0.9294, precision: 0.8923, recall: 0.8788\n",
      "2019-01-02T16:37:14.785684, step: 152, loss: 0.3030574321746826, acc: 0.8906, auc: 0.9504, precision: 0.9032, recall: 0.875\n",
      "2019-01-02T16:37:15.254562, step: 153, loss: 0.36406415700912476, acc: 0.8438, auc: 0.9189, precision: 0.8689, recall: 0.8154\n",
      "2019-01-02T16:37:15.683230, step: 154, loss: 0.2914322316646576, acc: 0.8516, auc: 0.9509, precision: 0.9245, recall: 0.7656\n",
      "2019-01-02T16:37:16.137810, step: 155, loss: 0.2927459478378296, acc: 0.8516, auc: 0.9587, precision: 0.9273, recall: 0.7727\n",
      "2019-01-02T16:37:16.507387, step: 156, loss: 0.36642521619796753, acc: 0.8906, auc: 0.9216, precision: 0.9107, recall: 0.85\n",
      "start training model\n",
      "2019-01-02T16:37:16.937909, step: 157, loss: 0.23499062657356262, acc: 0.9375, auc: 0.9692, precision: 0.8776, recall: 0.9556\n",
      "2019-01-02T16:37:17.367807, step: 158, loss: 0.25271767377853394, acc: 0.9141, auc: 0.9626, precision: 0.9667, recall: 0.8657\n",
      "2019-01-02T16:37:17.728363, step: 159, loss: 0.23374958336353302, acc: 0.9062, auc: 0.9702, precision: 0.9483, recall: 0.8594\n",
      "2019-01-02T16:37:18.157757, step: 160, loss: 0.1985962688922882, acc: 0.9453, auc: 0.9799, precision: 0.9333, recall: 0.9492\n",
      "2019-01-02T16:37:18.623053, step: 161, loss: 0.28277432918548584, acc: 0.9219, auc: 0.9494, precision: 0.9245, recall: 0.8909\n",
      "2019-01-02T16:37:19.018350, step: 162, loss: 0.1407305747270584, acc: 0.9688, auc: 0.9892, precision: 0.9516, recall: 0.9833\n",
      "2019-01-02T16:37:19.427747, step: 163, loss: 0.19377774000167847, acc: 0.9297, auc: 0.9787, precision: 0.9818, recall: 0.871\n",
      "2019-01-02T16:37:19.855181, step: 164, loss: 0.15983685851097107, acc: 0.9297, auc: 0.9892, precision: 1.0, recall: 0.8525\n",
      "2019-01-02T16:37:20.310328, step: 165, loss: 0.1745321899652481, acc: 0.9297, auc: 0.9806, precision: 0.931, recall: 0.9153\n",
      "2019-01-02T16:37:20.744082, step: 166, loss: 0.3021705150604248, acc: 0.9141, auc: 0.9538, precision: 0.8857, recall: 0.9538\n",
      "2019-01-02T16:37:21.146850, step: 167, loss: 0.22985996305942535, acc: 0.9219, auc: 0.9697, precision: 0.9474, recall: 0.8852\n",
      "2019-01-02T16:37:21.567434, step: 168, loss: 0.27271926403045654, acc: 0.9062, auc: 0.9597, precision: 0.9483, recall: 0.8594\n",
      "2019-01-02T16:37:21.953953, step: 169, loss: 0.2446191906929016, acc: 0.9297, auc: 0.9577, precision: 0.9545, recall: 0.913\n",
      "2019-01-02T16:37:22.384709, step: 170, loss: 0.26780956983566284, acc: 0.9141, auc: 0.9635, precision: 0.9032, recall: 0.918\n",
      "2019-01-02T16:37:22.783060, step: 171, loss: 0.23835068941116333, acc: 0.9141, auc: 0.9635, precision: 0.9667, recall: 0.8657\n",
      "2019-01-02T16:37:23.200722, step: 172, loss: 0.15930062532424927, acc: 0.9375, auc: 0.9887, precision: 0.9692, recall: 0.913\n",
      "2019-01-02T16:37:23.625302, step: 173, loss: 0.250620573759079, acc: 0.8594, auc: 0.9718, precision: 0.963, recall: 0.7647\n",
      "2019-01-02T16:37:24.040641, step: 174, loss: 0.18684720993041992, acc: 0.9297, auc: 0.9879, precision: 0.9692, recall: 0.9\n",
      "2019-01-02T16:37:24.480693, step: 175, loss: 0.22537411749362946, acc: 0.9219, auc: 0.9735, precision: 0.9464, recall: 0.8833\n",
      "2019-01-02T16:37:24.892916, step: 176, loss: 0.21436986327171326, acc: 0.9297, auc: 0.974, precision: 0.9538, recall: 0.9118\n",
      "2019-01-02T16:37:25.295306, step: 177, loss: 0.212285578250885, acc: 0.9375, auc: 0.9794, precision: 0.9103, recall: 0.9861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:37:25.711369, step: 178, loss: 0.16833995282649994, acc: 0.9375, auc: 0.9882, precision: 0.9107, recall: 0.9444\n",
      "2019-01-02T16:37:26.152049, step: 179, loss: 0.2390126734972, acc: 0.8828, auc: 0.9682, precision: 0.9322, recall: 0.8333\n",
      "2019-01-02T16:37:26.560170, step: 180, loss: 0.21417373418807983, acc: 0.9141, auc: 0.9713, precision: 0.9153, recall: 0.9\n",
      "2019-01-02T16:37:26.966428, step: 181, loss: 0.3094790577888489, acc: 0.8516, auc: 0.9718, precision: 0.9828, recall: 0.76\n",
      "2019-01-02T16:37:27.398147, step: 182, loss: 0.1629723310470581, acc: 0.9297, auc: 0.9834, precision: 0.9394, recall: 0.9254\n",
      "2019-01-02T16:37:27.814299, step: 183, loss: 0.11938142031431198, acc: 0.9688, auc: 0.9948, precision: 0.9718, recall: 0.9718\n",
      "2019-01-02T16:37:28.231880, step: 184, loss: 0.21871323883533478, acc: 0.9219, auc: 0.9775, precision: 0.9062, recall: 0.9355\n",
      "2019-01-02T16:37:28.645874, step: 185, loss: 0.23069381713867188, acc: 0.8906, auc: 0.9768, precision: 0.8657, recall: 0.9206\n",
      "2019-01-02T16:37:29.055743, step: 186, loss: 0.13770578801631927, acc: 0.9375, auc: 0.9927, precision: 0.9825, recall: 0.8889\n",
      "2019-01-02T16:37:29.468838, step: 187, loss: 0.297722190618515, acc: 0.8594, auc: 0.9612, precision: 0.9636, recall: 0.7681\n",
      "2019-01-02T16:37:29.878198, step: 188, loss: 0.2502390444278717, acc: 0.9062, auc: 0.9653, precision: 1.0, recall: 0.8033\n",
      "2019-01-02T16:37:30.296984, step: 189, loss: 0.21492242813110352, acc: 0.8828, auc: 0.9786, precision: 0.9677, recall: 0.8219\n",
      "2019-01-02T16:37:30.670498, step: 190, loss: 0.17095258831977844, acc: 0.9453, auc: 0.9905, precision: 0.913, recall: 0.9844\n",
      "2019-01-02T16:37:31.085935, step: 191, loss: 0.2808637022972107, acc: 0.8906, auc: 0.9661, precision: 0.8701, recall: 0.9437\n",
      "2019-01-02T16:37:31.500144, step: 192, loss: 0.20006150007247925, acc: 0.9453, auc: 0.9773, precision: 0.9444, recall: 0.9577\n",
      "2019-01-02T16:37:31.898617, step: 193, loss: 0.24796366691589355, acc: 0.8984, auc: 0.98, precision: 0.8696, recall: 0.9375\n",
      "2019-01-02T16:37:32.339212, step: 194, loss: 0.2773761451244354, acc: 0.8672, auc: 0.9538, precision: 0.9286, recall: 0.8\n",
      "2019-01-02T16:37:32.789225, step: 195, loss: 0.2688021659851074, acc: 0.8828, auc: 0.9565, precision: 0.9074, recall: 0.8305\n",
      "2019-01-02T16:37:33.219956, step: 196, loss: 0.2703476548194885, acc: 0.8516, auc: 0.9736, precision: 0.98, recall: 0.7313\n",
      "2019-01-02T16:37:33.635591, step: 197, loss: 0.3395593762397766, acc: 0.7812, auc: 0.9682, precision: 0.9565, recall: 0.6286\n",
      "2019-01-02T16:37:34.038942, step: 198, loss: 0.2536211907863617, acc: 0.8828, auc: 0.9605, precision: 0.9286, recall: 0.7647\n",
      "2019-01-02T16:37:34.454797, step: 199, loss: 0.22051818668842316, acc: 0.8984, auc: 0.9797, precision: 0.9057, recall: 0.8571\n",
      "2019-01-02T16:37:34.880223, step: 200, loss: 0.26055335998535156, acc: 0.875, auc: 0.9615, precision: 0.8947, recall: 0.8947\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T16:37:50.571296, step: 200, loss: 0.34166955718627345, acc: 0.8697820512820513, auc: 0.9468923076923074, precision: 0.8405615384615382, recall: 0.9145794871794869\n",
      "2019-01-02T16:37:50.972040, step: 201, loss: 0.29596757888793945, acc: 0.8984, auc: 0.9736, precision: 0.8533, recall: 0.9697\n",
      "2019-01-02T16:37:51.382012, step: 202, loss: 0.21823695302009583, acc: 0.9297, auc: 0.9752, precision: 0.918, recall: 0.9333\n",
      "2019-01-02T16:37:51.793136, step: 203, loss: 0.22500883042812347, acc: 0.8906, auc: 0.9708, precision: 0.9296, recall: 0.88\n",
      "2019-01-02T16:37:52.193553, step: 204, loss: 0.16835886240005493, acc: 0.9297, auc: 0.9863, precision: 0.9531, recall: 0.9104\n",
      "2019-01-02T16:37:52.601520, step: 205, loss: 0.19733142852783203, acc: 0.9297, auc: 0.9723, precision: 0.9583, recall: 0.92\n",
      "2019-01-02T16:37:53.037057, step: 206, loss: 0.1757441610097885, acc: 0.9297, auc: 0.9819, precision: 0.9672, recall: 0.8939\n",
      "2019-01-02T16:37:53.461249, step: 207, loss: 0.2565683424472809, acc: 0.8906, auc: 0.9616, precision: 0.8871, recall: 0.8871\n",
      "2019-01-02T16:37:53.870001, step: 208, loss: 0.28858423233032227, acc: 0.9062, auc: 0.9514, precision: 0.8923, recall: 0.9206\n",
      "2019-01-02T16:37:54.230377, step: 209, loss: 0.1939590871334076, acc: 0.8984, auc: 0.9762, precision: 0.9123, recall: 0.8667\n",
      "2019-01-02T16:37:54.627190, step: 210, loss: 0.14022955298423767, acc: 0.9531, auc: 0.9887, precision: 0.9643, recall: 0.931\n",
      "2019-01-02T16:37:55.020824, step: 211, loss: 0.15553489327430725, acc: 0.9062, auc: 0.9891, precision: 0.9762, recall: 0.7885\n",
      "2019-01-02T16:37:55.442888, step: 212, loss: 0.2688632607460022, acc: 0.8906, auc: 0.969, precision: 0.98, recall: 0.7903\n",
      "2019-01-02T16:37:55.873398, step: 213, loss: 0.23032593727111816, acc: 0.8984, auc: 0.9699, precision: 0.9492, recall: 0.8485\n",
      "2019-01-02T16:37:56.279442, step: 214, loss: 0.22473935782909393, acc: 0.9453, auc: 0.9682, precision: 0.9219, recall: 0.9672\n",
      "2019-01-02T16:37:56.643298, step: 215, loss: 0.39713844656944275, acc: 0.8906, auc: 0.9577, precision: 0.8395, recall: 0.9855\n",
      "2019-01-02T16:37:57.062491, step: 216, loss: 0.1838458627462387, acc: 0.9219, auc: 0.9885, precision: 0.8955, recall: 0.9524\n",
      "2019-01-02T16:37:57.460474, step: 217, loss: 0.35390201210975647, acc: 0.8516, auc: 0.9384, precision: 0.9216, recall: 0.7581\n",
      "2019-01-02T16:37:57.845764, step: 218, loss: 0.33388856053352356, acc: 0.8672, auc: 0.9565, precision: 0.9804, recall: 0.7576\n",
      "2019-01-02T16:37:58.237076, step: 219, loss: 0.2901528477668762, acc: 0.8672, auc: 0.9741, precision: 1.0, recall: 0.7344\n",
      "2019-01-02T16:37:58.608123, step: 220, loss: 0.2183060348033905, acc: 0.8984, auc: 0.9812, precision: 0.9783, recall: 0.7895\n",
      "2019-01-02T16:37:58.980319, step: 221, loss: 0.2272777557373047, acc: 0.8906, auc: 0.9768, precision: 0.9683, recall: 0.8356\n",
      "2019-01-02T16:37:59.384106, step: 222, loss: 0.30546921491622925, acc: 0.8594, auc: 0.9383, precision: 0.8816, recall: 0.8816\n",
      "2019-01-02T16:37:59.830284, step: 223, loss: 0.3261369466781616, acc: 0.8984, auc: 0.9395, precision: 0.8923, recall: 0.9062\n",
      "2019-01-02T16:38:00.229773, step: 224, loss: 0.2963094115257263, acc: 0.9219, auc: 0.9595, precision: 0.9103, recall: 0.9595\n",
      "2019-01-02T16:38:00.655699, step: 225, loss: 0.24595001339912415, acc: 0.9375, auc: 0.9924, precision: 0.9, recall: 0.9844\n",
      "2019-01-02T16:38:01.075302, step: 226, loss: 0.30636876821517944, acc: 0.8828, auc: 0.9602, precision: 0.8305, recall: 0.9074\n",
      "2019-01-02T16:38:01.515758, step: 227, loss: 0.2895972728729248, acc: 0.8906, auc: 0.9556, precision: 0.8857, recall: 0.9118\n",
      "2019-01-02T16:38:01.945988, step: 228, loss: 0.2199159413576126, acc: 0.9141, auc: 0.9777, precision: 0.9574, recall: 0.8333\n",
      "2019-01-02T16:38:02.337988, step: 229, loss: 0.2820451259613037, acc: 0.8828, auc: 0.9528, precision: 0.9565, recall: 0.7719\n",
      "2019-01-02T16:38:02.702288, step: 230, loss: 0.2389259934425354, acc: 0.8672, auc: 0.981, precision: 0.9792, recall: 0.746\n",
      "2019-01-02T16:38:03.062716, step: 231, loss: 0.27298370003700256, acc: 0.875, auc: 0.9664, precision: 0.9825, recall: 0.7887\n",
      "2019-01-02T16:38:03.463974, step: 232, loss: 0.21464253962039948, acc: 0.9062, auc: 0.9744, precision: 0.9636, recall: 0.8413\n",
      "2019-01-02T16:38:03.863126, step: 233, loss: 0.18705877661705017, acc: 0.9297, auc: 0.9761, precision: 0.9355, recall: 0.9206\n",
      "2019-01-02T16:38:04.254599, step: 234, loss: 0.20887038111686707, acc: 0.9219, auc: 0.9777, precision: 0.9107, recall: 0.9107\n",
      "2019-01-02T16:38:04.657234, step: 235, loss: 0.24842432141304016, acc: 0.9297, auc: 0.9653, precision: 0.9118, recall: 0.9538\n",
      "2019-01-02T16:38:05.076418, step: 236, loss: 0.1420544683933258, acc: 0.9453, auc: 0.991, precision: 0.9615, recall: 0.9091\n",
      "2019-01-02T16:38:05.501649, step: 237, loss: 0.1485130786895752, acc: 0.9297, auc: 0.9885, precision: 0.9155, recall: 0.9559\n",
      "2019-01-02T16:38:05.924391, step: 238, loss: 0.2780645489692688, acc: 0.875, auc: 0.9519, precision: 0.9077, recall: 0.8551\n",
      "2019-01-02T16:38:06.308829, step: 239, loss: 0.2575225234031677, acc: 0.8672, auc: 0.9677, precision: 0.9206, recall: 0.8286\n",
      "2019-01-02T16:38:06.688113, step: 240, loss: 0.25401076674461365, acc: 0.8672, auc: 0.968, precision: 0.9444, recall: 0.7846\n",
      "2019-01-02T16:38:07.072957, step: 241, loss: 0.15359655022621155, acc: 0.9297, auc: 0.9885, precision: 0.9818, recall: 0.871\n",
      "2019-01-02T16:38:07.467637, step: 242, loss: 0.24008014798164368, acc: 0.8906, auc: 0.9665, precision: 0.9138, recall: 0.8548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:38:07.906641, step: 243, loss: 0.20152463018894196, acc: 0.9141, auc: 0.9834, precision: 0.8833, recall: 0.9298\n",
      "2019-01-02T16:38:08.326265, step: 244, loss: 0.2615218162536621, acc: 0.8984, auc: 0.964, precision: 0.8852, recall: 0.9\n",
      "2019-01-02T16:38:08.749454, step: 245, loss: 0.1552199125289917, acc: 0.9531, auc: 0.9813, precision: 0.9571, recall: 0.9571\n",
      "2019-01-02T16:38:09.168221, step: 246, loss: 0.09124666452407837, acc: 0.9688, auc: 0.9968, precision: 0.9821, recall: 0.9483\n",
      "2019-01-02T16:38:09.550840, step: 247, loss: 0.2176436185836792, acc: 0.9062, auc: 0.9751, precision: 0.9636, recall: 0.8413\n",
      "2019-01-02T16:38:09.961869, step: 248, loss: 0.14162349700927734, acc: 0.9531, auc: 0.9893, precision: 0.9836, recall: 0.9231\n",
      "2019-01-02T16:38:10.344246, step: 249, loss: 0.23575223982334137, acc: 0.8672, auc: 0.9758, precision: 0.9423, recall: 0.7778\n",
      "2019-01-02T16:38:10.738275, step: 250, loss: 0.29355961084365845, acc: 0.8672, auc: 0.9491, precision: 0.9231, recall: 0.7869\n",
      "2019-01-02T16:38:11.113287, step: 251, loss: 0.29522380232810974, acc: 0.8906, auc: 0.9528, precision: 0.9014, recall: 0.9014\n",
      "2019-01-02T16:38:11.554734, step: 252, loss: 0.3276473879814148, acc: 0.8672, auc: 0.9525, precision: 0.8033, recall: 0.9074\n",
      "2019-01-02T16:38:11.951631, step: 253, loss: 0.2651786804199219, acc: 0.9375, auc: 0.9574, precision: 0.9429, recall: 0.9429\n",
      "2019-01-02T16:38:12.357712, step: 254, loss: 0.1909516155719757, acc: 0.9453, auc: 0.9787, precision: 0.961, recall: 0.9487\n",
      "2019-01-02T16:38:12.737530, step: 255, loss: 0.24971693754196167, acc: 0.9219, auc: 0.9684, precision: 0.931, recall: 0.9\n",
      "2019-01-02T16:38:13.116283, step: 256, loss: 0.215920090675354, acc: 0.9297, auc: 0.9807, precision: 0.9333, recall: 0.918\n",
      "2019-01-02T16:38:13.515902, step: 257, loss: 0.21911999583244324, acc: 0.8984, auc: 0.9776, precision: 0.9038, recall: 0.8545\n",
      "2019-01-02T16:38:13.881654, step: 258, loss: 0.2709827721118927, acc: 0.8672, auc: 0.9728, precision: 1.0, recall: 0.7463\n",
      "2019-01-02T16:38:14.244391, step: 259, loss: 0.377063512802124, acc: 0.7969, auc: 0.9294, precision: 0.9333, recall: 0.6462\n",
      "2019-01-02T16:38:14.637493, step: 260, loss: 0.23881514370441437, acc: 0.8594, auc: 0.9722, precision: 0.9516, recall: 0.7973\n",
      "2019-01-02T16:38:15.066559, step: 261, loss: 0.3113288879394531, acc: 0.8906, auc: 0.9506, precision: 0.8644, recall: 0.8947\n",
      "2019-01-02T16:38:15.510721, step: 262, loss: 0.23264366388320923, acc: 0.9375, auc: 0.9841, precision: 0.8871, recall: 0.9821\n",
      "2019-01-02T16:38:15.919614, step: 263, loss: 0.18088608980178833, acc: 0.9297, auc: 0.9826, precision: 0.9403, recall: 0.9265\n",
      "2019-01-02T16:38:16.341313, step: 264, loss: 0.23791661858558655, acc: 0.8828, auc: 0.9717, precision: 0.9, recall: 0.8571\n",
      "2019-01-02T16:38:16.731399, step: 265, loss: 0.1677054464817047, acc: 0.9375, auc: 0.9899, precision: 0.9559, recall: 0.9286\n",
      "2019-01-02T16:38:17.147863, step: 266, loss: 0.23178891837596893, acc: 0.8984, auc: 0.9685, precision: 0.9333, recall: 0.8615\n",
      "2019-01-02T16:38:17.585207, step: 267, loss: 0.23046422004699707, acc: 0.8594, auc: 0.9728, precision: 0.9455, recall: 0.7761\n",
      "2019-01-02T16:38:18.009997, step: 268, loss: 0.23624682426452637, acc: 0.9062, auc: 0.9651, precision: 0.9643, recall: 0.8438\n",
      "2019-01-02T16:38:18.478811, step: 269, loss: 0.2260385900735855, acc: 0.9062, auc: 0.9697, precision: 0.8889, recall: 0.8889\n",
      "2019-01-02T16:38:18.892961, step: 270, loss: 0.24710121750831604, acc: 0.875, auc: 0.9649, precision: 0.9216, recall: 0.7966\n",
      "2019-01-02T16:38:19.298409, step: 271, loss: 0.3041980266571045, acc: 0.8828, auc: 0.9418, precision: 0.9062, recall: 0.8657\n",
      "2019-01-02T16:38:19.673715, step: 272, loss: 0.2669825255870819, acc: 0.8672, auc: 0.9582, precision: 0.8947, recall: 0.8226\n",
      "2019-01-02T16:38:20.126509, step: 273, loss: 0.22384947538375854, acc: 0.8984, auc: 0.9697, precision: 0.8923, recall: 0.9062\n",
      "2019-01-02T16:38:20.553014, step: 274, loss: 0.18042027950286865, acc: 0.9375, auc: 0.9802, precision: 0.9344, recall: 0.9344\n",
      "2019-01-02T16:38:20.939988, step: 275, loss: 0.2641329765319824, acc: 0.875, auc: 0.9568, precision: 0.8615, recall: 0.8889\n",
      "2019-01-02T16:38:21.335383, step: 276, loss: 0.2722765803337097, acc: 0.875, auc: 0.9579, precision: 0.9048, recall: 0.8507\n",
      "2019-01-02T16:38:21.721987, step: 277, loss: 0.3324260115623474, acc: 0.8672, auc: 0.9401, precision: 0.8814, recall: 0.8387\n",
      "2019-01-02T16:38:22.144385, step: 278, loss: 0.2489713579416275, acc: 0.9297, auc: 0.9577, precision: 0.9362, recall: 0.88\n",
      "2019-01-02T16:38:22.544942, step: 279, loss: 0.2262267917394638, acc: 0.9062, auc: 0.9709, precision: 0.9677, recall: 0.8571\n",
      "2019-01-02T16:38:22.958032, step: 280, loss: 0.2370031177997589, acc: 0.9141, auc: 0.9699, precision: 0.8548, recall: 0.9636\n",
      "2019-01-02T16:38:23.338542, step: 281, loss: 0.24484069645404816, acc: 0.875, auc: 0.9651, precision: 0.9259, recall: 0.8065\n",
      "2019-01-02T16:38:23.756378, step: 282, loss: 0.21078401803970337, acc: 0.9141, auc: 0.9769, precision: 0.9138, recall: 0.8983\n",
      "2019-01-02T16:38:24.176130, step: 283, loss: 0.161948561668396, acc: 0.9531, auc: 0.9883, precision: 0.9661, recall: 0.9344\n",
      "2019-01-02T16:38:24.627429, step: 284, loss: 0.23531699180603027, acc: 0.9297, auc: 0.9626, precision: 0.9231, recall: 0.9375\n",
      "2019-01-02T16:38:25.006252, step: 285, loss: 0.19125020503997803, acc: 0.9531, auc: 0.974, precision: 0.9714, recall: 0.9444\n",
      "2019-01-02T16:38:25.366714, step: 286, loss: 0.22596092522144318, acc: 0.9297, auc: 0.9663, precision: 0.942, recall: 0.9286\n",
      "2019-01-02T16:38:25.759178, step: 287, loss: 0.3018782138824463, acc: 0.875, auc: 0.9539, precision: 0.8571, recall: 0.8235\n",
      "2019-01-02T16:38:26.138004, step: 288, loss: 0.16522234678268433, acc: 0.9297, auc: 0.9794, precision: 0.9565, recall: 0.9167\n",
      "2019-01-02T16:38:26.554917, step: 289, loss: 0.18195483088493347, acc: 0.9219, auc: 0.9805, precision: 0.9359, recall: 0.9359\n",
      "2019-01-02T16:38:26.922806, step: 290, loss: 0.16326166689395905, acc: 0.9141, auc: 0.9845, precision: 0.9394, recall: 0.8986\n",
      "2019-01-02T16:38:27.334397, step: 291, loss: 0.26295992732048035, acc: 0.9219, auc: 0.9628, precision: 0.9231, recall: 0.9474\n",
      "2019-01-02T16:38:27.759528, step: 292, loss: 0.21875274181365967, acc: 0.8906, auc: 0.97, precision: 0.9474, recall: 0.8308\n",
      "2019-01-02T16:38:28.154767, step: 293, loss: 0.2113833725452423, acc: 0.8906, auc: 0.9737, precision: 0.9104, recall: 0.8841\n",
      "2019-01-02T16:38:28.533686, step: 294, loss: 0.209383025765419, acc: 0.9453, auc: 0.9742, precision: 0.9333, recall: 0.9492\n",
      "2019-01-02T16:38:28.957669, step: 295, loss: 0.17583538591861725, acc: 0.9453, auc: 0.9808, precision: 0.9615, recall: 0.9091\n",
      "2019-01-02T16:38:29.365631, step: 296, loss: 0.2929411232471466, acc: 0.8906, auc: 0.9486, precision: 0.9344, recall: 0.8507\n",
      "2019-01-02T16:38:29.772729, step: 297, loss: 0.18376410007476807, acc: 0.9062, auc: 0.989, precision: 1.0, recall: 0.8\n",
      "2019-01-02T16:38:30.161665, step: 298, loss: 0.2413824200630188, acc: 0.9062, auc: 0.9619, precision: 0.9492, recall: 0.8615\n",
      "2019-01-02T16:38:30.599257, step: 299, loss: 0.1833776831626892, acc: 0.9375, auc: 0.9801, precision: 0.9815, recall: 0.8833\n",
      "2019-01-02T16:38:31.007433, step: 300, loss: 0.10550864785909653, acc: 0.9844, auc: 0.9993, precision: 0.9821, recall: 0.9821\n",
      "\n",
      "Evaluation:\n",
      "2019-01-02T16:38:46.636910, step: 300, loss: 0.3283058179494662, acc: 0.8796102564102566, auc: 0.9503153846153848, precision: 0.863051282051282, recall: 0.9047897435897434\n",
      "2019-01-02T16:38:47.025514, step: 301, loss: 0.17365290224552155, acc: 0.9609, auc: 0.989, precision: 0.9531, recall: 0.9683\n",
      "2019-01-02T16:38:47.412804, step: 302, loss: 0.16041196882724762, acc: 0.9531, auc: 0.991, precision: 0.9467, recall: 0.9726\n",
      "2019-01-02T16:38:47.816937, step: 303, loss: 0.20430585741996765, acc: 0.9141, auc: 0.9719, precision: 0.9219, recall: 0.9077\n",
      "2019-01-02T16:38:48.218962, step: 304, loss: 0.1695571094751358, acc: 0.9375, auc: 0.9784, precision: 0.9474, recall: 0.9153\n",
      "2019-01-02T16:38:48.623050, step: 305, loss: 0.23917223513126373, acc: 0.8594, auc: 0.9724, precision: 0.9792, recall: 0.7344\n",
      "2019-01-02T16:38:49.067264, step: 306, loss: 0.17186525464057922, acc: 0.9297, auc: 0.9873, precision: 0.9667, recall: 0.8923\n",
      "2019-01-02T16:38:49.543163, step: 307, loss: 0.3010072708129883, acc: 0.8906, auc: 0.949, precision: 0.9074, recall: 0.8448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-02T16:38:49.954998, step: 308, loss: 0.3083301782608032, acc: 0.8672, auc: 0.9465, precision: 0.8594, recall: 0.873\n",
      "2019-01-02T16:38:50.345615, step: 309, loss: 0.20197993516921997, acc: 0.9453, auc: 0.9734, precision: 0.9524, recall: 0.9375\n",
      "2019-01-02T16:38:50.777740, step: 310, loss: 0.29360535740852356, acc: 0.9219, auc: 0.944, precision: 0.9298, recall: 0.8983\n",
      "2019-01-02T16:38:51.238385, step: 311, loss: 0.14922231435775757, acc: 0.9766, auc: 0.9865, precision: 0.9661, recall: 0.9828\n",
      "2019-01-02T16:38:51.671896, step: 312, loss: 0.15930430591106415, acc: 0.9219, auc: 0.9873, precision: 0.9254, recall: 0.9254\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "#         saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
